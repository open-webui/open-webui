#!/usr/bin/env python3
"""
Daily Batch Processor for mAI Usage Tracking
Runs at 00:00 daily to process and aggregate usage data with NBP exchange rates

This script:
1. Processes previous day's usage data
2. Updates cumulative monthly totals (1st to current day)
3. Fetches fresh NBP exchange rates for PLN conversion
4. Maintains database consistency without real-time overhead
"""

import asyncio
import logging
import sqlite3
import time
from datetime import datetime, date, timedelta
from typing import Dict, Any, Optional

from open_webui.config import DATA_DIR

# Setup logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

class DailyBatchProcessor:
    """Daily batch processor for usage data aggregation"""
    
    def __init__(self):
        self.db_path = f"{DATA_DIR}/webui.db"
        
    async def run_daily_batch(self) -> Dict[str, Any]:
        """
        Main daily batch processing function
        Should be called at 00:00 daily via cron or scheduler
        """
        batch_start = time.time()
        log.info("üï∞Ô∏è Starting daily batch processing for mAI usage tracking")
        
        try:
            # Process yesterday's COMPLETE data (proper production logic)
            # This ensures data integrity and complete daily summaries for monthly counting
            today = date.today()
            yesterday = today - timedelta(days=1)
            processing_date = yesterday  # Always process complete previous day data
            
            results = {
                "success": True,
                "processing_date": processing_date.isoformat(),
                "current_date": today.isoformat(),
                "batch_start_time": datetime.now().isoformat(),
                "operations": []
            }
            
            # 1. Update exchange rates for PLN conversion
            exchange_result = await self._update_exchange_rates()
            results["operations"].append({
                "operation": "exchange_rate_update",
                "success": exchange_result["success"],
                "details": exchange_result
            })
            
            # 2. Process and consolidate daily usage data
            consolidation_result = await self._consolidate_daily_usage(processing_date)
            results["operations"].append({
                "operation": "daily_consolidation", 
                "success": consolidation_result["success"],
                "details": consolidation_result
            })
            
            # 3. Update monthly cumulative totals (1st to current day)
            monthly_result = await self._update_monthly_totals(today)
            results["operations"].append({
                "operation": "monthly_totals_update",
                "success": monthly_result["success"], 
                "details": monthly_result
            })
            
            # 4. Clean up old processed generations
            cleanup_result = await self._cleanup_old_data()
            results["operations"].append({
                "operation": "data_cleanup",
                "success": cleanup_result["success"],
                "details": cleanup_result
            })
            
            # Calculate batch processing time
            batch_duration = time.time() - batch_start
            results["batch_duration_seconds"] = round(batch_duration, 3)
            results["completed_at"] = datetime.now().isoformat()
            
            # Log summary
            successful_ops = sum(1 for op in results["operations"] if op["success"])
            total_ops = len(results["operations"])
            
            log.info(f"‚úÖ Daily batch processing completed: {successful_ops}/{total_ops} operations successful in {batch_duration:.2f}s")
            
            return results
            
        except Exception as e:
            error_result = {
                "success": False,
                "error": str(e),
                "processing_date": processing_date.isoformat() if 'processing_date' in locals() else None,
                "batch_duration_seconds": time.time() - batch_start,
                "failed_at": datetime.now().isoformat()
            }
            
            log.error(f"‚ùå Daily batch processing failed: {e}")
            return error_result
    
    async def _update_exchange_rates(self) -> Dict[str, Any]:
        """Update NBP exchange rates for accurate PLN conversion"""
        try:
            from open_webui.utils.currency_converter import get_exchange_rate_info
            
            # Get fresh exchange rate with holiday-aware logic
            exchange_info = await get_exchange_rate_info()
            
            # Store rate info for the day (could be stored in a rates table if needed)
            rate_result = {
                "success": True,
                "usd_pln_rate": exchange_info.get("usd_pln", 4.0),
                "effective_date": exchange_info.get("effective_date"),
                "rate_source": exchange_info.get("rate_source", "unknown"),
                "fetched_at": datetime.now().isoformat()
            }
            
            log.info(f"üí± Exchange rates updated: 1 USD = {rate_result['usd_pln_rate']} PLN (source: {rate_result['rate_source']})")
            
            return rate_result
            
        except Exception as e:
            fallback_result = {
                "success": True,  # Success with fallback
                "usd_pln_rate": 4.0,  # Fallback rate
                "effective_date": date.today().isoformat(),
                "rate_source": "fallback_nbp_unavailable", 
                "error": str(e),
                "fetched_at": datetime.now().isoformat()
            }
            
            log.warning(f"‚ö†Ô∏è NBP API unavailable, using fallback rate: {fallback_result['usd_pln_rate']} PLN")
            return fallback_result
    
    async def _consolidate_daily_usage(self, processing_date: date) -> Dict[str, Any]:
        """
        Consolidate and validate daily usage data for the given date
        1. Migrate data from client_live_counters to client_daily_usage
        2. Validate and correct existing records
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Get all client organizations
            cursor.execute("SELECT id, name, markup_rate FROM client_organizations WHERE is_active = 1")
            clients = cursor.fetchall()
            
            consolidation_stats = {
                "processing_date": processing_date.isoformat(),
                "clients_processed": 0,
                "total_records_verified": 0,
                "data_corrections": 0,
                "records_migrated": 0,
                "clients_data": []
            }
            
            for client_id, client_name, markup_rate in clients:
                # STEP 1: Migrate data from client_live_counters to client_daily_usage
                cursor.execute("""
                    SELECT current_date, today_tokens, today_requests, today_raw_cost, today_markup_cost, last_updated
                    FROM client_live_counters 
                    WHERE client_org_id = ? AND current_date = ?
                """, (client_id, processing_date))
                
                live_data = cursor.fetchone()
                
                if live_data:
                    # Found live data for processing date
                    live_date, live_tokens, live_requests, live_raw_cost, live_markup_cost, last_updated = live_data
                    
                    # Check if record already exists in client_daily_usage
                    daily_record_id = f"{client_id}:{processing_date}"
                    cursor.execute("""
                        SELECT total_tokens, total_requests, raw_cost, markup_cost 
                        FROM client_daily_usage 
                        WHERE id = ?
                    """, (daily_record_id,))
                    
                    existing_record = cursor.fetchone()
                    
                    if not existing_record:
                        # Migrate live data to daily usage table with duplicate prevention
                        # Calculate proper markup cost if needed
                        if live_markup_cost == 0.0 and live_raw_cost > 0.0:
                            calculated_markup_cost = live_raw_cost * markup_rate
                        else:
                            calculated_markup_cost = live_markup_cost
                        
                        # Use INSERT OR IGNORE to prevent duplicates in production
                        cursor.execute("""
                            INSERT OR IGNORE INTO client_daily_usage 
                            (id, client_org_id, usage_date, total_tokens, total_requests, 
                             raw_cost, markup_cost, primary_model, unique_users, created_at, updated_at)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            daily_record_id, client_id, processing_date, 
                            live_tokens, live_requests, live_raw_cost, calculated_markup_cost,
                            'unknown',  # We'll improve model tracking later
                            1,  # Default unique users
                            int(time.time()), int(time.time())
                        ))
                        
                        # Check if record was actually inserted (not ignored due to duplicate)
                        if cursor.rowcount > 0:
                            consolidation_stats["records_migrated"] += 1
                            log.info(f"üì¶ Migrated live data for {client_name}: {live_tokens} tokens, {live_requests} requests, ${calculated_markup_cost:.6f}")
                        else:
                            log.info(f"‚ö†Ô∏è Record already exists for {client_name} on {processing_date} - skipped to prevent duplicate")
                    else:
                        log.info(f"üìã Record already exists for {client_name} on {processing_date} - validating only")
                else:
                    # No live data found for processing date - this is normal in production
                    log.info(f"üìÖ No live data found for {client_name} on {processing_date} - client had no usage that day")
                
                # STEP 2: Validate and correct existing records
                # Get daily usage for this client and date
                cursor.execute("""
                    SELECT total_tokens, total_requests, raw_cost, markup_cost, primary_model
                    FROM client_daily_usage 
                    WHERE client_org_id = ? AND usage_date = ?
                """, (client_id, processing_date))
                
                daily_data = cursor.fetchone()
                
                if daily_data:
                    tokens, requests, raw_cost, markup_cost, primary_model = daily_data
                    
                    # Validate markup cost calculation
                    expected_markup_cost = raw_cost * markup_rate
                    cost_diff = abs(markup_cost - expected_markup_cost)
                    
                    client_stats = {
                        "client_id": client_id,
                        "client_name": client_name,
                        "tokens": tokens,
                        "requests": requests,
                        "raw_cost": raw_cost,
                        "markup_cost": markup_cost,
                        "markup_rate": markup_rate,
                        "primary_model": primary_model,
                        "data_validated": True
                    }
                    
                    # Correct markup cost if there's a significant difference (> 0.001)
                    if cost_diff > 0.001:
                        cursor.execute("""
                            UPDATE client_daily_usage 
                            SET markup_cost = ?, updated_at = ?
                            WHERE client_org_id = ? AND usage_date = ?
                        """, (expected_markup_cost, int(time.time()), client_id, processing_date))
                        
                        client_stats["markup_cost_corrected"] = {
                            "old_value": markup_cost,
                            "new_value": expected_markup_cost,
                            "difference": cost_diff
                        }
                        consolidation_stats["data_corrections"] += 1
                        
                        log.info(f"üí∞ Corrected markup cost for {client_name}: ${markup_cost:.6f} ‚Üí ${expected_markup_cost:.6f}")
                    
                    consolidation_stats["clients_data"].append(client_stats)
                    consolidation_stats["total_records_verified"] += 1
                
                consolidation_stats["clients_processed"] += 1
            
            conn.commit()
            conn.close()
            
            log.info(f"üìä Daily consolidation completed: {consolidation_stats['clients_processed']} clients, "
                    f"{consolidation_stats['records_migrated']} records migrated, "
                    f"{consolidation_stats['total_records_verified']} records verified, "
                    f"{consolidation_stats['data_corrections']} corrections made")
            
            return {
                "success": True,
                **consolidation_stats
            }
            
        except Exception as e:
            log.error(f"‚ùå Daily consolidation failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_date": processing_date.isoformat()
            }
    
    async def _update_monthly_totals(self, current_date: date) -> Dict[str, Any]:
        """
        Update cumulative monthly totals from 1st day to current day
        Provides business-focused monthly overview data
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Get current month boundaries
            month_start = current_date.replace(day=1)
            
            # Get all active clients
            cursor.execute("SELECT id, name FROM client_organizations WHERE is_active = 1")
            clients = cursor.fetchall()
            
            monthly_stats = {
                "processing_date": current_date.isoformat(),
                "month_range": f"{month_start.isoformat()} to {current_date.isoformat()}",
                "clients_processed": 0,
                "monthly_summaries": []
            }
            
            for client_id, client_name in clients:
                # Calculate monthly totals (1st to current day)
                cursor.execute("""
                    SELECT 
                        COUNT(*) as days_with_usage,
                        SUM(total_tokens) as total_tokens,
                        SUM(total_requests) as total_requests,
                        SUM(raw_cost) as total_raw_cost,
                        SUM(markup_cost) as total_markup_cost,
                        AVG(total_tokens) as avg_daily_tokens,
                        MAX(total_tokens) as max_daily_tokens,
                        MAX(usage_date) as last_usage_date
                    FROM client_daily_usage 
                    WHERE client_org_id = ? 
                    AND usage_date >= ? 
                    AND usage_date <= ?
                """, (client_id, month_start, current_date))
                
                monthly_data = cursor.fetchone()
                
                if monthly_data and monthly_data[0] > 0:  # Has usage data
                    days_with_usage, total_tokens, total_requests, total_raw_cost, total_markup_cost, avg_daily, max_daily, last_usage = monthly_data
                    
                    # Calculate usage percentage
                    days_in_month = current_date.day
                    usage_percentage = (days_with_usage / days_in_month * 100) if days_in_month > 0 else 0
                    
                    # Get most used model
                    cursor.execute("""
                        SELECT primary_model, SUM(total_tokens) as model_tokens
                        FROM client_daily_usage 
                        WHERE client_org_id = ? 
                        AND usage_date >= ? 
                        AND usage_date <= ?
                        AND primary_model IS NOT NULL
                        GROUP BY primary_model
                        ORDER BY model_tokens DESC
                        LIMIT 1
                    """, (client_id, month_start, current_date))
                    
                    most_used_result = cursor.fetchone()
                    most_used_model = most_used_result[0] if most_used_result else None
                    
                    client_monthly_summary = {
                        "client_id": client_id,
                        "client_name": client_name,
                        "days_with_usage": days_with_usage,
                        "days_in_month": days_in_month,
                        "usage_percentage": round(usage_percentage, 1),
                        "total_tokens": total_tokens or 0,
                        "total_requests": total_requests or 0,
                        "total_raw_cost": total_raw_cost or 0.0,
                        "total_markup_cost": total_markup_cost or 0.0,
                        "average_daily_tokens": round(avg_daily or 0),
                        "max_daily_tokens": max_daily or 0,
                        "most_used_model": most_used_model,
                        "last_usage_date": last_usage
                    }
                    
                    monthly_stats["monthly_summaries"].append(client_monthly_summary)
                    
                    log.debug(f"üìà Monthly summary for {client_name}: {total_tokens:,} tokens, "
                             f"{days_with_usage}/{days_in_month} days ({usage_percentage:.1f}%)")
                
                monthly_stats["clients_processed"] += 1
            
            conn.close()
            
            # Calculate overall totals
            total_tokens_all = sum(s["total_tokens"] for s in monthly_stats["monthly_summaries"])
            total_cost_all = sum(s["total_markup_cost"] for s in monthly_stats["monthly_summaries"])
            
            log.info(f"üìä Monthly totals updated: {len(monthly_stats['monthly_summaries'])} active clients, "
                    f"{total_tokens_all:,} total tokens, ${total_cost_all:.6f} total cost")
            
            return {
                "success": True,
                **monthly_stats,
                "overall_totals": {
                    "total_tokens": total_tokens_all,
                    "total_markup_cost": total_cost_all,
                    "active_clients": len(monthly_stats["monthly_summaries"])
                }
            }
            
        except Exception as e:
            log.error(f"‚ùå Monthly totals update failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_date": current_date.isoformat()
            }
    
    async def _cleanup_old_data(self) -> Dict[str, Any]:
        """Clean up old processed generation records to prevent database bloat"""
        try:
            from open_webui.models.organization_usage import ProcessedGenerationDB
            
            # Clean up records older than 90 days
            cleanup_result = ProcessedGenerationDB.cleanup_old_processed_generations(days_to_keep=90)
            
            if cleanup_result["success"]:
                log.info(f"üßπ Data cleanup completed: {cleanup_result['records_deleted']} old records removed, "
                        f"~{cleanup_result['storage_saved_kb']}KB saved")
            
            return cleanup_result
            
        except Exception as e:
            log.error(f"‚ùå Data cleanup failed: {e}")
            return {
                "success": False,
                "error": str(e)
            }


async def run_daily_batch() -> Dict[str, Any]:
    """
    Entry point for daily batch processing
    Can be called from cron job or scheduler
    """
    processor = DailyBatchProcessor()
    return await processor.run_daily_batch()


if __name__ == "__main__":
    """
    Direct execution for testing or manual runs
    Usage: python daily_batch_processor.py
    """
    import sys
    
    print("üï∞Ô∏è mAI Daily Batch Processor")
    print("=" * 40)
    print(f"Starting batch processing at {datetime.now().isoformat()}")
    
    try:
        result = asyncio.run(run_daily_batch())
        
        if result["success"]:
            print(f"‚úÖ Batch processing completed successfully in {result['batch_duration_seconds']}s")
            print(f"üìä Operations completed: {len(result['operations'])}")
            
            for op in result["operations"]:
                status = "‚úÖ" if op["success"] else "‚ùå"
                print(f"  {status} {op['operation']}")
        else:
            print(f"‚ùå Batch processing failed: {result.get('error', 'Unknown error')}")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Batch processing interrupted by user")
        sys.exit(0)
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        sys.exit(1)