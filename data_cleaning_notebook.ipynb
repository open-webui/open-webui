{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Dump Transformation and Cleaning\n",
    "\n",
    "This notebook transforms the PostgreSQL dump file into cleaned pandas DataFrames for analysis.\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Locate and Verify Dump File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dump file\n",
    "dump_files = [\n",
    "    Path.home() / \"Downloads\" / \"b078-20260113-215725.dump\",\n",
    "    Path(\"/workspace\") / \"heroku_psql_181025.dump\",\n",
    "    Path(\"./heroku_psql_181025.dump\"),\n",
    "    Path(\"./b078-20260113-215725.dump\")\n",
    "]\n",
    "\n",
    "dump_file = None\n",
    "for df in dump_files:\n",
    "    if df.exists():\n",
    "        dump_file = df\n",
    "        break\n",
    "\n",
    "if dump_file:\n",
    "    print(f\"Found dump file: {dump_file}\")\n",
    "    print(f\"File size: {dump_file.stat().st_size / 1024:.2f} KB\")\n",
    "    \n",
    "    # Check file format\n",
    "    with open(dump_file, 'rb') as f:\n",
    "        header = f.read(5)\n",
    "        if header == b'PGDMP':\n",
    "            print(\"Format: PostgreSQL custom format (requires pg_restore)\")\n",
    "            file_format = 'custom'\n",
    "        else:\n",
    "            print(\"Format: Plain SQL\")\n",
    "            file_format = 'sql'\n",
    "else:\n",
    "    print(\"ERROR: Could not find dump file.\")\n",
    "    print(\"Please ensure the dump file is in one of these locations:\")\n",
    "    for df in dump_files:\n",
    "        print(f\"  - {df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert Dump to SQL (if needed)\n",
    "\n",
    "If the dump is in custom format, we need to convert it to SQL first. This requires `pg_restore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "if file_format == 'custom':\n",
    "    print(\"Converting custom format dump to SQL...\")\n",
    "    \n",
    "    # Check if pg_restore is available\n",
    "    try:\n",
    "        result = subprocess.run(['pg_restore', '--version'], \n",
    "                             capture_output=True, check=True)\n",
    "        print(f\"pg_restore found: {result.stdout.decode().strip()}\")\n",
    "        \n",
    "        # Convert to SQL\n",
    "        sql_file = dump_file.with_suffix('.sql')\n",
    "        print(f\"Converting to: {sql_file}\")\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            ['pg_restore', '--no-owner', '--no-privileges', '-f', str(sql_file), str(dump_file)],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"✓ Conversion successful!\")\n",
    "            dump_file = sql_file\n",
    "            file_format = 'sql'\n",
    "        else:\n",
    "            print(f\"✗ Conversion failed: {result.stderr}\")\n",
    "            print(\"\\nPlease convert manually:\")\n",
    "            print(f\"  pg_restore -f dump.sql {dump_file}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ pg_restore not found.\")\n",
    "        print(\"\\nPlease install PostgreSQL client tools:\")\n",
    "        print(\"  macOS: brew install postgresql\")\n",
    "        print(\"  Ubuntu: sudo apt-get install postgresql-client\")\n",
    "        print(\"\\nOr convert manually:\")\n",
    "        print(f\"  pg_restore -f dump.sql {dump_file}\")\n",
    "        print(\"\\nThen update dump_file variable to point to the SQL file.\")\n",
    "else:\n",
    "    print(\"Dump is already in SQL format, no conversion needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Parse SQL Dump and Extract Tables\n",
    "\n",
    "We'll parse the SQL dump to extract table data into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sql_dump(sql_file_path):\n",
    "    \"\"\"Parse SQL dump file and extract table data.\"\"\"\n",
    "    print(f\"Reading SQL file: {sql_file_path}\")\n",
    "    \n",
    "    with open(sql_file_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Try to decode\n",
    "    try:\n",
    "        text_content = content.decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Warning: UTF-8 decode failed, trying latin-1...\")\n",
    "        text_content = content.decode('latin-1', errors='ignore')\n",
    "    \n",
    "    tables_data = {}\n",
    "    \n",
    "    # Find all COPY statements\n",
    "    copy_pattern = r'COPY \"public\"\\.\"(\\w+)\"\\s*\\(([^)]+)\\)\\s+FROM stdin;'\n",
    "    copy_matches = list(re.finditer(copy_pattern, text_content, re.MULTILINE))\n",
    "    \n",
    "    print(f\"Found {len(copy_matches)} COPY statements\")\n",
    "    \n",
    "    for i, copy_match in enumerate(copy_matches):\n",
    "        table_name = copy_match.group(1)\n",
    "        columns_str = copy_match.group(2)\n",
    "        \n",
    "        # Parse column names\n",
    "        columns = [col.strip().strip('\"') for col in columns_str.split(',')]\n",
    "        \n",
    "        # Find data section\n",
    "        start_pos = copy_match.end()\n",
    "        \n",
    "        # Find end marker\n",
    "        if i + 1 < len(copy_matches):\n",
    "            end_pos = copy_matches[i + 1].start()\n",
    "        else:\n",
    "            end_marker = text_content.find('\\\\.', start_pos)\n",
    "            end_pos = end_marker if end_marker != -1 else len(text_content)\n",
    "        \n",
    "        data_section = text_content[start_pos:end_pos]\n",
    "        \n",
    "        # Parse tab-separated values\n",
    "        rows = []\n",
    "        lines = data_section.strip().split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('\\\\'):\n",
    "                continue\n",
    "            \n",
    "            # Split by tab\n",
    "            values = line.split('\\t')\n",
    "            \n",
    "            if len(values) != len(columns):\n",
    "                continue\n",
    "            \n",
    "            row = {}\n",
    "            for col, val in zip(columns, values):\n",
    "                # Handle NULL\n",
    "                if val == '\\\\N':\n",
    "                    row[col] = None\n",
    "                else:\n",
    "                    # Try to parse JSON\n",
    "                    if val.startswith('{') or val.startswith('['):\n",
    "                        try:\n",
    "                            row[col] = json.loads(val)\n",
    "                        except:\n",
    "                            row[col] = val\n",
    "                    else:\n",
    "                        row[col] = val\n",
    "            \n",
    "            rows.append(row)\n",
    "        \n",
    "        if rows:\n",
    "            tables_data[table_name] = pd.DataFrame(rows)\n",
    "            print(f\"  ✓ {table_name}: {len(rows)} rows\")\n",
    "    \n",
    "    return tables_data\n",
    "\n",
    "# Parse the dump\n",
    "if file_format == 'sql' and dump_file:\n",
    "    raw_dataframes = parse_sql_dump(dump_file)\n",
    "    print(f\"\\nTotal tables extracted: {len(raw_dataframes)}\")\n",
    "    print(f\"Tables: {', '.join(sorted(raw_dataframes.keys()))}\")\n",
    "else:\n",
    "    print(\"Cannot proceed: dump file not in SQL format.\")\n",
    "    raw_dataframes = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Relevant Tables\n",
    "\n",
    "We'll focus on these tables for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANT_TABLES = [\n",
    "    'user',\n",
    "    'chat',\n",
    "    'message',\n",
    "    'child_profile',\n",
    "    'selection',\n",
    "    'moderation_scenario',\n",
    "    'moderation_session',\n",
    "    'moderation_applied',\n",
    "    'moderation_question_answer',\n",
    "    'exit_quiz_response',\n",
    "    'scenario_assignments',\n",
    "    'scenarios',\n",
    "    'attention_check_scenarios',\n",
    "    'assignment_session_activity',\n",
    "]\n",
    "\n",
    "print(\"Relevant tables for analysis:\")\n",
    "for table in RELEVANT_TABLES:\n",
    "    status = \"✓\" if table in raw_dataframes else \"✗\"\n",
    "    count = len(raw_dataframes[table]) if table in raw_dataframes else 0\n",
    "    print(f\"  {status} {table}: {count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Clean and Transform Data\n",
    "\n",
    "Now we'll clean and transform each relevant table systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Helper Functions for Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamps(df, timestamp_cols=None):\n",
    "    \"\"\"Convert timestamp columns to datetime.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if timestamp_cols is None:\n",
    "        # Auto-detect timestamp columns\n",
    "        timestamp_cols = [col for col in df.columns \n",
    "                         if 'at' in col.lower() or 'time' in col.lower()]\n",
    "    \n",
    "    for col in timestamp_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Try numeric conversion first\n",
    "            if df[col].dtype in ['int64', 'float64']:\n",
    "                sample_val = df[col].dropna()\n",
    "                if len(sample_val) > 0:\n",
    "                    val = sample_val.iloc[0]\n",
    "                    # Determine unit based on magnitude\n",
    "                    if val > 1e12:\n",
    "                        # Nanoseconds\n",
    "                        df[f'{col}_datetime'] = pd.to_datetime(df[col] / 1e9, unit='s', errors='coerce')\n",
    "                    elif val > 1e9:\n",
    "                        # Milliseconds\n",
    "                        df[f'{col}_datetime'] = pd.to_datetime(df[col], unit='ms', errors='coerce')\n",
    "                    else:\n",
    "                        # Seconds\n",
    "                        df[f'{col}_datetime'] = pd.to_datetime(df[col], unit='s', errors='coerce')\n",
    "            else:\n",
    "                # Try direct datetime conversion\n",
    "                df[f'{col}_datetime'] = pd.to_datetime(df[col], errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not convert {col}: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_strings(df):\n",
    "    \"\"\"Clean string columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Convert to string, handle None\n",
    "            df[col] = df[col].astype(str).replace('None', None).replace('nan', None)\n",
    "            # Remove null bytes\n",
    "            df[col] = df[col].str.replace('\\x00', '', regex=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def parse_json_column(df, col_name, new_col_name=None):\n",
    "    \"\"\"Parse JSON column into Python dict/list.\"\"\"\n",
    "    if new_col_name is None:\n",
    "        new_col_name = f'{col_name}_parsed'\n",
    "    \n",
    "    if col_name not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    def parse_json(val):\n",
    "        if isinstance(val, (dict, list)):\n",
    "            return val\n",
    "        if isinstance(val, str):\n",
    "            if val.startswith('{') or val.startswith('['):\n",
    "                try:\n",
    "                    return json.loads(val)\n",
    "                except:\n",
    "                    return None\n",
    "        return None\n",
    "    \n",
    "    df[new_col_name] = df[col_name].apply(parse_json)\n",
    "    return df\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2: Clean User Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'user' in raw_dataframes:\n",
    "    print(\"Cleaning user table...\")\n",
    "    df_users = raw_dataframes['user'].copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_users = clean_strings(df_users)\n",
    "    df_users = convert_timestamps(df_users)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    df_users = parse_json_column(df_users, 'info')\n",
    "    df_users = parse_json_column(df_users, 'settings')\n",
    "    \n",
    "    # Convert date_of_birth if present\n",
    "    if 'date_of_birth' in df_users.columns:\n",
    "        df_users['date_of_birth'] = pd.to_datetime(df_users['date_of_birth'], errors='coerce')\n",
    "    \n",
    "    print(f\"  ✓ Cleaned user table: {len(df_users)} rows, {len(df_users.columns)} columns\")\n",
    "    print(f\"  Columns: {', '.join(df_users.columns[:10])}...\" if len(df_users.columns) > 10 else f\"  Columns: {', '.join(df_users.columns)}\")\n",
    "else:\n",
    "    print(\"✗ User table not found\")\n",
    "    df_users = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3: Clean Chat Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'chat' in raw_dataframes:\n",
    "    print(\"Cleaning chat table...\")\n",
    "    df_chats = raw_dataframes['chat'].copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_chats = clean_strings(df_chats)\n",
    "    df_chats = convert_timestamps(df_chats)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    df_chats = parse_json_column(df_chats, 'chat')\n",
    "    df_chats = parse_json_column(df_chats, 'meta')\n",
    "    \n",
    "    # Extract message count from chat JSON\n",
    "    if 'chat_parsed' in df_chats.columns:\n",
    "        def count_messages(chat_data):\n",
    "            if isinstance(chat_data, dict):\n",
    "                history = chat_data.get('history', {})\n",
    "                messages = history.get('messages', {})\n",
    "                if isinstance(messages, dict):\n",
    "                    return len(messages)\n",
    "            return 0\n",
    "        \n",
    "        df_chats['message_count'] = df_chats['chat_parsed'].apply(count_messages)\n",
    "    \n",
    "    # Convert boolean columns\n",
    "    for col in ['archived', 'pinned']:\n",
    "        if col in df_chats.columns:\n",
    "            df_chats[col] = df_chats[col].astype(str).str.lower() == 'true'\n",
    "    \n",
    "    print(f\"  ✓ Cleaned chat table: {len(df_chats)} rows, {len(df_chats.columns)} columns\")\n",
    "    print(f\"  Total messages across all chats: {df_chats['message_count'].sum() if 'message_count' in df_chats.columns else 'N/A'}\")\n",
    "else:\n",
    "    print(\"✗ Chat table not found\")\n",
    "    df_chats = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4: Clean Message Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'message' in raw_dataframes:\n",
    "    print(\"Cleaning message table...\")\n",
    "    df_messages = raw_dataframes['message'].copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_messages = clean_strings(df_messages)\n",
    "    df_messages = convert_timestamps(df_messages)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    df_messages = parse_json_column(df_messages, 'data')\n",
    "    df_messages = parse_json_column(df_messages, 'meta')\n",
    "    \n",
    "    print(f\"  ✓ Cleaned message table: {len(df_messages)} rows, {len(df_messages.columns)} columns\")\n",
    "    if 'role' in df_messages.columns:\n",
    "        print(f\"  Messages by role:\")\n",
    "        print(df_messages['role'].value_counts().to_string())\n",
    "else:\n",
    "    print(\"✗ Message table not found\")\n",
    "    df_messages = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5: Clean Child Profile Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'child_profile' in raw_dataframes:\n",
    "    print(\"Cleaning child_profile table...\")\n",
    "    df_child_profiles = raw_dataframes['child_profile'].copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_child_profiles = clean_strings(df_child_profiles)\n",
    "    df_child_profiles = convert_timestamps(df_child_profiles)\n",
    "    \n",
    "    # Parse JSON fields if any\n",
    "    for col in df_child_profiles.columns:\n",
    "        if df_child_profiles[col].dtype == 'object':\n",
    "            sample = df_child_profiles[col].dropna().astype(str)\n",
    "            if len(sample) > 0 and sample.str.startswith('{').any():\n",
    "                df_child_profiles = parse_json_column(df_child_profiles, col)\n",
    "    \n",
    "    # Convert boolean columns\n",
    "    for col in ['is_current', 'is_only_child']:\n",
    "        if col in df_child_profiles.columns:\n",
    "            df_child_profiles[col] = df_child_profiles[col].astype(str).str.lower() == 'true'\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    for col in ['attempt_number', 'session_number']:\n",
    "        if col in df_child_profiles.columns:\n",
    "            df_child_profiles[col] = pd.to_numeric(df_child_profiles[col], errors='coerce')\n",
    "    \n",
    "    print(f\"  ✓ Cleaned child_profile table: {len(df_child_profiles)} rows, {len(df_child_profiles.columns)} columns\")\n",
    "    print(f\"  Unique users: {df_child_profiles['user_id'].nunique() if 'user_id' in df_child_profiles.columns else 'N/A'}\")\n",
    "else:\n",
    "    print(\"✗ Child profile table not found\")\n",
    "    df_child_profiles = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6: Clean Selection Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'selection' in raw_dataframes:\n",
    "    print(\"Cleaning selection table...\")\n",
    "    df_selections = raw_dataframes['selection'].copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_selections = clean_strings(df_selections)\n",
    "    df_selections = convert_timestamps(df_selections)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    df_selections = parse_json_column(df_selections, 'meta')\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    for col in ['start_offset', 'end_offset']:\n",
    "        if col in df_selections.columns:\n",
    "            df_selections[col] = pd.to_numeric(df_selections[col], errors='coerce')\n",
    "    \n",
    "    print(f\"  ✓ Cleaned selection table: {len(df_selections)} rows, {len(df_selections.columns)} columns\")\n",
    "    if 'role' in df_selections.columns:\n",
    "        print(f\"  Selections by role:\")\n",
    "        print(df_selections['role'].value_counts().to_string())\n",
    "    if 'source' in df_selections.columns:\n",
    "        print(f\"  Selections by source:\")\n",
    "        print(df_selections['source'].value_counts().to_string())\n",
    "else:\n",
    "    print(\"✗ Selection table not found\")\n",
    "    df_selections = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Clean Moderation Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moderation Scenario\n",
    "if 'moderation_scenario' in raw_dataframes:\n",
    "    print(\"Cleaning moderation_scenario table...\")\n",
    "    df_mod_scenarios = raw_dataframes['moderation_scenario'].copy()\n",
    "    df_mod_scenarios = clean_strings(df_mod_scenarios)\n",
    "    df_mod_scenarios = convert_timestamps(df_mod_scenarios)\n",
    "    \n",
    "    # Convert boolean columns\n",
    "    for col in ['is_applicable']:\n",
    "        if col in df_mod_scenarios.columns:\n",
    "            df_mod_scenarios[col] = df_mod_scenarios[col].astype(str).str.lower() == 'true'\n",
    "    \n",
    "    print(f\"  ✓ Cleaned moderation_scenario: {len(df_mod_scenarios)} rows\")\n",
    "else:\n",
    "    df_mod_scenarios = pd.DataFrame()\n",
    "\n",
    "# Moderation Session\n",
    "if 'moderation_session' in raw_dataframes:\n",
    "    print(\"Cleaning moderation_session table...\")\n",
    "    df_mod_sessions = raw_dataframes['moderation_session'].copy()\n",
    "    df_mod_sessions = clean_strings(df_mod_sessions)\n",
    "    df_mod_sessions = convert_timestamps(df_mod_sessions)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    json_cols = ['strategies', 'custom_instructions', 'highlighted_texts', \n",
    "                 'refactored_response', 'session_metadata']\n",
    "    for col in json_cols:\n",
    "        if col in df_mod_sessions.columns:\n",
    "            df_mod_sessions = parse_json_column(df_mod_sessions, col)\n",
    "    \n",
    "    # Convert boolean and numeric columns\n",
    "    if 'is_final_version' in df_mod_sessions.columns:\n",
    "        df_mod_sessions['is_final_version'] = df_mod_sessions['is_final_version'].astype(str).str.lower() == 'true'\n",
    "    for col in ['scenario_index', 'attempt_number', 'version_number']:\n",
    "        if col in df_mod_sessions.columns:\n",
    "            df_mod_sessions[col] = pd.to_numeric(df_mod_sessions[col], errors='coerce')\n",
    "    \n",
    "    print(f\"  ✓ Cleaned moderation_session: {len(df_mod_sessions)} rows\")\n",
    "else:\n",
    "    df_mod_sessions = pd.DataFrame()\n",
    "\n",
    "# Moderation Applied\n",
    "if 'moderation_applied' in raw_dataframes:\n",
    "    print(\"Cleaning moderation_applied table...\")\n",
    "    df_mod_applied = raw_dataframes['moderation_applied'].copy()\n",
    "    df_mod_applied = clean_strings(df_mod_applied)\n",
    "    df_mod_applied = convert_timestamps(df_mod_applied)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    json_cols = ['strategies', 'custom_instructions', 'highlighted_texts', 'refactored_response']\n",
    "    for col in json_cols:\n",
    "        if col in df_mod_applied.columns:\n",
    "            df_mod_applied = parse_json_column(df_mod_applied, col)\n",
    "    \n",
    "    if 'confirmed_preferred' in df_mod_applied.columns:\n",
    "        df_mod_applied['confirmed_preferred'] = df_mod_applied['confirmed_preferred'].astype(str).str.lower() == 'true'\n",
    "    if 'version_index' in df_mod_applied.columns:\n",
    "        df_mod_applied['version_index'] = pd.to_numeric(df_mod_applied['version_index'], errors='coerce')\n",
    "    \n",
    "    print(f\"  ✓ Cleaned moderation_applied: {len(df_mod_applied)} rows\")\n",
    "else:\n",
    "    df_mod_applied = pd.DataFrame()\n",
    "\n",
    "# Moderation Question Answer\n",
    "if 'moderation_question_answer' in raw_dataframes:\n",
    "    print(\"Cleaning moderation_question_answer table...\")\n",
    "    df_mod_qa = raw_dataframes['moderation_question_answer'].copy()\n",
    "    df_mod_qa = clean_strings(df_mod_qa)\n",
    "    df_mod_qa = convert_timestamps(df_mod_qa)\n",
    "    print(f\"  ✓ Cleaned moderation_question_answer: {len(df_mod_qa)} rows\")\n",
    "else:\n",
    "    df_mod_qa = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8: Clean Exit Quiz Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'exit_quiz_response' in raw_dataframes:\n",
    "    print(\"Cleaning exit_quiz_response table...\")\n",
    "    df_exit_quiz = raw_dataframes['exit_quiz_response'].copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_exit_quiz = clean_strings(df_exit_quiz)\n",
    "    df_exit_quiz = convert_timestamps(df_exit_quiz)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    df_exit_quiz = parse_json_column(df_exit_quiz, 'answers')\n",
    "    df_exit_quiz = parse_json_column(df_exit_quiz, 'score')\n",
    "    df_exit_quiz = parse_json_column(df_exit_quiz, 'meta')\n",
    "    \n",
    "    # Convert boolean and numeric columns\n",
    "    if 'is_current' in df_exit_quiz.columns:\n",
    "        df_exit_quiz['is_current'] = df_exit_quiz['is_current'].astype(str).str.lower() == 'true'\n",
    "    if 'attempt_number' in df_exit_quiz.columns:\n",
    "        df_exit_quiz['attempt_number'] = pd.to_numeric(df_exit_quiz['attempt_number'], errors='coerce')\n",
    "    \n",
    "    print(f\"  ✓ Cleaned exit_quiz_response table: {len(df_exit_quiz)} rows, {len(df_exit_quiz.columns)} columns\")\n",
    "else:\n",
    "    print(\"✗ Exit quiz table not found\")\n",
    "    df_exit_quiz = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9: Clean Scenario Tables (if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario Assignments\n",
    "if 'scenario_assignments' in raw_dataframes:\n",
    "    print(\"Cleaning scenario_assignments table...\")\n",
    "    df_scenario_assignments = raw_dataframes['scenario_assignments'].copy()\n",
    "    df_scenario_assignments = clean_strings(df_scenario_assignments)\n",
    "    df_scenario_assignments = convert_timestamps(df_scenario_assignments)\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['alpha', 'eligible_pool_size', 'n_assigned_before', 'weight', \n",
    "                   'sampling_prob', 'assignment_position', 'issue_any', 'duration_seconds']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_scenario_assignments.columns:\n",
    "            df_scenario_assignments[col] = pd.to_numeric(df_scenario_assignments[col], errors='coerce')\n",
    "    \n",
    "    print(f\"  ✓ Cleaned scenario_assignments: {len(df_scenario_assignments)} rows\")\n",
    "else:\n",
    "    df_scenario_assignments = pd.DataFrame()\n",
    "\n",
    "# Scenarios\n",
    "if 'scenarios' in raw_dataframes:\n",
    "    print(\"Cleaning scenarios table...\")\n",
    "    df_scenarios = raw_dataframes['scenarios'].copy()\n",
    "    df_scenarios = clean_strings(df_scenarios)\n",
    "    df_scenarios = convert_timestamps(df_scenarios)\n",
    "    \n",
    "    # Convert boolean and numeric columns\n",
    "    if 'is_active' in df_scenarios.columns:\n",
    "        df_scenarios['is_active'] = df_scenarios['is_active'].astype(str).str.lower() == 'true'\n",
    "    numeric_cols = ['n_assigned', 'n_completed', 'n_skipped', 'n_abandoned']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_scenarios.columns:\n",
    "            df_scenarios[col] = pd.to_numeric(df_scenarios[col], errors='coerce')\n",
    "    \n",
    "    print(f\"  ✓ Cleaned scenarios: {len(df_scenarios)} rows\")\n",
    "else:\n",
    "    df_scenarios = pd.DataFrame()\n",
    "\n",
    "# Attention Check Scenarios\n",
    "if 'attention_check_scenarios' in raw_dataframes:\n",
    "    print(\"Cleaning attention_check_scenarios table...\")\n",
    "    df_attention_checks = raw_dataframes['attention_check_scenarios'].copy()\n",
    "    df_attention_checks = clean_strings(df_attention_checks)\n",
    "    df_attention_checks = convert_timestamps(df_attention_checks)\n",
    "    \n",
    "    if 'is_active' in df_attention_checks.columns:\n",
    "        df_attention_checks['is_active'] = df_attention_checks['is_active'].astype(str).str.lower() == 'true'\n",
    "    \n",
    "    print(f\"  ✓ Cleaned attention_check_scenarios: {len(df_attention_checks)} rows\")\n",
    "else:\n",
    "    df_attention_checks = pd.DataFrame()\n",
    "\n",
    "# Assignment Session Activity\n",
    "if 'assignment_session_activity' in raw_dataframes:\n",
    "    print(\"Cleaning assignment_session_activity table...\")\n",
    "    df_activity = raw_dataframes['assignment_session_activity'].copy()\n",
    "    df_activity = clean_strings(df_activity)\n",
    "    df_activity = convert_timestamps(df_activity)\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['session_number', 'active_ms_delta', 'cumulative_ms']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_activity.columns:\n",
    "            df_activity[col] = pd.to_numeric(df_activity[col], errors='coerce')\n",
    "    \n",
    "    print(f\"  ✓ Cleaned assignment_session_activity: {len(df_activity)} rows\")\n",
    "else:\n",
    "    df_activity = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cleaned_tables = {\n",
    "    'users': df_users if 'df_users' in globals() else pd.DataFrame(),\n",
    "    'chats': df_chats if 'df_chats' in globals() else pd.DataFrame(),\n",
    "    'messages': df_messages if 'df_messages' in globals() else pd.DataFrame(),\n",
    "    'child_profiles': df_child_profiles if 'df_child_profiles' in globals() else pd.DataFrame(),\n",
    "    'selections': df_selections if 'df_selections' in globals() else pd.DataFrame(),\n",
    "    'moderation_scenarios': df_mod_scenarios if 'df_mod_scenarios' in globals() else pd.DataFrame(),\n",
    "    'moderation_sessions': df_mod_sessions if 'df_mod_sessions' in globals() else pd.DataFrame(),\n",
    "    'moderation_applied': df_mod_applied if 'df_mod_applied' in globals() else pd.DataFrame(),\n",
    "    'moderation_qa': df_mod_qa if 'df_mod_qa' in globals() else pd.DataFrame(),\n",
    "    'exit_quiz': df_exit_quiz if 'df_exit_quiz' in globals() else pd.DataFrame(),\n",
    "    'scenario_assignments': df_scenario_assignments if 'df_scenario_assignments' in globals() else pd.DataFrame(),\n",
    "    'scenarios': df_scenarios if 'df_scenarios' in globals() else pd.DataFrame(),\n",
    "    'attention_checks': df_attention_checks if 'df_attention_checks' in globals() else pd.DataFrame(),\n",
    "    'activity': df_activity if 'df_activity' in globals() else pd.DataFrame(),\n",
    "}\n",
    "\n",
    "for table_name, df in cleaned_tables.items():\n",
    "    if len(df) > 0:\n",
    "        print(f\"\\n{table_name.upper()}:\")\n",
    "        print(f\"  Rows: {len(df)}\")\n",
    "        print(f\"  Columns: {len(df.columns)}\")\n",
    "        print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        # Check for nulls\n",
    "        null_counts = df.isnull().sum()\n",
    "        if null_counts.sum() > 0:\n",
    "            print(f\"  Columns with nulls: {len(null_counts[null_counts > 0])}\")\n",
    "            top_nulls = null_counts[null_counts > 0].head(5)\n",
    "            for col, count in top_nulls.items():\n",
    "                pct = (count / len(df)) * 100\n",
    "                print(f\"    - {col}: {count} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n{table_name.upper()}: No data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"data_exports\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Saving cleaned DataFrames to {output_dir}/...\")\n",
    "\n",
    "saved_files = {}\n",
    "for table_name, df in cleaned_tables.items():\n",
    "    if len(df) > 0:\n",
    "        # Save as CSV\n",
    "        csv_file = output_dir / f\"{table_name}.csv\"\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        # Save as pickle (faster loading)\n",
    "        pkl_file = output_dir / f\"{table_name}.pkl\"\n",
    "        df.to_pickle(pkl_file)\n",
    "        \n",
    "        saved_files[table_name] = {\n",
    "            'csv': str(csv_file),\n",
    "            'pkl': str(pkl_file),\n",
    "            'rows': len(df)\n",
    "        }\n",
    "        print(f\"  ✓ {table_name}: {len(df)} rows -> {csv_file.name}, {pkl_file.name}\")\n",
    "\n",
    "print(f\"\\n✓ Saved {len(saved_files)} tables\")\n",
    "\n",
    "# Create summary JSON\n",
    "summary = {\n",
    "    'extraction_date': datetime.now().isoformat(),\n",
    "    'dump_file': str(dump_file) if 'dump_file' in locals() else None,\n",
    "    'tables': {}\n",
    "}\n",
    "\n",
    "for table_name, df in cleaned_tables.items():\n",
    "    if len(df) > 0:\n",
    "        summary['tables'][table_name] = {\n",
    "            'row_count': len(df),\n",
    "            'column_count': len(df.columns),\n",
    "            'columns': list(df.columns),\n",
    "            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "        }\n",
    "\n",
    "summary_file = output_dir / \"summary.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"✓ Summary saved to {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Quick Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data from key tables\n",
    "print(\"Sample data from key tables:\\n\")\n",
    "\n",
    "if len(df_users) > 0:\n",
    "    print(\"USERS (first 3 rows):\")\n",
    "    print(df_users.head(3).to_string())\n",
    "    print(\"\\n\")\n",
    "\n",
    "if len(df_chats) > 0:\n",
    "    print(\"CHATS (first 3 rows):\")\n",
    "    display_cols = ['id', 'user_id', 'title', 'created_at_datetime', 'message_count']\n",
    "    available_cols = [col for col in display_cols if col in df_chats.columns]\n",
    "    print(df_chats[available_cols].head(3).to_string())\n",
    "    print(\"\\n\")\n",
    "\n",
    "if len(df_selections) > 0:\n",
    "    print(\"SELECTIONS (first 3 rows):\")\n",
    "    display_cols = ['id', 'user_id', 'role', 'source', 'created_at_datetime']\n",
    "    available_cols = [col for col in display_cols if col in df_selections.columns]\n",
    "    print(df_selections[available_cols].head(3).to_string())\n",
    "    print(\"\\n\")\n",
    "\n",
    "if len(df_child_profiles) > 0:\n",
    "    print(\"CHILD PROFILES (first 3 rows):\")\n",
    "    display_cols = ['id', 'user_id', 'name', 'child_age', 'child_gender', 'is_current']\n",
    "    available_cols = [col for col in display_cols if col in df_child_profiles.columns]\n",
    "    print(df_child_profiles[available_cols].head(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "All relevant tables have been extracted, cleaned, and saved to the `data_exports/` directory.\n",
    "\n",
    "You can now load the cleaned data using:\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_pickle('data_exports/table_name.pkl')\n",
    "```\n",
    "\n",
    "The data has been:\n",
    "- Parsed from the PostgreSQL dump\n",
    "- Cleaned (null bytes removed, strings normalized)\n",
    "- Transformed (timestamps converted, JSON parsed, types corrected)\n",
    "- Saved in both CSV and pickle formats for easy access"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
