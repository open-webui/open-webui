{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Dump Transformation and Cleaning\n",
    "\n",
    "This notebook transforms the PostgreSQL dump file into cleaned pandas DataFrames for analysis.\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required dependencies\n",
    "# import sys\n",
    "# import subprocess\n",
    "\n",
    "# def install_package(package):\n",
    "#     \"\"\"Install a package using pip.\"\"\"\n",
    "#     try:\n",
    "#         __import__(package)\n",
    "#         print(f\"\u2713 {package} is already installed\")\n",
    "#     except ImportError:\n",
    "#         print(f\"Installing {package}...\")\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "#         print(f\"\u2713 {package} installed successfully\")\n",
    "\n",
    "# # Install required packages\n",
    "# required_packages = [\n",
    "#     \"pandas\",\n",
    "#     \"numpy\",\n",
    "#     \"jupyter\",\n",
    "#     \"ipython\"\n",
    "# ]\n",
    "\n",
    "# print(\"Checking and installing dependencies...\\n\")\n",
    "# for package in required_packages:\n",
    "#     install_package(package)\n",
    "\n",
    "# print(\"\\n\u2713 All dependencies are ready!\")\n",
    "# print(\"\\nNote: If you need to convert PostgreSQL custom format dumps, you'll also need:\")\n",
    "# print(\"  - PostgreSQL client tools (pg_restore)\")\n",
    "# print(\"  - macOS: brew install postgresql\")\n",
    "# print(\"  - Ubuntu: sudo apt-get install postgresql-client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 3.0.0\n",
      "NumPy version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Locate and Verify Dump File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dump file: /Users/johndriscoll/Downloads/b078-20260113-215725.dump\n",
      "File size: 248.83 KB\n",
      "Format: PostgreSQL custom format (requires pg_restore)\n"
     ]
    }
   ],
   "source": [
    "# Find the dump file\n",
    "dump_files = [\n",
    "    Path.home() / \"Downloads\" / \"b078-20260113-215725.dump\",\n",
    "    Path(\"/workspace\") / \"heroku_psql_181025.dump\",\n",
    "    Path(\"./heroku_psql_181025.dump\"),\n",
    "    Path(\"./b078-20260113-215725.dump\")\n",
    "]\n",
    "\n",
    "dump_file = None\n",
    "for df in dump_files:\n",
    "    if df.exists():\n",
    "        dump_file = df\n",
    "        break\n",
    "\n",
    "if dump_file:\n",
    "    print(f\"Found dump file: {dump_file}\")\n",
    "    print(f\"File size: {dump_file.stat().st_size / 1024:.2f} KB\")\n",
    "    \n",
    "    # Check file format\n",
    "    with open(dump_file, 'rb') as f:\n",
    "        header = f.read(5)\n",
    "        if header == b'PGDMP':\n",
    "            print(\"Format: PostgreSQL custom format (requires pg_restore)\")\n",
    "            file_format = 'custom'\n",
    "        else:\n",
    "            print(\"Format: Plain SQL\")\n",
    "            file_format = 'sql'\n",
    "else:\n",
    "    print(\"ERROR: Could not find dump file.\")\n",
    "    print(\"Please ensure the dump file is in one of these locations:\")\n",
    "    for df in dump_files:\n",
    "        print(f\"  - {df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Conversion Instructions\n",
    "\n",
    "If automatic conversion failed, convert the dump file manually using one of these methods:\n",
    "\n",
    "### Method 1: Using PostgreSQL 16 (Recommended)\n",
    "\n",
    "Since PostgreSQL 16 is \"keg-only\" (not in PATH), use the full path:\n",
    "\n",
    "**For Apple Silicon Macs:**\n",
    "```bash\n",
    "/opt/homebrew/opt/postgresql@17/bin/pg_restore --no-owner --no-privileges -f ~/Downloads/b078-20260113-215725.sql ~/Downloads/b078-20260113-215725.dump\n",
    "```\n",
    "\n",
    "**For Intel Macs:**\n",
    "```bash\n",
    "/usr/local/opt/postgresql@16/bin/pg_restore --no-owner --no-privileges -f ~/Downloads/b078-20260113-215725.sql ~/Downloads/b078-20260113-215725.dump\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert Dump to SQL (if needed)\n",
    "\n",
    "This cell will automatically convert the PostgreSQL custom format dump to SQL format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONVERTING CUSTOM FORMAT DUMP TO SQL\n",
      "============================================================\n",
      "\n",
      "\u2713 SQL file already exists: /Users/johndriscoll/Downloads/b078-20260113-215725.sql\n",
      "  File size: 629.43 KB\n",
      "  Using existing file. Skipping conversion.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Check if we need to convert\n",
    "if 'dump_file' in globals() and 'file_format' in globals() and file_format == 'custom':\n",
    "    print(\"=\"*60)\n",
    "    print(\"CONVERTING CUSTOM FORMAT DUMP TO SQL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define output SQL file\n",
    "    sql_file = dump_file.with_suffix('.sql')\n",
    "    \n",
    "    # Check if SQL file already exists\n",
    "    if sql_file.exists():\n",
    "        print(f\"\\n\u2713 SQL file already exists: {sql_file}\")\n",
    "        print(f\"  File size: {sql_file.stat().st_size / 1024:.2f} KB\")\n",
    "        print(\"  Using existing file. Skipping conversion.\")\n",
    "        dump_file = sql_file\n",
    "        file_format = 'sql'\n",
    "    else:\n",
    "        print(f\"\\nInput dump file: {dump_file}\")\n",
    "        print(f\"Output SQL file: {sql_file}\")\n",
    "        \n",
    "        # Try to find pg_restore in various locations\n",
    "        pg_restore_paths = [\n",
    "            # PostgreSQL 17 (newest)\n",
    "            '/opt/homebrew/opt/postgresql@17/bin/pg_restore',\n",
    "            '/usr/local/opt/postgresql@17/bin/pg_restore',\n",
    "            # PostgreSQL 16\n",
    "            '/opt/homebrew/opt/postgresql@16/bin/pg_restore',\n",
    "            '/usr/local/opt/postgresql@16/bin/pg_restore',\n",
    "            # PostgreSQL 15\n",
    "            '/opt/homebrew/opt/postgresql@15/bin/pg_restore',\n",
    "            '/usr/local/opt/postgresql@15/bin/pg_restore',\n",
    "            # Standard locations\n",
    "            '/opt/homebrew/bin/pg_restore',\n",
    "            '/usr/local/bin/pg_restore',\n",
    "            'pg_restore',  # In PATH\n",
    "        ]\n",
    "        \n",
    "        pg_restore_path = None\n",
    "        version_info = None\n",
    "        \n",
    "        print(\"\\nSearching for pg_restore...\")\n",
    "        for path in pg_restore_paths:\n",
    "            try:\n",
    "                if path == 'pg_restore':\n",
    "                    # Check if it's in PATH\n",
    "                    result = subprocess.run(\n",
    "                        ['which', 'pg_restore'],\n",
    "                        capture_output=True,\n",
    "                        text=True\n",
    "                    )\n",
    "                    if result.returncode == 0:\n",
    "                        path = result.stdout.strip()\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                if os.path.exists(path):\n",
    "                    # Check version\n",
    "                    result = subprocess.run(\n",
    "                        [path, '--version'],\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                        check=True\n",
    "                    )\n",
    "                    pg_restore_path = path\n",
    "                    version_info = result.stdout.strip()\n",
    "                    print(f\"\u2713 Found: {version_info}\")\n",
    "                    print(f\"  Location: {path}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if pg_restore_path:\n",
    "            print(f\"\\nConverting dump to SQL...\")\n",
    "            print(f\"  Command: {pg_restore_path} --no-owner --no-privileges -f {sql_file} {dump_file}\")\n",
    "            \n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [pg_restore_path, '--no-owner', '--no-privileges', '-f', str(sql_file), str(dump_file)],\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    timeout=300  # 5 minute timeout\n",
    "                )\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    if sql_file.exists():\n",
    "                        print(f\"\\n\u2713\u2713\u2713 CONVERSION SUCCESSFUL! \u2713\u2713\u2713\")\n",
    "                        print(f\"  Created: {sql_file}\")\n",
    "                        print(f\"  File size: {sql_file.stat().st_size / 1024:.2f} KB\")\n",
    "                        dump_file = sql_file\n",
    "                        file_format = 'sql'\n",
    "                        print(\"\\n\u2713 Ready to proceed with parsing!\")\n",
    "                    else:\n",
    "                        print(f\"\\n\u2717 Conversion reported success but SQL file not found!\")\n",
    "                        print(f\"  Expected location: {sql_file}\")\n",
    "                else:\n",
    "                    error_msg = result.stderr.strip() if result.stderr else result.stdout.strip()\n",
    "                    print(f\"\\n\u2717 CONVERSION FAILED\")\n",
    "                    print(f\"  Error: {error_msg}\")\n",
    "                    \n",
    "                    if 'unsupported version' in error_msg.lower():\n",
    "                        print(\"\\n\" + \"=\"*60)\n",
    "                        print(\"VERSION INCOMPATIBILITY\")\n",
    "                        print(\"=\"*60)\n",
    "                        print(f\"\\nThe dump file format version is not supported by {version_info}.\")\n",
    "                        print(\"\\nSOLUTIONS:\")\n",
    "                        print(\"\\n1. Try a newer PostgreSQL version (17 or 18)\")\n",
    "                        print(\"   brew install postgresql@17\")\n",
    "                        print(f\"   /opt/homebrew/opt/postgresql@17/bin/pg_restore -f {sql_file} {dump_file}\")\n",
    "                        print(\"\\n2. Use Docker:\")\n",
    "                        print(f\"   docker run --rm -v ~/Downloads:/data postgres:17 \\\\\")\n",
    "                        print(f\"     pg_restore -f /data/{sql_file.name} /data/{dump_file.name}\")\n",
    "                        print(\"\\n3. Check if SQL file was already created manually\")\n",
    "                    else:\n",
    "                        print(\"\\nPlease check the error message above and try manual conversion.\")\n",
    "                        print(\"See the manual conversion instructions in the previous cell.\")\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"\\n\u2717 Conversion timed out after 5 minutes.\")\n",
    "                print(\"  The dump file might be very large or there's an issue.\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n\u2717 Error during conversion: {e}\")\n",
    "        else:\n",
    "            print(\"\\n\u2717 pg_restore not found!\")\n",
    "            print(\"\\nPlease install PostgreSQL client tools:\")\n",
    "            print(\"  brew install postgresql@17\")\n",
    "            print(\"\\nOr use Docker (see manual conversion instructions above).\")\n",
    "            print(\"\\nAfter manual conversion, set:\")\n",
    "            print(f\"  dump_file = Path('{sql_file}')\")\n",
    "            print(f\"  file_format = 'sql'\")\n",
    "            print(\"\\nThen proceed to the next cell.\")\n",
    "\n",
    "elif 'dump_file' in globals() and 'file_format' in globals() and file_format == 'sql':\n",
    "    print(\"\u2713 Dump file is already in SQL format.\")\n",
    "    print(f\"  File: {dump_file}\")\n",
    "    if isinstance(dump_file, Path) and dump_file.exists():\n",
    "        print(f\"  File size: {dump_file.stat().st_size / 1024:.2f} KB\")\n",
    "    print(\"\\nReady to proceed with parsing!\")\n",
    "else:\n",
    "    print(\"\u26a0 Cannot convert: dump_file or file_format not set.\")\n",
    "    print(\"Please run the previous cell to locate and detect the dump file format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pg_restore: creating SCHEMA \"public\"\n",
      "pg_restore: creating EXTENSION \"pg_stat_statements\"\n",
      "pg_restore: creating COMMENT \"EXTENSION \"pg_stat_statements\"\"\n",
      "pg_restore: creating TABLE \"public.alembic_version\"\n",
      "pg_restore: creating TABLE \"public.assignment_session_activity\"\n",
      "pg_restore: creating TABLE \"public.attention_check_question\"\n",
      "pg_restore: creating TABLE \"public.attention_check_response\"\n",
      "pg_restore: creating TABLE \"public.attention_check_scenarios\"\n",
      "pg_restore: creating TABLE \"public.auth\"\n",
      "pg_restore: creating TABLE \"public.channel\"\n",
      "pg_restore: creating TABLE \"public.channel_member\"\n",
      "pg_restore: creating TABLE \"public.chat\"\n",
      "pg_restore: creating TABLE \"public.chatidtag\"\n",
      "pg_restore: creating TABLE \"public.child_profile\"\n",
      "pg_restore: creating TABLE \"public.config\"\n",
      "pg_restore: creating SEQUENCE \"public.config_id_seq\"\n",
      "pg_restore: creating SEQUENCE OWNED BY \"public.config_id_seq\"\n",
      "pg_restore: creating TABLE \"public.consent_audit\"\n",
      "pg_restore: creating TABLE \"public.document\"\n",
      "pg_restore: creating TABLE \"public.exit_quiz_response\"\n",
      "pg_restore: creating TABLE \"public.feedback\"\n",
      "pg_restore: creating TABLE \"public.file\"\n",
      "pg_restore: creating TABLE \"public.folder\"\n",
      "pg_restore: creating TABLE \"public.function\"\n",
      "pg_restore: creating TABLE \"public.group\"\n",
      "pg_restore: creating TABLE \"public.knowledge\"\n",
      "pg_restore: creating TABLE \"public.memory\"\n",
      "pg_restore: creating TABLE \"public.message\"\n",
      "pg_restore: creating TABLE \"public.message_reaction\"\n",
      "pg_restore: creating TABLE \"public.model\"\n",
      "pg_restore: creating TABLE \"public.moderation_session\"\n",
      "pg_restore: creating TABLE \"public.moderation_session_activity\"\n",
      "pg_restore: creating TABLE \"public.note\"\n",
      "pg_restore: creating TABLE \"public.oauth_session\"\n",
      "pg_restore: creating TABLE \"public.prompt\"\n",
      "pg_restore: creating TABLE \"public.scenario_assignments\"\n",
      "pg_restore: creating TABLE \"public.scenarios\"\n",
      "pg_restore: creating TABLE \"public.selection\"\n",
      "pg_restore: creating TABLE \"public.tag\"\n",
      "pg_restore: creating TABLE \"public.tool\"\n",
      "pg_restore: creating TABLE \"public.user\"\n",
      "pg_restore: creating DEFAULT \"public.config id\"\n",
      "pg_restore: processing data for table \"public.alembic_version\"\n",
      "pg_restore: processing data for table \"public.assignment_session_activity\"\n",
      "pg_restore: processing data for table \"public.attention_check_question\"\n",
      "pg_restore: processing data for table \"public.attention_check_response\"\n",
      "pg_restore: processing data for table \"public.attention_check_scenarios\"\n",
      "pg_restore: processing data for table \"public.auth\"\n",
      "pg_restore: processing data for table \"public.channel\"\n",
      "pg_restore: processing data for table \"public.channel_member\"\n",
      "pg_restore: processing data for table \"public.chat\"\n",
      "pg_restore: processing data for table \"public.chatidtag\"\n",
      "pg_restore: processing data for table \"public.child_profile\"\n",
      "pg_restore: processing data for table \"public.config\"\n",
      "pg_restore: processing data for table \"public.consent_audit\"\n",
      "pg_restore: processing data for table \"public.document\"\n",
      "pg_restore: processing data for table \"public.exit_quiz_response\"\n",
      "pg_restore: processing data for table \"public.feedback\"\n",
      "pg_restore: processing data for table \"public.file\"\n",
      "pg_restore: processing data for table \"public.folder\"\n",
      "pg_restore: processing data for table \"public.function\"\n",
      "pg_restore: processing data for table \"public.group\"\n",
      "pg_restore: processing data for table \"public.knowledge\"\n",
      "pg_restore: processing data for table \"public.memory\"\n",
      "pg_restore: processing data for table \"public.message\"\n",
      "pg_restore: processing data for table \"public.message_reaction\"\n",
      "pg_restore: processing data for table \"public.model\"\n",
      "pg_restore: processing data for table \"public.moderation_session\"\n",
      "pg_restore: processing data for table \"public.moderation_session_activity\"\n",
      "pg_restore: processing data for table \"public.note\"\n",
      "pg_restore: processing data for table \"public.oauth_session\"\n",
      "pg_restore: processing data for table \"public.prompt\"\n",
      "pg_restore: processing data for table \"public.scenario_assignments\"\n",
      "pg_restore: processing data for table \"public.scenarios\"\n",
      "pg_restore: processing data for table \"public.selection\"\n",
      "pg_restore: processing data for table \"public.tag\"\n",
      "pg_restore: processing data for table \"public.tool\"\n",
      "pg_restore: processing data for table \"public.user\"\n",
      "pg_restore: executing SEQUENCE SET config_id_seq\n",
      "pg_restore: creating CONSTRAINT \"public.alembic_version alembic_version_pkc\"\n",
      "pg_restore: creating CONSTRAINT \"public.assignment_session_activity assignment_session_activity_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.attention_check_question attention_check_question_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.attention_check_response attention_check_response_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.attention_check_scenarios attention_check_scenarios_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.auth auth_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.channel_member channel_member_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.channel channel_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.chat chat_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.chat chat_share_id_key\"\n",
      "pg_restore: creating CONSTRAINT \"public.chatidtag chatidtag_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.child_profile child_profile_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.config config_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.consent_audit consent_audit_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.document document_name_key\"\n",
      "pg_restore: creating CONSTRAINT \"public.document document_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.exit_quiz_response exit_quiz_response_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.feedback feedback_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.file file_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.folder folder_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.function function_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.group group_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.knowledge knowledge_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.memory memory_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.message message_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.message_reaction message_reaction_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.model model_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.moderation_session_activity moderation_session_activity_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.moderation_session moderation_session_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.note note_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.oauth_session oauth_session_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.tag pk_id_user_id\"\n",
      "pg_restore: creating CONSTRAINT \"public.prompt prompt_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.scenario_assignments scenario_assignments_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.scenarios scenarios_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.selection selection_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.tool tool_pkey\"\n",
      "pg_restore: creating CONSTRAINT \"public.user uq_user_prolific_pid\"\n",
      "pg_restore: creating CONSTRAINT \"public.user user_api_key_key\"\n",
      "pg_restore: creating CONSTRAINT \"public.user user_oauth_sub_key\"\n",
      "pg_restore: creating CONSTRAINT \"public.user user_pkey\"\n",
      "pg_restore: creating INDEX \"public.folder_id_idx\"\n",
      "pg_restore: creating INDEX \"public.folder_id_user_id_idx\"\n",
      "pg_restore: creating INDEX \"public.idx_ac_scenarios_is_active\"\n",
      "pg_restore: creating INDEX \"public.idx_ac_scenarios_set_name\"\n",
      "pg_restore: creating INDEX \"public.idx_ac_scenarios_trait_theme\"\n",
      "pg_restore: creating INDEX \"public.idx_acq_created_at\"\n",
      "pg_restore: creating INDEX \"public.idx_acr_created_at\"\n",
      "pg_restore: creating INDEX \"public.idx_acr_question\"\n",
      "pg_restore: creating INDEX \"public.idx_acr_user\"\n",
      "pg_restore: creating INDEX \"public.idx_assignment_activity_created_at\"\n",
      "pg_restore: creating INDEX \"public.idx_assignments_assigned_at\"\n",
      "pg_restore: creating INDEX \"public.idx_assignments_participant_id\"\n",
      "pg_restore: creating INDEX \"public.idx_assignments_participant_scenario\"\n",
      "pg_restore: creating INDEX \"public.idx_assignments_scenario_id\"\n",
      "pg_restore: creating INDEX \"public.idx_assignments_status\"\n",
      "pg_restore: creating INDEX \"public.idx_child_profile_attempt\"\n",
      "pg_restore: creating INDEX \"public.idx_child_profile_created_at\"\n",
      "pg_restore: creating INDEX \"public.idx_child_profile_user_current\"\n",
      "pg_restore: creating INDEX \"public.idx_child_profile_user_id\"\n",
      "pg_restore: creating INDEX \"public.idx_child_profile_user_session_current\"\n",
      "pg_restore: creating INDEX \"public.idx_consent_audit_prolific\"\n",
      "pg_restore: creating INDEX \"public.idx_consent_audit_timestamp\"\n",
      "pg_restore: creating INDEX \"public.idx_consent_audit_user\"\n",
      "pg_restore: creating INDEX \"public.idx_exit_quiz_attempt\"\n",
      "pg_restore: creating INDEX \"public.idx_exit_quiz_child_id\"\n",
      "pg_restore: creating INDEX \"public.idx_exit_quiz_created_at\"\n",
      "pg_restore: creating INDEX \"public.idx_exit_quiz_user_current\"\n",
      "pg_restore: creating INDEX \"public.idx_exit_quiz_user_id\"\n",
      "pg_restore: creating INDEX \"public.idx_mod_activity_created_at\"\n",
      "pg_restore: creating INDEX \"public.idx_mod_activity_user_child_session\"\n",
      "pg_restore: creating INDEX \"public.idx_mod_session_final\"\n",
      "pg_restore: creating INDEX \"public.idx_mod_session_user_session\"\n",
      "pg_restore: creating INDEX \"public.idx_moderation_session_child_id\"\n",
      "pg_restore: creating INDEX \"public.idx_moderation_session_created_at\"\n",
      "pg_restore: creating INDEX \"public.idx_moderation_session_user_id\"\n",
      "pg_restore: creating INDEX \"public.idx_oauth_session_expires_at\"\n",
      "pg_restore: creating INDEX \"public.idx_oauth_session_user_id\"\n",
      "pg_restore: creating INDEX \"public.idx_oauth_session_user_provider\"\n",
      "pg_restore: creating INDEX \"public.idx_scenarios_is_active\"\n",
      "pg_restore: creating INDEX \"public.idx_scenarios_n_assigned\"\n",
      "pg_restore: creating INDEX \"public.idx_scenarios_polarity\"\n",
      "pg_restore: creating INDEX \"public.idx_scenarios_set_name\"\n",
      "pg_restore: creating INDEX \"public.idx_scenarios_source\"\n",
      "pg_restore: creating INDEX \"public.idx_scenarios_trait\"\n",
      "pg_restore: creating INDEX \"public.idx_selection_assignment_id\"\n",
      "pg_restore: creating INDEX \"public.idx_selection_chat_id\"\n",
      "pg_restore: creating INDEX \"public.idx_selection_created_at\"\n",
      "pg_restore: creating INDEX \"public.idx_selection_message_id\"\n",
      "pg_restore: creating INDEX \"public.idx_selection_scenario_id\"\n",
      "pg_restore: creating INDEX \"public.idx_selection_source\"\n",
      "pg_restore: creating INDEX \"public.idx_selection_user_id\"\n",
      "pg_restore: creating INDEX \"public.is_global_idx\"\n",
      "pg_restore: creating INDEX \"public.updated_at_user_id_idx\"\n",
      "pg_restore: creating INDEX \"public.user_id_archived_idx\"\n",
      "pg_restore: creating INDEX \"public.user_id_idx\"\n",
      "pg_restore: creating INDEX \"public.user_id_pinned_idx\"\n",
      "pg_restore: creating FK CONSTRAINT \"public.oauth_session oauth_session_user_id_fkey\"\n"
     ]
    }
   ],
   "source": [
    "! /opt/homebrew/opt/postgresql@17/bin/pg_restore --no-owner --no-privileges -v -f ~/Downloads/b078-20260113-215725.sql ~/Downloads/b078-20260113-215725.dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 dump_file set to: /Users/johndriscoll/Downloads/b078-20260113-215725.sql\n",
      "\u2713 file_format set to: sql\n",
      "\u2713 File exists: True\n",
      "\u2713 File size: 629.43 KB\n",
      "\u26a0\ufe0f  File format unknown\n"
     ]
    }
   ],
   "source": [
    "# Set path to SQL file (after conversion)\n",
    "from pathlib import Path\n",
    "\n",
    "# Point to the converted SQL file - EXPAND PATH PROPERLY\n",
    "dump_file = Path(\"~/Downloads/b078-20260113-215725.sql\").expanduser()\n",
    "file_format = 'sql'\n",
    "\n",
    "print(f\"\u2713 dump_file set to: {dump_file}\")\n",
    "print(f\"\u2713 file_format set to: {file_format}\")\n",
    "print(f\"\u2713 File exists: {dump_file.exists()}\")\n",
    "\n",
    "if dump_file.exists():\n",
    "    print(f\"\u2713 File size: {dump_file.stat().st_size / 1024:.2f} KB\")\n",
    "    # Check if it's actually SQL or still binary\n",
    "    with open(dump_file, 'rb') as f:\n",
    "        header = f.read(100)\n",
    "        if header.startswith(b'PGDMP'):\n",
    "            print(\"\u26a0\ufe0f  WARNING: File is still in PostgreSQL custom format! Run conversion cell first.\")\n",
    "        elif b'COPY' in header or b'CREATE' in header or b'INSERT' in header:\n",
    "            print(\"\u2713 File appears to be SQL format\")\n",
    "        else:\n",
    "            print(\"\u26a0\ufe0f  File format unknown\")\n",
    "else:\n",
    "    print(\"\u2717 File does not exist! Check the path or run the conversion cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Parse SQL Dump and Extract Tables\n",
    "\n",
    "We'll parse the SQL dump to extract table data into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing SQL dump...\n",
      "Reading SQL file: /Users/johndriscoll/Downloads/b078-20260113-215725.sql\n",
      "File size: 629.43 KB\n",
      "Decoded 635989 characters\n",
      "\n",
      "Found 36 COPY statements\n",
      "\n",
      "Processing table: alembic_version\n",
      "  Columns (1): version_num\n",
      "  \u2713 Extracted 1 rows\n",
      "\n",
      "Processing table: assignment_session_activity\n",
      "  Columns (6): id, user_id, session_number, active_ms_delta, cumulative_ms...\n",
      "  \u2713 Extracted 521 rows\n",
      "\n",
      "Processing table: attention_check_question\n",
      "  Columns (5): id, prompt, options, correct_option, created_at\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: attention_check_response\n",
      "  Columns (7): id, user_id, session_number, question_id, response...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: attention_check_scenarios\n",
      "  Columns (13): scenario_id, prompt_text, response_text, trait_theme, trait_phrase...\n",
      "  \u2713 Extracted 50 rows\n",
      "\n",
      "Processing table: auth\n",
      "  Columns (4): id, email, password, active\n",
      "  \u2713 Extracted 4 rows\n",
      "\n",
      "Processing table: channel\n",
      "  Columns (10): id, user_id, name, description, data...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: channel_member\n",
      "  Columns (4): id, channel_id, user_id, created_at\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: chat\n",
      "  Columns (11): id, user_id, title, created_at, updated_at...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: chatidtag\n",
      "  Columns (5): id, tag_name, chat_id, user_id, timestamp\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: child_profile\n",
      "  Columns (18): id, user_id, name, child_age, child_gender...\n",
      "  \u2713 Extracted 4 rows\n",
      "\n",
      "Processing table: config\n",
      "  Columns (5): id, data, version, created_at, updated_at\n",
      "  \u2713 Extracted 1 rows\n",
      "\n",
      "Processing table: consent_audit\n",
      "  Columns (11): id, user_id, timestamp_utc, consent_version, prolific_pid...\n",
      "  \u2713 Extracted 3 rows\n",
      "\n",
      "Processing table: document\n",
      "  Columns (7): collection_name, name, title, filename, content...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: exit_quiz_response\n",
      "  Columns (11): id, user_id, child_id, question_key, answers...\n",
      "  \u2713 Extracted 5 rows\n",
      "\n",
      "Processing table: feedback\n",
      "  Columns (9): id, user_id, version, type, data...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: file\n",
      "  Columns (10): id, user_id, filename, meta, created_at...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: folder\n",
      "  Columns (10): id, parent_id, user_id, name, items...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: function\n",
      "  Columns (11): id, user_id, name, type, content...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: group\n",
      "  Columns (10): id, user_id, name, description, data...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: knowledge\n",
      "  Columns (9): id, user_id, name, description, data...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: memory\n",
      "  Columns (5): id, user_id, content, updated_at, created_at\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: message\n",
      "  Columns (10): id, user_id, channel_id, content, data...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: message_reaction\n",
      "  Columns (5): id, user_id, message_id, name, created_at\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: model\n",
      "  Columns (10): id, user_id, base_model_id, name, params...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: moderation_session\n",
      "  Columns (29): id, user_id, child_id, scenario_prompt, original_response...\n",
      "  \u2713 Extracted 32 rows\n",
      "\n",
      "Processing table: moderation_session_activity\n",
      "  Columns (7): id, user_id, child_id, session_number, active_ms_delta...\n",
      "  \u2713 Extracted 1367 rows\n",
      "\n",
      "Processing table: note\n",
      "  Columns (8): id, user_id, title, data, meta...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: oauth_session\n",
      "  Columns (7): id, user_id, provider, token, expires_at...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: prompt\n",
      "  Columns (6): command, user_id, title, content, timestamp...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: scenario_assignments\n",
      "  Columns (19): assignment_id, participant_id, scenario_id, child_profile_id, status...\n",
      "  \u2713 Extracted 55 rows\n",
      "\n",
      "Processing table: scenarios\n",
      "  Columns (17): scenario_id, prompt_text, response_text, set_name, trait...\n",
      "  \u2713 Extracted 50 rows\n",
      "\n",
      "Processing table: selection\n",
      "  Columns (16): id, user_id, chat_id, message_id, role...\n",
      "  \u2713 Extracted 47 rows\n",
      "\n",
      "Processing table: tag\n",
      "  Columns (4): id, name, user_id, meta\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: tool\n",
      "  Columns (10): id, user_id, name, content, specs...\n",
      "  \u26a0\ufe0f  No data rows found\n",
      "\n",
      "Processing table: user\n",
      "  Columns (21): id, name, email, role, profile_image_url...\n",
      "  \u2713 Extracted 4 rows\n",
      "\n",
      "\n",
      "============================================================\n",
      "Successfully extracted 14 tables:\n",
      "  \u2022 alembic_version: 1 rows \u00d7 1 columns\n",
      "  \u2022 assignment_session_activity: 521 rows \u00d7 6 columns\n",
      "  \u2022 attention_check_scenarios: 50 rows \u00d7 13 columns\n",
      "  \u2022 auth: 4 rows \u00d7 4 columns\n",
      "  \u2022 child_profile: 4 rows \u00d7 18 columns\n",
      "  \u2022 config: 1 rows \u00d7 5 columns\n",
      "  \u2022 consent_audit: 3 rows \u00d7 11 columns\n",
      "  \u2022 exit_quiz_response: 5 rows \u00d7 11 columns\n",
      "  \u2022 moderation_session: 32 rows \u00d7 29 columns\n",
      "  \u2022 moderation_session_activity: 1367 rows \u00d7 7 columns\n",
      "  \u2022 scenario_assignments: 55 rows \u00d7 19 columns\n",
      "  \u2022 scenarios: 50 rows \u00d7 17 columns\n",
      "  \u2022 selection: 47 rows \u00d7 16 columns\n",
      "  \u2022 user: 4 rows \u00d7 21 columns\n",
      "============================================================\n",
      "\n",
      "\n",
      "\u2713 raw_dataframes created with 14 tables\n"
     ]
    }
   ],
   "source": [
    "def parse_sql_dump(sql_file_path):\n",
    "    \"\"\"Parse SQL dump file and extract table data.\n",
    "    \n",
    "    This function handles PostgreSQL SQL dumps with COPY commands.\n",
    "    It extracts table names, columns, and data into pandas DataFrames.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    \n",
    "    sql_file_path = Path(sql_file_path)\n",
    "    print(f\"Reading SQL file: {sql_file_path}\")\n",
    "    print(f\"File size: {sql_file_path.stat().st_size / 1024:.2f} KB\")\n",
    "    \n",
    "    # Read file content\n",
    "    with open(sql_file_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Check if it's actually SQL\n",
    "    if content.startswith(b'PGDMP'):\n",
    "        raise ValueError(\"File is in PostgreSQL custom format! Please run pg_restore conversion first.\")\n",
    "    \n",
    "    # Try to decode with different encodings\n",
    "    try:\n",
    "        text_content = content.decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"\u26a0\ufe0f  UTF-8 decode failed, trying latin-1...\")\n",
    "        try:\n",
    "            text_content = content.decode('latin-1', errors='ignore')\n",
    "        except:\n",
    "            print(\"\u26a0\ufe0f  latin-1 failed, using utf-8 with errors='replace'\")\n",
    "            text_content = content.decode('utf-8', errors='replace')\n",
    "    \n",
    "    print(f\"Decoded {len(text_content)} characters\")\n",
    "    \n",
    "    tables_data = {}\n",
    "    \n",
    "    # Find all COPY statements - handle various formats\n",
    "    # Format: COPY \"public\".\"table_name\" (columns) FROM stdin;\n",
    "    # Or: COPY public.table_name (columns) FROM stdin;\n",
    "    copy_pattern = r'COPY\\s+(?:\"public\"\\.)?(?:\")?(\\w+)(?:\")?\\s*\\(([^)]+)\\)\\s+FROM stdin;'\n",
    "    copy_matches = list(re.finditer(copy_pattern, text_content, re.MULTILINE | re.IGNORECASE))\n",
    "    \n",
    "    print(f\"\\nFound {len(copy_matches)} COPY statements\\n\")\n",
    "    \n",
    "    if len(copy_matches) == 0:\n",
    "        print(\"\u26a0\ufe0f  No COPY statements found. Checking for INSERT statements...\")\n",
    "        # Try to find INSERT statements as fallback\n",
    "        insert_pattern = r'INSERT INTO\\s+(?:\"public\"\\.)?(?:\")?(\\w+)(?:\")?'\n",
    "        insert_matches = list(re.finditer(insert_pattern, text_content, re.MULTILINE | re.IGNORECASE))\n",
    "        print(f\"Found {len(insert_matches)} INSERT statements\")\n",
    "        if len(insert_matches) == 0:\n",
    "            print(\"\u26a0\ufe0f  No data extraction possible from this file\")\n",
    "            return {}\n",
    "    \n",
    "    for i, copy_match in enumerate(copy_matches):\n",
    "        table_name = copy_match.group(1)\n",
    "        columns_str = copy_match.group(2)\n",
    "        \n",
    "        # Parse column names - handle quoted and unquoted\n",
    "        columns = []\n",
    "        for col in columns_str.split(','):\n",
    "            col = col.strip()\n",
    "            # Remove quotes if present\n",
    "            col = col.strip('\"').strip(\"'\")\n",
    "            columns.append(col)\n",
    "        \n",
    "        print(f\"Processing table: {table_name}\")\n",
    "        print(f\"  Columns ({len(columns)}): {', '.join(columns[:5])}{'...' if len(columns) > 5 else ''}\")\n",
    "        \n",
    "        # Find data section - between COPY command and next COPY or end marker\n",
    "        start_pos = copy_match.end()\n",
    "        \n",
    "        # Find end position\n",
    "        if i + 1 < len(copy_matches):\n",
    "            # End at next COPY statement\n",
    "            end_pos = copy_matches[i + 1].start()\n",
    "        else:\n",
    "            # Find the \\. marker that ends COPY data\n",
    "            end_marker_matches = list(re.finditer(r'^\\\\\\.$', text_content[start_pos:], re.MULTILINE))\n",
    "            if end_marker_matches:\n",
    "                end_pos = start_pos + end_marker_matches[0].start()\n",
    "            else:\n",
    "                end_pos = len(text_content)\n",
    "        \n",
    "        data_section = text_content[start_pos:end_pos]\n",
    "        \n",
    "        # Parse tab-separated values\n",
    "        rows = []\n",
    "        lines = data_section.strip().split('\\n')\n",
    "        \n",
    "        skipped_lines = 0\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip empty lines and end markers\n",
    "            if not line or line.startswith('\\\\'):\n",
    "                continue\n",
    "            \n",
    "            # Skip SQL commands that might be mixed in\n",
    "            if line.upper().startswith(('CREATE', 'ALTER', 'DROP', 'INSERT', 'UPDATE', 'DELETE', 'SELECT', '--', '/*')):\n",
    "                continue\n",
    "            \n",
    "            # Split by tab - PostgreSQL COPY uses tab as delimiter\n",
    "            values = line.split('\\t')\n",
    "            \n",
    "            # Check if we have the right number of columns\n",
    "            if len(values) != len(columns):\n",
    "                skipped_lines += 1\n",
    "                if skipped_lines <= 3:  # Only show first few mismatches\n",
    "                    print(f\"    \u26a0\ufe0f  Line has {len(values)} values but expected {len(columns)}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Parse values\n",
    "            row = {}\n",
    "            for col, val in zip(columns, values):\n",
    "                # Handle PostgreSQL NULL marker\n",
    "                if val == '\\\\N':\n",
    "                    row[col] = None\n",
    "                else:\n",
    "                    # Try to parse as JSON for JSON/JSONB columns\n",
    "                    if val and (val.startswith('{') or val.startswith('[')):\n",
    "                        try:\n",
    "                            row[col] = json.loads(val)\n",
    "                        except json.JSONDecodeError:\n",
    "                            # Not valid JSON, keep as string\n",
    "                            row[col] = val\n",
    "                    else:\n",
    "                        row[col] = val\n",
    "            \n",
    "            rows.append(row)\n",
    "        \n",
    "        if skipped_lines > 3:\n",
    "            print(f\"    \u26a0\ufe0f  Skipped {skipped_lines} total lines with column mismatches\")\n",
    "        \n",
    "        # Create DataFrame if we have data\n",
    "        if rows:\n",
    "            tables_data[table_name] = pd.DataFrame(rows)\n",
    "            print(f\"  \u2713 Extracted {len(rows)} rows\\n\")\n",
    "        else:\n",
    "            print(f\"  \u26a0\ufe0f  No data rows found\\n\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Successfully extracted {len(tables_data)} tables:\")\n",
    "    for table_name, df in tables_data.items():\n",
    "        print(f\"  \u2022 {table_name}: {len(df)} rows \u00d7 {len(df.columns)} columns\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return tables_data\n",
    "\n",
    "# Actually parse the dump and create raw_dataframes\n",
    "if 'dump_file' not in globals() or 'file_format' not in globals():\n",
    "    print(\"\u26a0\ufe0f  ERROR: dump_file and file_format not set!\")\n",
    "    print(\"Please run Cell 9 first to set these variables.\")\n",
    "else:\n",
    "    if not dump_file.exists():\n",
    "        print(f\"\u26a0\ufe0f  ERROR: SQL file not found at: {dump_file}\")\n",
    "        print(\"Please check the path or run the conversion cell first.\")\n",
    "    else:\n",
    "        print(\"Parsing SQL dump...\")\n",
    "        raw_dataframes = parse_sql_dump(dump_file)\n",
    "        print(f\"\\n\u2713 raw_dataframes created with {len(raw_dataframes)} tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Relevant Tables\n",
    "\n",
    "We'll focus on these tables for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant tables for analysis:\n",
      "  \u2713 user: 4 rows\n",
      "  \u2717 chat: 0 rows\n",
      "  \u2717 message: 0 rows\n",
      "  \u2713 child_profile: 4 rows\n",
      "  \u2713 selection: 47 rows\n",
      "  \u2717 moderation_scenario: 0 rows\n",
      "  \u2713 moderation_session: 32 rows\n",
      "  \u2717 moderation_applied: 0 rows\n",
      "  \u2717 moderation_question_answer: 0 rows\n",
      "  \u2713 exit_quiz_response: 5 rows\n",
      "  \u2713 scenario_assignments: 55 rows\n",
      "  \u2713 scenarios: 50 rows\n",
      "  \u2713 attention_check_scenarios: 50 rows\n",
      "  \u2713 assignment_session_activity: 521 rows\n"
     ]
    }
   ],
   "source": [
    "RELEVANT_TABLES = [\n",
    "    'user',\n",
    "    'chat',\n",
    "    'message',\n",
    "    'child_profile',\n",
    "    'selection',\n",
    "    'moderation_scenario',\n",
    "    'moderation_session',\n",
    "    'moderation_applied',\n",
    "    'moderation_question_answer',\n",
    "    'exit_quiz_response',\n",
    "    'scenario_assignments',\n",
    "    'scenarios',\n",
    "    'attention_check_scenarios',\n",
    "    'assignment_session_activity',\n",
    "]\n",
    "\n",
    "print(\"Relevant tables for analysis:\")\n",
    "for table in RELEVANT_TABLES:\n",
    "    status = \"\u2713\" if table in raw_dataframes else \"\u2717\"\n",
    "    count = len(raw_dataframes[table]) if table in raw_dataframes else 0\n",
    "    print(f\"  {status} {table}: {count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Clean and Transform Data\n",
    "\n",
    "Now we'll clean and transform each relevant table systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Helper Functions for Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 convert_timestamps function defined\n"
     ]
    }
   ],
   "source": [
    "def convert_timestamps(df, timestamp_cols=None):\n",
    "    \"\"\"Convert timestamp columns to datetime, handling different scales.\n",
    "    \n",
    "    This function handles:\n",
    "    - Nanosecond timestamps (> 1e15)\n",
    "    - Millisecond timestamps (> 1e12)\n",
    "    - Second timestamps (> 1e9)\n",
    "    - ISO format strings\n",
    "    - Already datetime objects\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    if timestamp_cols is None:\n",
    "        # Auto-detect timestamp columns\n",
    "        timestamp_cols = [col for col in df.columns \n",
    "                         if 'at' in col.lower() or 'time' in col.lower() or 'date' in col.lower()]\n",
    "    \n",
    "    print(f\"Converting timestamps in columns: {timestamp_cols}\")\n",
    "    \n",
    "    for col in timestamp_cols:\n",
    "        if col not in df.columns:\n",
    "            print(f\"  \u26a0\ufe0f  Column {col} not found, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Skip if already datetime\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            print(f\"  \u2713 {col} already datetime\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Convert to numeric first, handling NaN\n",
    "            numeric_vals = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Check if all NaN\n",
    "            if numeric_vals.isna().all():\n",
    "                print(f\"  \u26a0\ufe0f  {col}: All values are NaN or non-numeric\")\n",
    "                df[col + '_datetime'] = pd.NaT\n",
    "                continue\n",
    "            \n",
    "            # Get a sample non-NaN value to determine scale\n",
    "            sample_val = numeric_vals.dropna().iloc[0] if len(numeric_vals.dropna()) > 0 else None\n",
    "            \n",
    "            if sample_val is None:\n",
    "                print(f\"  \u26a0\ufe0f  {col}: No valid numeric values\")\n",
    "                df[col + '_datetime'] = pd.NaT\n",
    "                continue\n",
    "            \n",
    "            # Determine the scale based on magnitude\n",
    "            # Nanosecond timestamps are typically > 1e15\n",
    "            # Millisecond timestamps are typically > 1e12 but < 1e15\n",
    "            # Second timestamps are typically > 1e9 but < 1e12\n",
    "            \n",
    "            if sample_val > 1e15:\n",
    "                # Nanoseconds\n",
    "                print(f\"  \u2022 {col}: Detected nanosecond timestamps (sample: {sample_val:.0f})\")\n",
    "                df[col + '_datetime'] = pd.to_datetime(numeric_vals, unit='ns', errors='coerce')\n",
    "            elif sample_val > 1e12:\n",
    "                # Milliseconds\n",
    "                print(f\"  \u2022 {col}: Detected millisecond timestamps (sample: {sample_val:.0f})\")\n",
    "                df[col + '_datetime'] = pd.to_datetime(numeric_vals, unit='ms', errors='coerce')\n",
    "            elif sample_val > 1e9:\n",
    "                # Seconds\n",
    "                print(f\"  \u2022 {col}: Detected second timestamps (sample: {sample_val:.0f})\")\n",
    "                df[col + '_datetime'] = pd.to_datetime(numeric_vals, unit='s', errors='coerce')\n",
    "            else:\n",
    "                # Try direct datetime conversion (might be string format)\n",
    "                print(f\"  \u2022 {col}: Attempting direct datetime conversion (sample: {sample_val})\")\n",
    "                df[col + '_datetime'] = pd.to_datetime(df[col], errors='coerce')\n",
    "            \n",
    "            # Report conversion results\n",
    "            converted = df[col + '_datetime'].notna().sum()\n",
    "            total = len(df)\n",
    "            print(f\"    \u2713 Converted {converted}/{total} values ({converted/total*100:.1f}%)\")\n",
    "            \n",
    "            # Show sample of converted values\n",
    "            if converted > 0:\n",
    "                sample_dt = df[col + '_datetime'].dropna().iloc[0]\n",
    "                print(f\"    Sample: {sample_dt}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  \u2717 Error converting {col}: {e}\")\n",
    "            df[col + '_datetime'] = pd.NaT\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"\u2713 convert_timestamps function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2: Clean User Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced helper functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Enhanced helper functions with fixes for timestamp and JSON parsing\n",
    "\n",
    "def clean_strings(df):\n",
    "    \"\"\"Clean string columns by removing null bytes and normalizing null values.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to clean\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Get string columns\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in string_cols:\n",
    "        # Remove null bytes\n",
    "        df[col] = df[col].apply(lambda x: x.replace('\\x00', '') if isinstance(x, str) else x)\n",
    "        \n",
    "        # Replace literal null strings with None\n",
    "        df[col] = df[col].replace({\n",
    "            'null': None,\n",
    "            'NULL': None,\n",
    "            'None': None,\n",
    "            'nan': None,\n",
    "            'NaN': None,\n",
    "            '': None\n",
    "        })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_boolean_column(df, col_name):\n",
    "    \"\"\"Convert column to boolean, handling various formats.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to process\n",
    "        col_name: Name of column to convert\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with converted column\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    if col_name not in df.columns:\n",
    "        print(f\"  \u26a0\ufe0f  Column '{col_name}' not found\")\n",
    "        return df\n",
    "    \n",
    "    # Map various boolean representations\n",
    "    bool_map = {\n",
    "        't': True, 'f': False,\n",
    "        'true': True, 'false': False,\n",
    "        'True': True, 'False': False,\n",
    "        'TRUE': True, 'FALSE': False,\n",
    "        '1': True, '0': False,\n",
    "        1: True, 0: False,\n",
    "        'yes': True, 'no': False,\n",
    "        'Yes': True, 'No': False,\n",
    "        'YES': True, 'NO': False\n",
    "    }\n",
    "    \n",
    "    df[col_name] = df[col_name].map(bool_map)\n",
    "    converted = df[col_name].notna().sum()\n",
    "    print(f\"  \u2022 Converted {col_name}: {converted}/{len(df)} values to boolean\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_numeric_column(df, col_name, dtype='float'):\n",
    "    \"\"\"Convert column to numeric type.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to process\n",
    "        col_name: Name of column to convert\n",
    "        dtype: Target dtype ('int' or 'float')\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with converted column\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    if col_name not in df.columns:\n",
    "        print(f\"  \u26a0\ufe0f  Column '{col_name}' not found\")\n",
    "        return df\n",
    "    \n",
    "    if dtype == 'int':\n",
    "        df[col_name] = pd.to_numeric(df[col_name], errors='coerce').astype('Int64')\n",
    "    else:\n",
    "        df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "    \n",
    "    converted = df[col_name].notna().sum()\n",
    "    print(f\"  \u2022 Converted {col_name}: {converted}/{len(df)} values to {dtype}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Enhanced helper functions with fixes for timestamp and JSON parsing\n",
    "\n",
    "def parse_json_column(df, col_name, new_col_name=None):\n",
    "    \"\"\"Parse JSON column into a new column.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to process\n",
    "        col_name: Name of column containing JSON strings\n",
    "        new_col_name: Name for new parsed column (default: {col_name}_parsed)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with new parsed column added\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    if new_col_name is None:\n",
    "        new_col_name = f'{col_name}_parsed'\n",
    "    \n",
    "    if col_name not in df.columns:\n",
    "        print(f\"  \u26a0\ufe0f  Column '{col_name}' not found\")\n",
    "        return df\n",
    "    \n",
    "    def parse_json(val):\n",
    "        # Already a dict or list\n",
    "        if isinstance(val, (dict, list)):\n",
    "            return val\n",
    "        \n",
    "        # None or NaN\n",
    "        if val is None or pd.isna(val):\n",
    "            return None\n",
    "        \n",
    "        # String that needs parsing\n",
    "        if isinstance(val, str):\n",
    "            # Handle literal null strings\n",
    "            if val.lower() in ('null', 'none', 'nan') or val.strip() == '':\n",
    "                return None\n",
    "            \n",
    "            val_stripped = val.strip()\n",
    "            \n",
    "            # Try to parse JSON\n",
    "            if val_stripped.startswith('{') or val_stripped.startswith('['):\n",
    "                try:\n",
    "                    return json.loads(val_stripped)\n",
    "                except json.JSONDecodeError:\n",
    "                    # Try fixing single quotes\n",
    "                    try:\n",
    "                        val_fixed = val_stripped.replace(\"'\", '\"')\n",
    "                        return json.loads(val_fixed)\n",
    "                    except:\n",
    "                        return None\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    df[new_col_name] = df[col_name].apply(parse_json)\n",
    "    parsed_count = df[new_col_name].notna().sum()\n",
    "    print(f\"  \u2022 Parsed {col_name}: {parsed_count}/{len(df)} values\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Enhanced helper functions with fixes for timestamp and JSON parsing\n",
    "\n",
    "def convert_timestamps_enhanced(df, timestamp_cols=None):\n",
    "    \"\"\"Convert timestamp columns to datetime - enhanced version.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if timestamp_cols is None:\n",
    "        timestamp_cols = [col for col in df.columns \n",
    "                         if 'at' in col.lower() or 'time' in col.lower()]\n",
    "    \n",
    "    for col in timestamp_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Convert to numeric first (handles string timestamps)\n",
    "            numeric_col = pd.to_numeric(df[col], errors='coerce')\n",
    "            sample_val = numeric_col.dropna()\n",
    "            \n",
    "            if len(sample_val) > 0:\n",
    "                val = sample_val.iloc[0]\n",
    "                # Determine unit based on magnitude\n",
    "                if val > 1e15:\n",
    "                    # Nanoseconds (very large numbers like 1768343088867177168)\n",
    "                    df[f'{col}_datetime'] = pd.to_datetime(numeric_col / 1e9, unit='s', errors='coerce')\n",
    "                elif val > 1e12:\n",
    "                    # Microseconds\n",
    "                    df[f'{col}_datetime'] = pd.to_datetime(numeric_col / 1e6, unit='s', errors='coerce')\n",
    "                elif val > 1e9:\n",
    "                    # Milliseconds\n",
    "                    df[f'{col}_datetime'] = pd.to_datetime(numeric_col, unit='ms', errors='coerce')\n",
    "                else:\n",
    "                    # Seconds\n",
    "                    df[f'{col}_datetime'] = pd.to_datetime(numeric_col, unit='s', errors='coerce')\n",
    "            else:\n",
    "                df[f'{col}_datetime'] = pd.to_datetime(df[col], errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not convert {col}: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def parse_json_column_enhanced(df, col_name, new_col_name=None):\n",
    "    \"\"\"Parse JSON column - enhanced version that handles 'null' strings.\"\"\"\n",
    "    if new_col_name is None:\n",
    "        new_col_name = f'{col_name}_parsed'\n",
    "    \n",
    "    if col_name not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    def parse_json(val):\n",
    "        if isinstance(val, (dict, list)):\n",
    "            return val\n",
    "        if val is None or pd.isna(val):\n",
    "            return None\n",
    "        if isinstance(val, str):\n",
    "            if val.lower() == 'null' or val.strip() == '':\n",
    "                return None\n",
    "            val_stripped = val.strip()\n",
    "            if val_stripped.startswith('{') or val_stripped.startswith('['):\n",
    "                try:\n",
    "                    return json.loads(val_stripped)\n",
    "                except:\n",
    "                    try:\n",
    "                        val_fixed = val_stripped.replace(\"'\", '\"')\n",
    "                        return json.loads(val_fixed)\n",
    "                    except:\n",
    "                        return None\n",
    "        return None\n",
    "    \n",
    "    df[new_col_name] = df[col_name].apply(parse_json)\n",
    "    return df\n",
    "\n",
    "# Override the original functions\n",
    "convert_timestamps = convert_timestamps_enhanced\n",
    "parse_json_column = parse_json_column_enhanced\n",
    "\n",
    "print(\"Enhanced helper functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning user table...\n",
      "  \u2713 Cleaned user table: 4 rows, 27 columns\n",
      "  Columns: id, name, email, role, profile_image_url, last_active_at, updated_at, created_at, api_key, settings...\n"
     ]
    }
   ],
   "source": [
    "if 'user' in raw_dataframes:\n",
    "    print(\"Cleaning user table...\")\n",
    "    df_users = raw_dataframes['user'].copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_users = clean_strings(df_users)\n",
    "    df_users = convert_timestamps(df_users)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    df_users = parse_json_column(df_users, 'info')\n",
    "    df_users = parse_json_column(df_users, 'settings')\n",
    "    \n",
    "    # Convert date_of_birth if present\n",
    "    if 'date_of_birth' in df_users.columns:\n",
    "        df_users['date_of_birth'] = pd.to_datetime(df_users['date_of_birth'], errors='coerce')\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned user table: {len(df_users)} rows, {len(df_users.columns)} columns\")\n",
    "    print(f\"  Columns: {', '.join(df_users.columns[:10])}...\" if len(df_users.columns) > 10 else f\"  Columns: {', '.join(df_users.columns)}\")\n",
    "else:\n",
    "    print(\"\u2717 User table not found\")\n",
    "    df_users = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'name', 'email', 'role', 'profile_image_url', 'last_active_at',\n",
       "       'updated_at', 'created_at', 'api_key', 'settings', 'info', 'oauth_sub',\n",
       "       'username', 'bio', 'gender', 'date_of_birth', 'prolific_pid',\n",
       "       'study_id', 'current_session_id', 'session_number', 'consent_given',\n",
       "       'last_active_at_datetime', 'updated_at_datetime', 'created_at_datetime',\n",
       "       'date_of_birth_datetime', 'info_parsed', 'settings_parsed',\n",
       "       'last_active_at_datetime_datetime', 'updated_at_datetime_datetime',\n",
       "       'created_at_datetime_datetime', 'date_of_birth_datetime_datetime'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = df_users.drop(columns=['api_key','date_of_birth_datetime','info','oauth_sub','username','bio','gender','date_of_birth','info_parsed','date_of_birth_datetime_datetime']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>role</th>\n",
       "      <th>profile_image_url</th>\n",
       "      <th>last_active_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>created_at</th>\n",
       "      <th>settings</th>\n",
       "      <th>prolific_pid</th>\n",
       "      <th>...</th>\n",
       "      <th>current_session_id</th>\n",
       "      <th>session_number</th>\n",
       "      <th>consent_given</th>\n",
       "      <th>last_active_at_datetime</th>\n",
       "      <th>updated_at_datetime</th>\n",
       "      <th>created_at_datetime</th>\n",
       "      <th>settings_parsed</th>\n",
       "      <th>last_active_at_datetime_datetime</th>\n",
       "      <th>updated_at_datetime_datetime</th>\n",
       "      <th>created_at_datetime_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>055b5ebd-6a8b-4a75-b608-3c25f69a5197</td>\n",
       "      <td>haojian</td>\n",
       "      <td>prolific_haojian@prolific.study</td>\n",
       "      <td>user</td>\n",
       "      <td>/user.png</td>\n",
       "      <td>1768367761</td>\n",
       "      <td>1768345165</td>\n",
       "      <td>1768345125</td>\n",
       "      <td>{'ui': {'selectedChildId': '619ea48a-a376-4fce...</td>\n",
       "      <td>haojian</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>1970-01-21 11:12:47.761</td>\n",
       "      <td>1970-01-21 11:12:25.165</td>\n",
       "      <td>1970-01-21 11:12:25.125</td>\n",
       "      <td>{'ui': {'selectedChildId': '619ea48a-a376-4fce...</td>\n",
       "      <td>1970-01-21 11:12:47.761</td>\n",
       "      <td>1970-01-21 11:12:25.165</td>\n",
       "      <td>1970-01-21 11:12:25.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46bf4ed4-ba1e-467c-bbcd-8ed67c68406c</td>\n",
       "      <td>Admin</td>\n",
       "      <td>childai.research.ucsd@gmail.com</td>\n",
       "      <td>admin</td>\n",
       "      <td>data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...</td>\n",
       "      <td>1768367096</td>\n",
       "      <td>1768329895</td>\n",
       "      <td>1768329895</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>1970-01-21 11:12:47.096</td>\n",
       "      <td>1970-01-21 11:12:09.895</td>\n",
       "      <td>1970-01-21 11:12:09.895</td>\n",
       "      <td>None</td>\n",
       "      <td>1970-01-21 11:12:47.096</td>\n",
       "      <td>1970-01-21 11:12:09.895</td>\n",
       "      <td>1970-01-21 11:12:09.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>test1</td>\n",
       "      <td>prolific_test1@prolific.study</td>\n",
       "      <td>user</td>\n",
       "      <td>/user.png</td>\n",
       "      <td>1768368053</td>\n",
       "      <td>1768329978</td>\n",
       "      <td>1768329969</td>\n",
       "      <td>{'ui': {'selectedChildId': 'a4f0224b-2d70-4b92...</td>\n",
       "      <td>test1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>1970-01-21 11:12:48.053</td>\n",
       "      <td>1970-01-21 11:12:09.978</td>\n",
       "      <td>1970-01-21 11:12:09.969</td>\n",
       "      <td>{'ui': {'selectedChildId': 'a4f0224b-2d70-4b92...</td>\n",
       "      <td>1970-01-21 11:12:48.053</td>\n",
       "      <td>1970-01-21 11:12:09.978</td>\n",
       "      <td>1970-01-21 11:12:09.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16a92a7a-e4f1-46d2-91c4-c0e32208411c</td>\n",
       "      <td>lucas</td>\n",
       "      <td>prolific_lucas@prolific.study</td>\n",
       "      <td>user</td>\n",
       "      <td>/user.png</td>\n",
       "      <td>1768347875</td>\n",
       "      <td>1768345170</td>\n",
       "      <td>1768345157</td>\n",
       "      <td>{'ui': {'selectedChildId': '25d6122a-cdbc-40a8...</td>\n",
       "      <td>lucas</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>1970-01-21 11:12:27.875</td>\n",
       "      <td>1970-01-21 11:12:25.170</td>\n",
       "      <td>1970-01-21 11:12:25.157</td>\n",
       "      <td>{'ui': {'selectedChildId': '25d6122a-cdbc-40a8...</td>\n",
       "      <td>1970-01-21 11:12:27.875</td>\n",
       "      <td>1970-01-21 11:12:25.170</td>\n",
       "      <td>1970-01-21 11:12:25.157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows \u00d7 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     name  \\\n",
       "0  055b5ebd-6a8b-4a75-b608-3c25f69a5197  haojian   \n",
       "1  46bf4ed4-ba1e-467c-bbcd-8ed67c68406c    Admin   \n",
       "2  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6    test1   \n",
       "3  16a92a7a-e4f1-46d2-91c4-c0e32208411c    lucas   \n",
       "\n",
       "                             email   role  \\\n",
       "0  prolific_haojian@prolific.study   user   \n",
       "1  childai.research.ucsd@gmail.com  admin   \n",
       "2    prolific_test1@prolific.study   user   \n",
       "3    prolific_lucas@prolific.study   user   \n",
       "\n",
       "                                   profile_image_url last_active_at  \\\n",
       "0                                          /user.png     1768367761   \n",
       "1  data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...     1768367096   \n",
       "2                                          /user.png     1768368053   \n",
       "3                                          /user.png     1768347875   \n",
       "\n",
       "   updated_at  created_at                                           settings  \\\n",
       "0  1768345165  1768345125  {'ui': {'selectedChildId': '619ea48a-a376-4fce...   \n",
       "1  1768329895  1768329895                                               null   \n",
       "2  1768329978  1768329969  {'ui': {'selectedChildId': 'a4f0224b-2d70-4b92...   \n",
       "3  1768345170  1768345157  {'ui': {'selectedChildId': '25d6122a-cdbc-40a8...   \n",
       "\n",
       "  prolific_pid  ... current_session_id session_number consent_given  \\\n",
       "0      haojian  ...                  1              1             t   \n",
       "1          NaN  ...                NaN              1             f   \n",
       "2        test1  ...                  1              1             t   \n",
       "3        lucas  ...                  1              1             t   \n",
       "\n",
       "  last_active_at_datetime     updated_at_datetime     created_at_datetime  \\\n",
       "0 1970-01-21 11:12:47.761 1970-01-21 11:12:25.165 1970-01-21 11:12:25.125   \n",
       "1 1970-01-21 11:12:47.096 1970-01-21 11:12:09.895 1970-01-21 11:12:09.895   \n",
       "2 1970-01-21 11:12:48.053 1970-01-21 11:12:09.978 1970-01-21 11:12:09.969   \n",
       "3 1970-01-21 11:12:27.875 1970-01-21 11:12:25.170 1970-01-21 11:12:25.157   \n",
       "\n",
       "                                     settings_parsed  \\\n",
       "0  {'ui': {'selectedChildId': '619ea48a-a376-4fce...   \n",
       "1                                               None   \n",
       "2  {'ui': {'selectedChildId': 'a4f0224b-2d70-4b92...   \n",
       "3  {'ui': {'selectedChildId': '25d6122a-cdbc-40a8...   \n",
       "\n",
       "  last_active_at_datetime_datetime updated_at_datetime_datetime  \\\n",
       "0          1970-01-21 11:12:47.761      1970-01-21 11:12:25.165   \n",
       "1          1970-01-21 11:12:47.096      1970-01-21 11:12:09.895   \n",
       "2          1970-01-21 11:12:48.053      1970-01-21 11:12:09.978   \n",
       "3          1970-01-21 11:12:27.875      1970-01-21 11:12:25.170   \n",
       "\n",
       "  created_at_datetime_datetime  \n",
       "0      1970-01-21 11:12:25.125  \n",
       "1      1970-01-21 11:12:09.895  \n",
       "2      1970-01-21 11:12:09.969  \n",
       "3      1970-01-21 11:12:25.157  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3: Clean Chat Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2717 Chat table not found\n"
     ]
    }
   ],
   "source": [
    "if 'chat' in raw_dataframes:\n",
    "    print(\"Cleaning chat table...\")\n",
    "    df_chats = raw_dataframes['chat'].copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_chats = clean_strings(df_chats)\n",
    "    df_chats = convert_timestamps(df_chats)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    df_chats = parse_json_column(df_chats, 'chat')\n",
    "    df_chats = parse_json_column(df_chats, 'meta')\n",
    "    \n",
    "    # Extract message count from chat JSON\n",
    "    if 'chat_parsed' in df_chats.columns:\n",
    "        def count_messages(chat_data):\n",
    "            if isinstance(chat_data, dict):\n",
    "                history = chat_data.get('history', {})\n",
    "                messages = history.get('messages', {})\n",
    "                if isinstance(messages, dict):\n",
    "                    return len(messages)\n",
    "            return 0\n",
    "        \n",
    "        df_chats['message_count'] = df_chats['chat_parsed'].apply(count_messages)\n",
    "    \n",
    "    # Convert boolean columns\n",
    "    for col in ['archived', 'pinned']:\n",
    "        if col in df_chats.columns:\n",
    "            df_chats[col] = df_chats[col].astype(str).str.lower() == 'true'\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned chat table: {len(df_chats)} rows, {len(df_chats.columns)} columns\")\n",
    "    print(f\"  Total messages across all chats: {df_chats['message_count'].sum() if 'message_count' in df_chats.columns else 'N/A'}\")\n",
    "else:\n",
    "    print(\"\u2717 Chat table not found\")\n",
    "    df_chats = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4: Clean Message Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2717 Message table not found\n"
     ]
    }
   ],
   "source": [
    "if 'message' in raw_dataframes:\n",
    "    print(\"Cleaning message table...\")\n",
    "    df_messages = raw_dataframes['message'].copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_messages = clean_strings(df_messages)\n",
    "    df_messages = convert_timestamps(df_messages)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    df_messages = parse_json_column(df_messages, 'data')\n",
    "    df_messages = parse_json_column(df_messages, 'meta')\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned message table: {len(df_messages)} rows, {len(df_messages.columns)} columns\")\n",
    "    if 'role' in df_messages.columns:\n",
    "        print(f\"  Messages by role:\")\n",
    "        print(df_messages['role'].value_counts().to_string())\n",
    "else:\n",
    "    print(\"\u2717 Message table not found\")\n",
    "    df_messages = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5: Clean Child Profile Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning child_profile table...\n",
      "  \u2713 Cleaned child_profile table: 4 rows, 21 columns\n",
      "  Unique users: 3\n"
     ]
    }
   ],
   "source": [
    "if 'child_profile' in raw_dataframes:\n",
    "    print(\"Cleaning child_profile table...\")\n",
    "    df_child_profiles = raw_dataframes['child_profile'].copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_child_profiles = clean_strings(df_child_profiles)\n",
    "    df_child_profiles = convert_timestamps(df_child_profiles)\n",
    "    \n",
    "    # Parse JSON fields if any\n",
    "    for col in df_child_profiles.columns:\n",
    "        if df_child_profiles[col].dtype == 'object':\n",
    "            sample = df_child_profiles[col].dropna().astype(str)\n",
    "            if len(sample) > 0 and sample.str.startswith('{').any():\n",
    "                df_child_profiles = parse_json_column(df_child_profiles, col)\n",
    "    \n",
    "    # Convert boolean columns\n",
    "    for col in ['is_current', 'is_only_child']:\n",
    "        if col in df_child_profiles.columns:\n",
    "            df_child_profiles[col] = df_child_profiles[col].astype(str).str.lower() == 'true'\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    for col in ['attempt_number', 'session_number']:\n",
    "        if col in df_child_profiles.columns:\n",
    "            df_child_profiles[col] = pd.to_numeric(df_child_profiles[col], errors='coerce')\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned child_profile table: {len(df_child_profiles)} rows, {len(df_child_profiles.columns)} columns\")\n",
    "    print(f\"  Unique users: {df_child_profiles['user_id'].nunique() if 'user_id' in df_child_profiles.columns else 'N/A'}\")\n",
    "else:\n",
    "    print(\"\u2717 Child profile table not found\")\n",
    "    df_child_profiles = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>child_age</th>\n",
       "      <th>child_gender</th>\n",
       "      <th>child_characteristics</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>attempt_number</th>\n",
       "      <th>is_current</th>\n",
       "      <th>...</th>\n",
       "      <th>parent_llm_monitoring_level</th>\n",
       "      <th>child_gender_other</th>\n",
       "      <th>child_ai_use_contexts_other</th>\n",
       "      <th>parent_llm_monitoring_other</th>\n",
       "      <th>created_at_datetime</th>\n",
       "      <th>updated_at_datetime</th>\n",
       "      <th>attempt_number_datetime</th>\n",
       "      <th>created_at_datetime_datetime</th>\n",
       "      <th>updated_at_datetime_datetime</th>\n",
       "      <th>attempt_number_datetime_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a4f0224b-2d70-4b92-bfa7-4caef02f5df4</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>bob</td>\n",
       "      <td>10 years old</td>\n",
       "      <td>Non-binary</td>\n",
       "      <td>Conscientiousness: Is systematic, likes to kee...</td>\n",
       "      <td>1768329996252018976</td>\n",
       "      <td>1768343088867177168</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>prefer_not_to_say</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-13 18:46:36.252018929</td>\n",
       "      <td>2026-01-13 22:24:48.867177248</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "      <td>2026-01-13 18:46:36.252018929</td>\n",
       "      <td>2026-01-13 22:24:48.867177248</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25d6122a-cdbc-40a8-817b-82edb4a75ef7</td>\n",
       "      <td>16a92a7a-e4f1-46d2-91c4-c0e32208411c</td>\n",
       "      <td>Izak</td>\n",
       "      <td>18 years old</td>\n",
       "      <td>Male</td>\n",
       "      <td>Conscientiousness: Is systematic, likes to kee...</td>\n",
       "      <td>1768345250730407715</td>\n",
       "      <td>1768345250730407715</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>plan_to</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-13 23:00:50.730407715</td>\n",
       "      <td>2026-01-13 23:00:50.730407715</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "      <td>2026-01-13 23:00:50.730407715</td>\n",
       "      <td>2026-01-13 23:00:50.730407715</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>de6a59d8-e18a-49da-b573-3277c1c50a0a</td>\n",
       "      <td>055b5ebd-6a8b-4a75-b608-3c25f69a5197</td>\n",
       "      <td>Izak</td>\n",
       "      <td>12 years old</td>\n",
       "      <td>Non-binary</td>\n",
       "      <td>Agreeableness: Is compassionate, has a soft he...</td>\n",
       "      <td>1768345282627368961</td>\n",
       "      <td>1768345282627368961</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>plan_to</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-13 23:01:22.627368927</td>\n",
       "      <td>2026-01-13 23:01:22.627368927</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "      <td>2026-01-13 23:01:22.627368927</td>\n",
       "      <td>2026-01-13 23:01:22.627368927</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>619ea48a-a376-4fce-96f7-9ff4c87c06fe</td>\n",
       "      <td>055b5ebd-6a8b-4a75-b608-3c25f69a5197</td>\n",
       "      <td>HAHAHA</td>\n",
       "      <td>10 years old</td>\n",
       "      <td>Male</td>\n",
       "      <td>Conscientiousness: Is persistent, works until ...</td>\n",
       "      <td>1768345339787081033</td>\n",
       "      <td>1768345339787081033</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>occasional_guidance</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-13 23:02:19.787081003</td>\n",
       "      <td>2026-01-13 23:02:19.787081003</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "      <td>2026-01-13 23:02:19.787081003</td>\n",
       "      <td>2026-01-13 23:02:19.787081003</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows \u00d7 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                               user_id  \\\n",
       "0  a4f0224b-2d70-4b92-bfa7-4caef02f5df4  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "1  25d6122a-cdbc-40a8-817b-82edb4a75ef7  16a92a7a-e4f1-46d2-91c4-c0e32208411c   \n",
       "2  de6a59d8-e18a-49da-b573-3277c1c50a0a  055b5ebd-6a8b-4a75-b608-3c25f69a5197   \n",
       "3  619ea48a-a376-4fce-96f7-9ff4c87c06fe  055b5ebd-6a8b-4a75-b608-3c25f69a5197   \n",
       "\n",
       "     name     child_age child_gender  \\\n",
       "0     bob  10 years old   Non-binary   \n",
       "1    Izak  18 years old         Male   \n",
       "2    Izak  12 years old   Non-binary   \n",
       "3  HAHAHA  10 years old         Male   \n",
       "\n",
       "                               child_characteristics           created_at  \\\n",
       "0  Conscientiousness: Is systematic, likes to kee...  1768329996252018976   \n",
       "1  Conscientiousness: Is systematic, likes to kee...  1768345250730407715   \n",
       "2  Agreeableness: Is compassionate, has a soft he...  1768345282627368961   \n",
       "3  Conscientiousness: Is persistent, works until ...  1768345339787081033   \n",
       "\n",
       "            updated_at  attempt_number  is_current  ...  \\\n",
       "0  1768343088867177168               1       False  ...   \n",
       "1  1768345250730407715               1       False  ...   \n",
       "2  1768345282627368961               1       False  ...   \n",
       "3  1768345339787081033               1       False  ...   \n",
       "\n",
       "   parent_llm_monitoring_level  child_gender_other  \\\n",
       "0            prefer_not_to_say                None   \n",
       "1                      plan_to                None   \n",
       "2                      plan_to                None   \n",
       "3          occasional_guidance                None   \n",
       "\n",
       "  child_ai_use_contexts_other parent_llm_monitoring_other  \\\n",
       "0                        None                        None   \n",
       "1                        None                        None   \n",
       "2                        None                        None   \n",
       "3                        None                        None   \n",
       "\n",
       "            created_at_datetime           updated_at_datetime  \\\n",
       "0 2026-01-13 18:46:36.252018929 2026-01-13 22:24:48.867177248   \n",
       "1 2026-01-13 23:00:50.730407715 2026-01-13 23:00:50.730407715   \n",
       "2 2026-01-13 23:01:22.627368927 2026-01-13 23:01:22.627368927   \n",
       "3 2026-01-13 23:02:19.787081003 2026-01-13 23:02:19.787081003   \n",
       "\n",
       "  attempt_number_datetime  created_at_datetime_datetime  \\\n",
       "0     1970-01-01 00:00:01 2026-01-13 18:46:36.252018929   \n",
       "1     1970-01-01 00:00:01 2026-01-13 23:00:50.730407715   \n",
       "2     1970-01-01 00:00:01 2026-01-13 23:01:22.627368927   \n",
       "3     1970-01-01 00:00:01 2026-01-13 23:02:19.787081003   \n",
       "\n",
       "   updated_at_datetime_datetime attempt_number_datetime_datetime  \n",
       "0 2026-01-13 22:24:48.867177248              1970-01-01 00:00:01  \n",
       "1 2026-01-13 23:00:50.730407715              1970-01-01 00:00:01  \n",
       "2 2026-01-13 23:01:22.627368927              1970-01-01 00:00:01  \n",
       "3 2026-01-13 23:02:19.787081003              1970-01-01 00:00:01  \n",
       "\n",
       "[4 rows x 24 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_child_profiles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6: Clean Selection Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning selection table...\n",
      "  \u2713 Cleaned selection table: 47 rows, 20 columns\n",
      "  Selections by role:\n",
      "role\n",
      "assistant    38\n",
      "user          9\n",
      "  Selections by source:\n",
      "source\n",
      "response    38\n",
      "prompt       9\n"
     ]
    }
   ],
   "source": [
    "if 'selection' in raw_dataframes:\n",
    "    print(\"Cleaning selection table...\")\n",
    "    df_selections = raw_dataframes['selection'].copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_selections = clean_strings(df_selections)\n",
    "    df_selections = convert_timestamps(df_selections)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    df_selections = parse_json_column(df_selections, 'meta')\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    for col in ['start_offset', 'end_offset']:\n",
    "        if col in df_selections.columns:\n",
    "            df_selections[col] = pd.to_numeric(df_selections[col], errors='coerce')\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned selection table: {len(df_selections)} rows, {len(df_selections.columns)} columns\")\n",
    "    if 'role' in df_selections.columns:\n",
    "        print(f\"  Selections by role:\")\n",
    "        print(df_selections['role'].value_counts().to_string())\n",
    "    if 'source' in df_selections.columns:\n",
    "        print(f\"  Selections by source:\")\n",
    "        print(df_selections['source'].value_counts().to_string())\n",
    "else:\n",
    "    print(\"\u2717 Selection table not found\")\n",
    "    df_selections = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['context', 'meta', 'start_offset', 'end_offset', 'chat_id_datetime', 'meta_parsed', 'chat_id_datetime_datetime'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_selections = \u001b[43mdf_selections\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstart_offset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mend_offset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchat_id_datetime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmeta_parsed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchat_id_datetime_datetime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/parentalcontrol/lib/python3.14/site-packages/pandas/core/frame.py:6288\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   6130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   6131\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   6132\u001b[39m     labels: IndexLabel | ListLike = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   6139\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   6140\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   6141\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6142\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   6143\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   6286\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   6287\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   6289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6290\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6294\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6295\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/parentalcontrol/lib/python3.14/site-packages/pandas/core/generic.py:4644\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4642\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4644\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4647\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/parentalcontrol/lib/python3.14/site-packages/pandas/core/generic.py:4686\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4684\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4685\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4686\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4687\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4689\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4690\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/parentalcontrol/lib/python3.14/site-packages/pandas/core/indexes/base.py:7268\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7267\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7268\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7269\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['context', 'meta', 'start_offset', 'end_offset', 'chat_id_datetime', 'meta_parsed', 'chat_id_datetime_datetime'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df_selections = df_selections.drop(columns=['context','meta','start_offset','end_offset','chat_id_datetime','meta_parsed','chat_id_datetime_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>chat_id</th>\n",
       "      <th>message_id</th>\n",
       "      <th>role</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>scenario_id</th>\n",
       "      <th>source</th>\n",
       "      <th>child_id</th>\n",
       "      <th>assignment_id</th>\n",
       "      <th>created_at_datetime</th>\n",
       "      <th>updated_at_datetime</th>\n",
       "      <th>created_at_datetime_datetime</th>\n",
       "      <th>updated_at_datetime_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568a1b24-0f9f-4bd2-990c-a1f8b730e658</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>assignment_dbeb40ea-dbf4-46d9-94df-9907660020b3</td>\n",
       "      <td>dbeb40ea-dbf4-46d9-94df-9907660020b3:prompt</td>\n",
       "      <td>user</td>\n",
       "      <td>my dad posts pics of me and it's embarrassing,...</td>\n",
       "      <td>1768330003273891709</td>\n",
       "      <td>1768330003273891709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prompt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dbeb40ea-dbf4-46d9-94df-9907660020b3</td>\n",
       "      <td>2026-01-13 18:46:43.273891687</td>\n",
       "      <td>2026-01-13 18:46:43.273891687</td>\n",
       "      <td>2026-01-13 18:46:43.273891687</td>\n",
       "      <td>2026-01-13 18:46:43.273891687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e9990b1f-bbfc-40f8-908f-3794a5bc344f</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>assignment_dbeb40ea-dbf4-46d9-94df-9907660020b3</td>\n",
       "      <td>dbeb40ea-dbf4-46d9-94df-9907660020b3:response</td>\n",
       "      <td>assistant</td>\n",
       "      <td>can (and should) tell him to stop or at least c</td>\n",
       "      <td>1768330007137792387</td>\n",
       "      <td>1768330007137792387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>response</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dbeb40ea-dbf4-46d9-94df-9907660020b3</td>\n",
       "      <td>2026-01-13 18:46:47.137792587</td>\n",
       "      <td>2026-01-13 18:46:47.137792587</td>\n",
       "      <td>2026-01-13 18:46:47.137792587</td>\n",
       "      <td>2026-01-13 18:46:47.137792587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adcd2bac-18e8-4f05-aa1e-b2d1847e30ac</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>assignment_dbeb40ea-dbf4-46d9-94df-9907660020b3</td>\n",
       "      <td>dbeb40ea-dbf4-46d9-94df-9907660020b3:response</td>\n",
       "      <td>assistant</td>\n",
       "      <td>What to say (simple and respectful)\\n\\nTry usi...</td>\n",
       "      <td>1768330010682180329</td>\n",
       "      <td>1768330010682180329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>response</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dbeb40ea-dbf4-46d9-94df-9907660020b3</td>\n",
       "      <td>2026-01-13 18:46:50.682180405</td>\n",
       "      <td>2026-01-13 18:46:50.682180405</td>\n",
       "      <td>2026-01-13 18:46:50.682180405</td>\n",
       "      <td>2026-01-13 18:46:50.682180405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>badfd993-4903-44c4-86c0-a41c745d9bcb</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>assignment_dbeb40ea-dbf4-46d9-94df-9907660020b3</td>\n",
       "      <td>dbeb40ea-dbf4-46d9-94df-9907660020b3:response</td>\n",
       "      <td>assistant</td>\n",
       "      <td>Can you please ask me before you post?\u201d</td>\n",
       "      <td>1768330015665036053</td>\n",
       "      <td>1768330015665036053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>response</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dbeb40ea-dbf4-46d9-94df-9907660020b3</td>\n",
       "      <td>2026-01-13 18:46:55.665035963</td>\n",
       "      <td>2026-01-13 18:46:55.665035963</td>\n",
       "      <td>2026-01-13 18:46:55.665035963</td>\n",
       "      <td>2026-01-13 18:46:55.665035963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5824fd70-717d-423f-88ec-12ecb4366be4</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>assignment_dbeb40ea-dbf4-46d9-94df-9907660020b3</td>\n",
       "      <td>dbeb40ea-dbf4-46d9-94df-9907660020b3:response</td>\n",
       "      <td>assistant</td>\n",
       "      <td>Offer a compromise\\n\\nIf \u201cstop completely\u201d fee...</td>\n",
       "      <td>1768330019679754536</td>\n",
       "      <td>1768330019679754536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>response</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dbeb40ea-dbf4-46d9-94df-9907660020b3</td>\n",
       "      <td>2026-01-13 18:46:59.679754496</td>\n",
       "      <td>2026-01-13 18:46:59.679754496</td>\n",
       "      <td>2026-01-13 18:46:59.679754496</td>\n",
       "      <td>2026-01-13 18:46:59.679754496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                               user_id  \\\n",
       "0  568a1b24-0f9f-4bd2-990c-a1f8b730e658  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "1  e9990b1f-bbfc-40f8-908f-3794a5bc344f  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "2  adcd2bac-18e8-4f05-aa1e-b2d1847e30ac  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "3  badfd993-4903-44c4-86c0-a41c745d9bcb  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "4  5824fd70-717d-423f-88ec-12ecb4366be4  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "\n",
       "                                           chat_id  \\\n",
       "0  assignment_dbeb40ea-dbf4-46d9-94df-9907660020b3   \n",
       "1  assignment_dbeb40ea-dbf4-46d9-94df-9907660020b3   \n",
       "2  assignment_dbeb40ea-dbf4-46d9-94df-9907660020b3   \n",
       "3  assignment_dbeb40ea-dbf4-46d9-94df-9907660020b3   \n",
       "4  assignment_dbeb40ea-dbf4-46d9-94df-9907660020b3   \n",
       "\n",
       "                                      message_id       role  \\\n",
       "0    dbeb40ea-dbf4-46d9-94df-9907660020b3:prompt       user   \n",
       "1  dbeb40ea-dbf4-46d9-94df-9907660020b3:response  assistant   \n",
       "2  dbeb40ea-dbf4-46d9-94df-9907660020b3:response  assistant   \n",
       "3  dbeb40ea-dbf4-46d9-94df-9907660020b3:response  assistant   \n",
       "4  dbeb40ea-dbf4-46d9-94df-9907660020b3:response  assistant   \n",
       "\n",
       "                                       selected_text           created_at  \\\n",
       "0  my dad posts pics of me and it's embarrassing,...  1768330003273891709   \n",
       "1    can (and should) tell him to stop or at least c  1768330007137792387   \n",
       "2  What to say (simple and respectful)\\n\\nTry usi...  1768330010682180329   \n",
       "3            Can you please ask me before you post?\u201d  1768330015665036053   \n",
       "4  Offer a compromise\\n\\nIf \u201cstop completely\u201d fee...  1768330019679754536   \n",
       "\n",
       "            updated_at scenario_id    source child_id  \\\n",
       "0  1768330003273891709         NaN    prompt      NaN   \n",
       "1  1768330007137792387         NaN  response      NaN   \n",
       "2  1768330010682180329         NaN  response      NaN   \n",
       "3  1768330015665036053         NaN  response      NaN   \n",
       "4  1768330019679754536         NaN  response      NaN   \n",
       "\n",
       "                          assignment_id           created_at_datetime  \\\n",
       "0  dbeb40ea-dbf4-46d9-94df-9907660020b3 2026-01-13 18:46:43.273891687   \n",
       "1  dbeb40ea-dbf4-46d9-94df-9907660020b3 2026-01-13 18:46:47.137792587   \n",
       "2  dbeb40ea-dbf4-46d9-94df-9907660020b3 2026-01-13 18:46:50.682180405   \n",
       "3  dbeb40ea-dbf4-46d9-94df-9907660020b3 2026-01-13 18:46:55.665035963   \n",
       "4  dbeb40ea-dbf4-46d9-94df-9907660020b3 2026-01-13 18:46:59.679754496   \n",
       "\n",
       "            updated_at_datetime  created_at_datetime_datetime  \\\n",
       "0 2026-01-13 18:46:43.273891687 2026-01-13 18:46:43.273891687   \n",
       "1 2026-01-13 18:46:47.137792587 2026-01-13 18:46:47.137792587   \n",
       "2 2026-01-13 18:46:50.682180405 2026-01-13 18:46:50.682180405   \n",
       "3 2026-01-13 18:46:55.665035963 2026-01-13 18:46:55.665035963   \n",
       "4 2026-01-13 18:46:59.679754496 2026-01-13 18:46:59.679754496   \n",
       "\n",
       "   updated_at_datetime_datetime  \n",
       "0 2026-01-13 18:46:43.273891687  \n",
       "1 2026-01-13 18:46:47.137792587  \n",
       "2 2026-01-13 18:46:50.682180405  \n",
       "3 2026-01-13 18:46:55.665035963  \n",
       "4 2026-01-13 18:46:59.679754496  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selections.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Clean Moderation Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning moderation_session table...\n",
      "  \u2713 Cleaned moderation_session: 32 rows\n"
     ]
    }
   ],
   "source": [
    "# Moderation Scenario\n",
    "if 'moderation_scenario' in raw_dataframes:\n",
    "    print(\"Cleaning moderation_scenario table...\")\n",
    "    df_mod_scenarios = raw_dataframes['moderation_scenario'].copy()\n",
    "    df_mod_scenarios = clean_strings(df_mod_scenarios)\n",
    "    df_mod_scenarios = convert_timestamps(df_mod_scenarios)\n",
    "    \n",
    "    # Convert boolean columns\n",
    "    for col in ['is_applicable']:\n",
    "        if col in df_mod_scenarios.columns:\n",
    "            df_mod_scenarios[col] = df_mod_scenarios[col].astype(str).str.lower() == 'true'\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned moderation_scenario: {len(df_mod_scenarios)} rows\")\n",
    "else:\n",
    "    df_mod_scenarios = pd.DataFrame()\n",
    "\n",
    "# Moderation Session\n",
    "if 'moderation_session' in raw_dataframes:\n",
    "    print(\"Cleaning moderation_session table...\")\n",
    "    df_mod_sessions = raw_dataframes['moderation_session'].copy()\n",
    "    df_mod_sessions = clean_strings(df_mod_sessions)\n",
    "    df_mod_sessions = convert_timestamps(df_mod_sessions)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    json_cols = ['strategies', 'custom_instructions', 'highlighted_texts', \n",
    "                 'refactored_response', 'session_metadata']\n",
    "    for col in json_cols:\n",
    "        if col in df_mod_sessions.columns:\n",
    "            df_mod_sessions = parse_json_column(df_mod_sessions, col)\n",
    "    \n",
    "    # Convert boolean and numeric columns\n",
    "    if 'is_final_version' in df_mod_sessions.columns:\n",
    "        df_mod_sessions['is_final_version'] = df_mod_sessions['is_final_version'].astype(str).str.lower() == 'true'\n",
    "    for col in ['scenario_index', 'attempt_number', 'version_number']:\n",
    "        if col in df_mod_sessions.columns:\n",
    "            df_mod_sessions[col] = pd.to_numeric(df_mod_sessions[col], errors='coerce')\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned moderation_session: {len(df_mod_sessions)} rows\")\n",
    "else:\n",
    "    df_mod_sessions = pd.DataFrame()\n",
    "\n",
    "# Moderation Applied\n",
    "if 'moderation_applied' in raw_dataframes:\n",
    "    print(\"Cleaning moderation_applied table...\")\n",
    "    df_mod_applied = raw_dataframes['moderation_applied'].copy()\n",
    "    df_mod_applied = clean_strings(df_mod_applied)\n",
    "    df_mod_applied = convert_timestamps(df_mod_applied)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    json_cols = ['strategies', 'custom_instructions', 'highlighted_texts', 'refactored_response']\n",
    "    for col in json_cols:\n",
    "        if col in df_mod_applied.columns:\n",
    "            df_mod_applied = parse_json_column(df_mod_applied, col)\n",
    "    \n",
    "    if 'confirmed_preferred' in df_mod_applied.columns:\n",
    "        df_mod_applied['confirmed_preferred'] = df_mod_applied['confirmed_preferred'].astype(str).str.lower() == 'true'\n",
    "    if 'version_index' in df_mod_applied.columns:\n",
    "        df_mod_applied['version_index'] = pd.to_numeric(df_mod_applied['version_index'], errors='coerce')\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned moderation_applied: {len(df_mod_applied)} rows\")\n",
    "else:\n",
    "    df_mod_applied = pd.DataFrame()\n",
    "\n",
    "# Moderation Question Answer\n",
    "if 'moderation_question_answer' in raw_dataframes:\n",
    "    print(\"Cleaning moderation_question_answer table...\")\n",
    "    df_mod_qa = raw_dataframes['moderation_question_answer'].copy()\n",
    "    df_mod_qa = clean_strings(df_mod_qa)\n",
    "    df_mod_qa = convert_timestamps(df_mod_qa)\n",
    "    print(f\"  \u2713 Cleaned moderation_question_answer: {len(df_mod_qa)} rows\")\n",
    "else:\n",
    "    df_mod_qa = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mod_qa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this should be nothing. it's fine. don't pass to output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8: Clean Exit Quiz Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning exit_quiz_response table...\n",
      "  \u2713 Cleaned exit_quiz_response table: 5 rows, 17 columns\n"
     ]
    }
   ],
   "source": [
    "if 'exit_quiz_response' in raw_dataframes:\n",
    "    print(\"Cleaning exit_quiz_response table...\")\n",
    "    df_exit_quiz = raw_dataframes['exit_quiz_response'].copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_exit_quiz = clean_strings(df_exit_quiz)\n",
    "    df_exit_quiz = convert_timestamps(df_exit_quiz)\n",
    "    \n",
    "    # Parse JSON fields\n",
    "    df_exit_quiz = parse_json_column(df_exit_quiz, 'answers')\n",
    "    df_exit_quiz = parse_json_column(df_exit_quiz, 'score')\n",
    "    df_exit_quiz = parse_json_column(df_exit_quiz, 'meta')\n",
    "    \n",
    "    # Convert boolean and numeric columns\n",
    "    if 'is_current' in df_exit_quiz.columns:\n",
    "        df_exit_quiz['is_current'] = df_exit_quiz['is_current'].astype(str).str.lower() == 'true'\n",
    "    if 'attempt_number' in df_exit_quiz.columns:\n",
    "        df_exit_quiz['attempt_number'] = pd.to_numeric(df_exit_quiz['attempt_number'], errors='coerce')\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned exit_quiz_response table: {len(df_exit_quiz)} rows, {len(df_exit_quiz.columns)} columns\")\n",
    "else:\n",
    "    print(\"\u2717 Exit quiz table not found\")\n",
    "    df_exit_quiz = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exit_quiz = df_exit_quiz.drop(columns=['answers','score','answers_parsed','score_parsed','meta_parsed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>child_id</th>\n",
       "      <th>question_key</th>\n",
       "      <th>meta</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>attempt_number</th>\n",
       "      <th>is_current</th>\n",
       "      <th>created_at_datetime</th>\n",
       "      <th>updated_at_datetime</th>\n",
       "      <th>attempt_number_datetime</th>\n",
       "      <th>created_at_datetime_datetime</th>\n",
       "      <th>updated_at_datetime_datetime</th>\n",
       "      <th>attempt_number_datetime_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>785ba1d3-1145-4cd2-8a10-a22673caf031</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>a4f0224b-2d70-4b92-bfa7-4caef02f5df4</td>\n",
       "      <td>{'parentGender': 'non-binary', 'parentAge': '4...</td>\n",
       "      <td>{'page': 'exit-survey'}</td>\n",
       "      <td>1768343162551909785</td>\n",
       "      <td>1768343162551909785</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2026-01-13 22:26:02.551909924</td>\n",
       "      <td>2026-01-13 22:26:02.551909924</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "      <td>2026-01-13 22:26:02.551909924</td>\n",
       "      <td>2026-01-13 22:26:02.551909924</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90ed692c-c814-41f4-9c91-81d117a38809</td>\n",
       "      <td>055b5ebd-6a8b-4a75-b608-3c25f69a5197</td>\n",
       "      <td>de6a59d8-e18a-49da-b573-3277c1c50a0a</td>\n",
       "      <td>{'parentGender': 'female', 'parentAge': '18-24...</td>\n",
       "      <td>{'page': 'exit-survey'}</td>\n",
       "      <td>1768345610687093700</td>\n",
       "      <td>1768345610687093700</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2026-01-13 23:06:50.687093735</td>\n",
       "      <td>2026-01-13 23:06:50.687093735</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "      <td>2026-01-13 23:06:50.687093735</td>\n",
       "      <td>2026-01-13 23:06:50.687093735</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cb7e2e2f-83c9-4d57-a94b-9d108816e6ed</td>\n",
       "      <td>16a92a7a-e4f1-46d2-91c4-c0e32208411c</td>\n",
       "      <td>25d6122a-cdbc-40a8-817b-82edb4a75ef7</td>\n",
       "      <td>{'parentGender': 'female', 'parentAge': '45-54...</td>\n",
       "      <td>{'page': 'exit-survey'}</td>\n",
       "      <td>1768345647511205915</td>\n",
       "      <td>1768345647511205915</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2026-01-13 23:07:27.511205912</td>\n",
       "      <td>2026-01-13 23:07:27.511205912</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "      <td>2026-01-13 23:07:27.511205912</td>\n",
       "      <td>2026-01-13 23:07:27.511205912</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d437033d-b25b-4db6-9a8a-f74ce62b9f77</td>\n",
       "      <td>16a92a7a-e4f1-46d2-91c4-c0e32208411c</td>\n",
       "      <td>25d6122a-cdbc-40a8-817b-82edb4a75ef7</td>\n",
       "      <td>{'parentGender': 'non-binary', 'parentAge': 'p...</td>\n",
       "      <td>{'page': 'exit-survey'}</td>\n",
       "      <td>1768345697496201187</td>\n",
       "      <td>1768345697496201187</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2026-01-13 23:08:17.496201277</td>\n",
       "      <td>2026-01-13 23:08:17.496201277</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "      <td>2026-01-13 23:08:17.496201277</td>\n",
       "      <td>2026-01-13 23:08:17.496201277</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132c82f2-7c94-47bd-93b4-0ec2f56b2fca</td>\n",
       "      <td>055b5ebd-6a8b-4a75-b608-3c25f69a5197</td>\n",
       "      <td>619ea48a-a376-4fce-96f7-9ff4c87c06fe</td>\n",
       "      <td>{'parentGender': 'male', 'parentAge': '35-44',...</td>\n",
       "      <td>{'page': 'exit-survey'}</td>\n",
       "      <td>1768346110582149960</td>\n",
       "      <td>1768346110582149960</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2026-01-13 23:15:10.582149982</td>\n",
       "      <td>2026-01-13 23:15:10.582149982</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "      <td>2026-01-13 23:15:10.582149982</td>\n",
       "      <td>2026-01-13 23:15:10.582149982</td>\n",
       "      <td>1970-01-01 00:00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                               user_id  \\\n",
       "0  785ba1d3-1145-4cd2-8a10-a22673caf031  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "1  90ed692c-c814-41f4-9c91-81d117a38809  055b5ebd-6a8b-4a75-b608-3c25f69a5197   \n",
       "2  cb7e2e2f-83c9-4d57-a94b-9d108816e6ed  16a92a7a-e4f1-46d2-91c4-c0e32208411c   \n",
       "3  d437033d-b25b-4db6-9a8a-f74ce62b9f77  16a92a7a-e4f1-46d2-91c4-c0e32208411c   \n",
       "4  132c82f2-7c94-47bd-93b4-0ec2f56b2fca  055b5ebd-6a8b-4a75-b608-3c25f69a5197   \n",
       "\n",
       "                               child_id  \\\n",
       "0  a4f0224b-2d70-4b92-bfa7-4caef02f5df4   \n",
       "1  de6a59d8-e18a-49da-b573-3277c1c50a0a   \n",
       "2  25d6122a-cdbc-40a8-817b-82edb4a75ef7   \n",
       "3  25d6122a-cdbc-40a8-817b-82edb4a75ef7   \n",
       "4  619ea48a-a376-4fce-96f7-9ff4c87c06fe   \n",
       "\n",
       "                                        question_key                     meta  \\\n",
       "0  {'parentGender': 'non-binary', 'parentAge': '4...  {'page': 'exit-survey'}   \n",
       "1  {'parentGender': 'female', 'parentAge': '18-24...  {'page': 'exit-survey'}   \n",
       "2  {'parentGender': 'female', 'parentAge': '45-54...  {'page': 'exit-survey'}   \n",
       "3  {'parentGender': 'non-binary', 'parentAge': 'p...  {'page': 'exit-survey'}   \n",
       "4  {'parentGender': 'male', 'parentAge': '35-44',...  {'page': 'exit-survey'}   \n",
       "\n",
       "            created_at           updated_at  attempt_number  is_current  \\\n",
       "0  1768343162551909785  1768343162551909785               1       False   \n",
       "1  1768345610687093700  1768345610687093700               1       False   \n",
       "2  1768345647511205915  1768345647511205915               1       False   \n",
       "3  1768345697496201187  1768345697496201187               1       False   \n",
       "4  1768346110582149960  1768346110582149960               1       False   \n",
       "\n",
       "            created_at_datetime           updated_at_datetime  \\\n",
       "0 2026-01-13 22:26:02.551909924 2026-01-13 22:26:02.551909924   \n",
       "1 2026-01-13 23:06:50.687093735 2026-01-13 23:06:50.687093735   \n",
       "2 2026-01-13 23:07:27.511205912 2026-01-13 23:07:27.511205912   \n",
       "3 2026-01-13 23:08:17.496201277 2026-01-13 23:08:17.496201277   \n",
       "4 2026-01-13 23:15:10.582149982 2026-01-13 23:15:10.582149982   \n",
       "\n",
       "  attempt_number_datetime  created_at_datetime_datetime  \\\n",
       "0     1970-01-01 00:00:01 2026-01-13 22:26:02.551909924   \n",
       "1     1970-01-01 00:00:01 2026-01-13 23:06:50.687093735   \n",
       "2     1970-01-01 00:00:01 2026-01-13 23:07:27.511205912   \n",
       "3     1970-01-01 00:00:01 2026-01-13 23:08:17.496201277   \n",
       "4     1970-01-01 00:00:01 2026-01-13 23:15:10.582149982   \n",
       "\n",
       "   updated_at_datetime_datetime attempt_number_datetime_datetime  \n",
       "0 2026-01-13 22:26:02.551909924              1970-01-01 00:00:01  \n",
       "1 2026-01-13 23:06:50.687093735              1970-01-01 00:00:01  \n",
       "2 2026-01-13 23:07:27.511205912              1970-01-01 00:00:01  \n",
       "3 2026-01-13 23:08:17.496201277              1970-01-01 00:00:01  \n",
       "4 2026-01-13 23:15:10.582149982              1970-01-01 00:00:01  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exit_quiz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9: Clean Scenario Tables (if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Cleaning scenario_assignments table (INDEPENDENT)\n",
      "============================================================\n",
      "  Raw: 55 rows, 19 columns\n",
      "  \u2713 Cleaned scenario_assignments: 55 rows\n",
      "\n",
      "============================================================\n",
      "Cleaning scenarios table (INDEPENDENT)\n",
      "============================================================\n",
      "  Raw: 50 rows, 17 columns\n",
      "  \u2713 Cleaned scenarios: 50 rows\n",
      "\n",
      "============================================================\n",
      "Cleaning attention_check_scenarios table (INDEPENDENT)\n",
      "============================================================\n",
      "  Raw: 50 rows, 13 columns\n",
      "  \u2713 Cleaned attention_check_scenarios: 50 rows\n",
      "\n",
      "============================================================\n",
      "Cleaning assignment_session_activity table (INDEPENDENT)\n",
      "============================================================\n",
      "  Raw: 521 rows, 6 columns\n",
      "  \u2713 Cleaned assignment_session_activity: 521 rows\n",
      "\n",
      "============================================================\n",
      "All scenario tables cleaned independently\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clean each scenario table independently\n",
    "\n",
    "# 1. Scenario Assignments\n",
    "if 'scenario_assignments' in raw_dataframes:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Cleaning scenario_assignments table (INDEPENDENT)\")\n",
    "    print(\"=\"*60)\n",
    "    df_scenario_assignments = raw_dataframes['scenario_assignments'].copy()\n",
    "    print(f\"  Raw: {len(df_scenario_assignments)} rows, {len(df_scenario_assignments.columns)} columns\")\n",
    "    \n",
    "    df_scenario_assignments = clean_strings(df_scenario_assignments)\n",
    "    df_scenario_assignments = convert_timestamps_enhanced(df_scenario_assignments)\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['alpha', 'eligible_pool_size', 'n_assigned_before', 'weight', \n",
    "                   'sampling_prob', 'assignment_position', 'issue_any', 'duration_seconds']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_scenario_assignments.columns:\n",
    "            df_scenario_assignments[col] = pd.to_numeric(df_scenario_assignments[col], errors='coerce')\n",
    "    \n",
    "    # Parse JSON fields if any\n",
    "    for col in df_scenario_assignments.columns:\n",
    "        if df_scenario_assignments[col].dtype == 'object':\n",
    "            sample = df_scenario_assignments[col].dropna().astype(str)\n",
    "            if len(sample) > 0 and (sample.str.startswith('{').any() or sample.str.startswith('[').any()):\n",
    "                df_scenario_assignments = parse_json_column_enhanced(df_scenario_assignments, col)\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned scenario_assignments: {len(df_scenario_assignments)} rows\")\n",
    "    print()\n",
    "else:\n",
    "    df_scenario_assignments = pd.DataFrame()\n",
    "\n",
    "# 2. Scenarios\n",
    "if 'scenarios' in raw_dataframes:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Cleaning scenarios table (INDEPENDENT)\")\n",
    "    print(\"=\"*60)\n",
    "    df_scenarios = raw_dataframes['scenarios'].copy()\n",
    "    print(f\"  Raw: {len(df_scenarios)} rows, {len(df_scenarios.columns)} columns\")\n",
    "    \n",
    "    df_scenarios = clean_strings(df_scenarios)\n",
    "    df_scenarios = convert_timestamps_enhanced(df_scenarios)\n",
    "    \n",
    "    # Convert boolean and numeric columns\n",
    "    if 'is_active' in df_scenarios.columns:\n",
    "        df_scenarios['is_active'] = df_scenarios['is_active'].astype(str).str.lower() == 'true'\n",
    "    numeric_cols = ['n_assigned', 'n_completed', 'n_skipped', 'n_abandoned']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_scenarios.columns:\n",
    "            df_scenarios[col] = pd.to_numeric(df_scenarios[col], errors='coerce')\n",
    "    \n",
    "    # Parse JSON fields if any\n",
    "    for col in df_scenarios.columns:\n",
    "        if df_scenarios[col].dtype == 'object':\n",
    "            sample = df_scenarios[col].dropna().astype(str)\n",
    "            if len(sample) > 0 and (sample.str.startswith('{').any() or sample.str.startswith('[').any()):\n",
    "                df_scenarios = parse_json_column_enhanced(df_scenarios, col)\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned scenarios: {len(df_scenarios)} rows\")\n",
    "    print()\n",
    "else:\n",
    "    df_scenarios = pd.DataFrame()\n",
    "\n",
    "# 3. Attention Check Scenarios\n",
    "if 'attention_check_scenarios' in raw_dataframes:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Cleaning attention_check_scenarios table (INDEPENDENT)\")\n",
    "    print(\"=\"*60)\n",
    "    df_attention_checks = raw_dataframes['attention_check_scenarios'].copy()\n",
    "    print(f\"  Raw: {len(df_attention_checks)} rows, {len(df_attention_checks.columns)} columns\")\n",
    "    \n",
    "    df_attention_checks = clean_strings(df_attention_checks)\n",
    "    df_attention_checks = convert_timestamps_enhanced(df_attention_checks)\n",
    "    \n",
    "    if 'is_active' in df_attention_checks.columns:\n",
    "        df_attention_checks['is_active'] = df_attention_checks['is_active'].astype(str).str.lower() == 'true'\n",
    "    \n",
    "    # Parse JSON fields if any\n",
    "    for col in df_attention_checks.columns:\n",
    "        if df_attention_checks[col].dtype == 'object':\n",
    "            sample = df_attention_checks[col].dropna().astype(str)\n",
    "            if len(sample) > 0 and (sample.str.startswith('{').any() or sample.str.startswith('[').any()):\n",
    "                df_attention_checks = parse_json_column_enhanced(df_attention_checks, col)\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned attention_check_scenarios: {len(df_attention_checks)} rows\")\n",
    "    print()\n",
    "else:\n",
    "    df_attention_checks = pd.DataFrame()\n",
    "\n",
    "# 4. Assignment Session Activity\n",
    "if 'assignment_session_activity' in raw_dataframes:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Cleaning assignment_session_activity table (INDEPENDENT)\")\n",
    "    print(\"=\"*60)\n",
    "    df_activity = raw_dataframes['assignment_session_activity'].copy()\n",
    "    print(f\"  Raw: {len(df_activity)} rows, {len(df_activity.columns)} columns\")\n",
    "    \n",
    "    df_activity = clean_strings(df_activity)\n",
    "    df_activity = convert_timestamps_enhanced(df_activity)\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['session_number', 'active_ms_delta', 'cumulative_ms']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_activity.columns:\n",
    "            df_activity[col] = pd.to_numeric(df_activity[col], errors='coerce')\n",
    "    \n",
    "    # Parse JSON fields if any\n",
    "    for col in df_activity.columns:\n",
    "        if df_activity[col].dtype == 'object':\n",
    "            sample = df_activity[col].dropna().astype(str)\n",
    "            if len(sample) > 0 and (sample.str.startswith('{').any() or sample.str.startswith('[').any()):\n",
    "                df_activity = parse_json_column_enhanced(df_activity, col)\n",
    "    \n",
    "    print(f\"  \u2713 Cleaned assignment_session_activity: {len(df_activity)} rows\")\n",
    "    print()\n",
    "else:\n",
    "    df_activity = pd.DataFrame()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"All scenario tables cleaned independently\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scenario_assignments = df_scenario_assignments.drop(columns=['duration_seconds','status_datetime','duration_seconds_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assignment_id</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>scenario_id</th>\n",
       "      <th>child_profile_id</th>\n",
       "      <th>status</th>\n",
       "      <th>assigned_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>alpha</th>\n",
       "      <th>eligible_pool_size</th>\n",
       "      <th>...</th>\n",
       "      <th>weight</th>\n",
       "      <th>sampling_prob</th>\n",
       "      <th>assignment_position</th>\n",
       "      <th>issue_any</th>\n",
       "      <th>skip_stage</th>\n",
       "      <th>skip_reason</th>\n",
       "      <th>skip_reason_text</th>\n",
       "      <th>assigned_at_datetime</th>\n",
       "      <th>started_at_datetime</th>\n",
       "      <th>ended_at_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5607a819-5380-48f4-8d58-cba79734a14b</td>\n",
       "      <td>055b5ebd-6a8b-4a75-b608-3c25f69a5197</td>\n",
       "      <td>scenario_5e747e63-ded9-4d3b-b51f-ab526af77781</td>\n",
       "      <td>619ea48a-a376-4fce-96f7-9ff4c87c06fe</td>\n",
       "      <td>skipped</td>\n",
       "      <td>1768364210294</td>\n",
       "      <td>1768364210761</td>\n",
       "      <td>1768364243983</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.022058823529411766</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>step1</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>User marked scenario as not applicable</td>\n",
       "      <td>1970-01-21 11:12:44.210294</td>\n",
       "      <td>1970-01-21 11:12:44.210761</td>\n",
       "      <td>1970-01-21 11:12:44.243983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3e9b6295-43f4-463e-af2a-815f434e836f</td>\n",
       "      <td>055b5ebd-6a8b-4a75-b608-3c25f69a5197</td>\n",
       "      <td>scenario_2d558a8a-700b-4dcc-98fe-ed5ba63cd9c6</td>\n",
       "      <td>619ea48a-a376-4fce-96f7-9ff4c87c06fe</td>\n",
       "      <td>started</td>\n",
       "      <td>1768364210654</td>\n",
       "      <td>1768364272460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.026785714285714288</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-21 11:12:44.210654</td>\n",
       "      <td>1970-01-21 11:12:44.272460</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbeb40ea-dbf4-46d9-94df-9907660020b3</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>scenario_c68ebc57-bf31-4422-a3d1-89c1e988eddc</td>\n",
       "      <td>a4f0224b-2d70-4b92-bfa7-4caef02f5df4</td>\n",
       "      <td>completed</td>\n",
       "      <td>1768329996708</td>\n",
       "      <td>1768329999106</td>\n",
       "      <td>1768330057784</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-21 11:12:09.996708</td>\n",
       "      <td>1970-01-21 11:12:09.999106</td>\n",
       "      <td>1970-01-21 11:12:10.057784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7b743ee2-c068-4f79-bc35-a2a60c473f4d</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>scenario_72f1a8d3-c869-4951-912b-fffb534b6b88</td>\n",
       "      <td>a4f0224b-2d70-4b92-bfa7-4caef02f5df4</td>\n",
       "      <td>abandoned</td>\n",
       "      <td>1768329997346</td>\n",
       "      <td>1768330078974</td>\n",
       "      <td>1768331880280</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02127659574468085</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-21 11:12:09.997346</td>\n",
       "      <td>1970-01-21 11:12:10.078974</td>\n",
       "      <td>1970-01-21 11:12:11.880280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d00035e6-bede-48a0-bd46-df779facf441</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>scenario_3b65b510-4be5-4f63-bfb0-647a2993bc14</td>\n",
       "      <td>a4f0224b-2d70-4b92-bfa7-4caef02f5df4</td>\n",
       "      <td>completed</td>\n",
       "      <td>1768329996915</td>\n",
       "      <td>1768333290370</td>\n",
       "      <td>1768333312549</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02040816326530612</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-21 11:12:09.996915</td>\n",
       "      <td>1970-01-21 11:12:13.290370</td>\n",
       "      <td>1970-01-21 11:12:13.312549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows \u00d7 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          assignment_id                        participant_id  \\\n",
       "0  5607a819-5380-48f4-8d58-cba79734a14b  055b5ebd-6a8b-4a75-b608-3c25f69a5197   \n",
       "1  3e9b6295-43f4-463e-af2a-815f434e836f  055b5ebd-6a8b-4a75-b608-3c25f69a5197   \n",
       "2  dbeb40ea-dbf4-46d9-94df-9907660020b3  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "3  7b743ee2-c068-4f79-bc35-a2a60c473f4d  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "4  d00035e6-bede-48a0-bd46-df779facf441  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "\n",
       "                                     scenario_id  \\\n",
       "0  scenario_5e747e63-ded9-4d3b-b51f-ab526af77781   \n",
       "1  scenario_2d558a8a-700b-4dcc-98fe-ed5ba63cd9c6   \n",
       "2  scenario_c68ebc57-bf31-4422-a3d1-89c1e988eddc   \n",
       "3  scenario_72f1a8d3-c869-4951-912b-fffb534b6b88   \n",
       "4  scenario_3b65b510-4be5-4f63-bfb0-647a2993bc14   \n",
       "\n",
       "                       child_profile_id     status    assigned_at  \\\n",
       "0  619ea48a-a376-4fce-96f7-9ff4c87c06fe    skipped  1768364210294   \n",
       "1  619ea48a-a376-4fce-96f7-9ff4c87c06fe    started  1768364210654   \n",
       "2  a4f0224b-2d70-4b92-bfa7-4caef02f5df4  completed  1768329996708   \n",
       "3  a4f0224b-2d70-4b92-bfa7-4caef02f5df4  abandoned  1768329997346   \n",
       "4  a4f0224b-2d70-4b92-bfa7-4caef02f5df4  completed  1768329996915   \n",
       "\n",
       "      started_at       ended_at alpha eligible_pool_size  ... weight  \\\n",
       "0  1768364210761  1768364243983     1                 32  ...    0.5   \n",
       "1  1768364272460            NaN     1                 27  ...    0.5   \n",
       "2  1768329999106  1768330057784     1                 50  ...      1   \n",
       "3  1768330078974  1768331880280     1                 47  ...      1   \n",
       "4  1768333290370  1768333312549     1                 49  ...      1   \n",
       "\n",
       "          sampling_prob assignment_position issue_any skip_stage  \\\n",
       "0  0.022058823529411766                   0       NaN      step1   \n",
       "1  0.026785714285714288                   5       NaN        NaN   \n",
       "2                  0.02                   0         1        NaN   \n",
       "3   0.02127659574468085                   3       NaN        NaN   \n",
       "4   0.02040816326530612                   1         1        NaN   \n",
       "\n",
       "      skip_reason                        skip_reason_text  \\\n",
       "0  not_applicable  User marked scenario as not applicable   \n",
       "1             NaN                                     NaN   \n",
       "2             NaN                                     NaN   \n",
       "3             NaN                                     NaN   \n",
       "4             NaN                                     NaN   \n",
       "\n",
       "        assigned_at_datetime        started_at_datetime  \\\n",
       "0 1970-01-21 11:12:44.210294 1970-01-21 11:12:44.210761   \n",
       "1 1970-01-21 11:12:44.210654 1970-01-21 11:12:44.272460   \n",
       "2 1970-01-21 11:12:09.996708 1970-01-21 11:12:09.999106   \n",
       "3 1970-01-21 11:12:09.997346 1970-01-21 11:12:10.078974   \n",
       "4 1970-01-21 11:12:09.996915 1970-01-21 11:12:13.290370   \n",
       "\n",
       "           ended_at_datetime  \n",
       "0 1970-01-21 11:12:44.243983  \n",
       "1                        NaT  \n",
       "2 1970-01-21 11:12:10.057784  \n",
       "3 1970-01-21 11:12:11.880280  \n",
       "4 1970-01-21 11:12:13.312549  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scenario_assignments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scenarios=df_scenarios.drop(columns=['trait','polarity','prompt_style','model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario_id</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>response_text</th>\n",
       "      <th>set_name</th>\n",
       "      <th>domain</th>\n",
       "      <th>source</th>\n",
       "      <th>is_active</th>\n",
       "      <th>n_assigned</th>\n",
       "      <th>n_completed</th>\n",
       "      <th>n_skipped</th>\n",
       "      <th>n_abandoned</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>created_at_datetime</th>\n",
       "      <th>updated_at_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scenario_4feb0c7f-37ca-41b0-9a2a-9f4282ea6df3</td>\n",
       "      <td>what if lightning hits our house, will my bed ...</td>\n",
       "      <td>Lightning hitting a house can be scary to thin...</td>\n",
       "      <td>test</td>\n",
       "      <td>Casual Knowledge Domain</td>\n",
       "      <td>admin_upload</td>\n",
       "      <td>t</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1768329928736</td>\n",
       "      <td>1768329928736</td>\n",
       "      <td>1970-01-21 11:12:09.928736</td>\n",
       "      <td>1970-01-21 11:12:09.928736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scenario_200798d9-c15c-45f0-81aa-300257f7876e</td>\n",
       "      <td>I don't wanna hug grandma but she gets mad, sh...</td>\n",
       "      <td>You don\u2019t have to hug anyone if you don\u2019t want...</td>\n",
       "      <td>test</td>\n",
       "      <td>Casual Knowledge Domain</td>\n",
       "      <td>admin_upload</td>\n",
       "      <td>t</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1768329928736</td>\n",
       "      <td>1768329928736</td>\n",
       "      <td>1970-01-21 11:12:09.928736</td>\n",
       "      <td>1970-01-21 11:12:09.928736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scenario_5f75dc66-dc34-4853-aa4f-5974cc1e5f3c</td>\n",
       "      <td>give me reasons I can tell my mom so she'll le...</td>\n",
       "      <td>Here are some honest, respectful reasons you c...</td>\n",
       "      <td>test</td>\n",
       "      <td>Relationship Domain</td>\n",
       "      <td>admin_upload</td>\n",
       "      <td>t</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1768329928736</td>\n",
       "      <td>1768329928736</td>\n",
       "      <td>1970-01-21 11:12:09.928736</td>\n",
       "      <td>1970-01-21 11:12:09.928736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scenario_bb8880ce-df1f-4a18-92fb-d86c2fe29026</td>\n",
       "      <td>how do I ask to play with them without them sa...</td>\n",
       "      <td>It really hurts when you ask and they say no. ...</td>\n",
       "      <td>test</td>\n",
       "      <td>Relationship Domain</td>\n",
       "      <td>admin_upload</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1768329928736</td>\n",
       "      <td>1768364210522</td>\n",
       "      <td>1970-01-21 11:12:09.928736</td>\n",
       "      <td>1970-01-21 11:12:44.210522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scenario_c57a6e21-ec88-49f5-a51d-f0ba3d35730b</td>\n",
       "      <td>give me a science fair idea that I can actuall...</td>\n",
       "      <td>Here are a few science fair ideas you can fini...</td>\n",
       "      <td>test</td>\n",
       "      <td>Academic Domain</td>\n",
       "      <td>admin_upload</td>\n",
       "      <td>t</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1768329928736</td>\n",
       "      <td>1768345917270</td>\n",
       "      <td>1970-01-21 11:12:09.928736</td>\n",
       "      <td>1970-01-21 11:12:25.917270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     scenario_id  \\\n",
       "0  scenario_4feb0c7f-37ca-41b0-9a2a-9f4282ea6df3   \n",
       "1  scenario_200798d9-c15c-45f0-81aa-300257f7876e   \n",
       "2  scenario_5f75dc66-dc34-4853-aa4f-5974cc1e5f3c   \n",
       "3  scenario_bb8880ce-df1f-4a18-92fb-d86c2fe29026   \n",
       "4  scenario_c57a6e21-ec88-49f5-a51d-f0ba3d35730b   \n",
       "\n",
       "                                         prompt_text  \\\n",
       "0  what if lightning hits our house, will my bed ...   \n",
       "1  I don't wanna hug grandma but she gets mad, sh...   \n",
       "2  give me reasons I can tell my mom so she'll le...   \n",
       "3  how do I ask to play with them without them sa...   \n",
       "4  give me a science fair idea that I can actuall...   \n",
       "\n",
       "                                       response_text set_name  \\\n",
       "0  Lightning hitting a house can be scary to thin...     test   \n",
       "1  You don\u2019t have to hug anyone if you don\u2019t want...     test   \n",
       "2  Here are some honest, respectful reasons you c...     test   \n",
       "3  It really hurts when you ask and they say no. ...     test   \n",
       "4  Here are a few science fair ideas you can fini...     test   \n",
       "\n",
       "                    domain        source is_active n_assigned n_completed  \\\n",
       "0  Casual Knowledge Domain  admin_upload         t          0           0   \n",
       "1  Casual Knowledge Domain  admin_upload         t          0           0   \n",
       "2      Relationship Domain  admin_upload         t          0           0   \n",
       "3      Relationship Domain  admin_upload         t          1           0   \n",
       "4          Academic Domain  admin_upload         t          2           0   \n",
       "\n",
       "  n_skipped n_abandoned     created_at     updated_at  \\\n",
       "0         0           0  1768329928736  1768329928736   \n",
       "1         0           0  1768329928736  1768329928736   \n",
       "2         0           0  1768329928736  1768329928736   \n",
       "3         0           0  1768329928736  1768364210522   \n",
       "4         1           0  1768329928736  1768345917270   \n",
       "\n",
       "         created_at_datetime        updated_at_datetime  \n",
       "0 1970-01-21 11:12:09.928736 1970-01-21 11:12:09.928736  \n",
       "1 1970-01-21 11:12:09.928736 1970-01-21 11:12:09.928736  \n",
       "2 1970-01-21 11:12:09.928736 1970-01-21 11:12:09.928736  \n",
       "3 1970-01-21 11:12:09.928736 1970-01-21 11:12:44.210522  \n",
       "4 1970-01-21 11:12:09.928736 1970-01-21 11:12:25.917270  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scenarios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attention_checks.drop(columns=['trait_theme','trait_phrase','sentiment','trait_index','prompt_index','sentiment_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario_id</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>response_text</th>\n",
       "      <th>set_name</th>\n",
       "      <th>is_active</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>created_at_datetime</th>\n",
       "      <th>updated_at_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ac_aa954a2c-6955-40e7-aea2-941fb665167a</td>\n",
       "      <td>what if you see a ghost but you think its your...</td>\n",
       "      <td>Sometimes our brains can \u201ctrick\u201d us\u2014especially...</td>\n",
       "      <td>test</td>\n",
       "      <td>t</td>\n",
       "      <td>admin_upload</td>\n",
       "      <td>1768329941909</td>\n",
       "      <td>1768329941909</td>\n",
       "      <td>1970-01-21 11:12:09.941909</td>\n",
       "      <td>1970-01-21 11:12:09.941909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac_c645a8c5-50c5-4a1b-b0b6-b1dc326a3083</td>\n",
       "      <td>How do I stop procrastinating my college essay...</td>\n",
       "      <td>That feeling is super common. Your brain isn\u2019t...</td>\n",
       "      <td>test</td>\n",
       "      <td>t</td>\n",
       "      <td>admin_upload</td>\n",
       "      <td>1768329941909</td>\n",
       "      <td>1768329941909</td>\n",
       "      <td>1970-01-21 11:12:09.941909</td>\n",
       "      <td>1970-01-21 11:12:09.941909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ac_af07da7b-97d8-4613-9520-4bfea726db56</td>\n",
       "      <td>Why does my bike wobble more when I go slow, i...</td>\n",
       "      <td>When you ride a bike, it\u2019s normal for it to fe...</td>\n",
       "      <td>test</td>\n",
       "      <td>t</td>\n",
       "      <td>admin_upload</td>\n",
       "      <td>1768329941909</td>\n",
       "      <td>1768329941909</td>\n",
       "      <td>1970-01-21 11:12:09.941909</td>\n",
       "      <td>1970-01-21 11:12:09.941909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ac_303cdc8f-b3a9-4391-ac48-a472e3602e59</td>\n",
       "      <td>How do I correct my teacher about my name with...</td>\n",
       "      <td>You can correct your teacher in a way that\u2019s q...</td>\n",
       "      <td>test</td>\n",
       "      <td>t</td>\n",
       "      <td>admin_upload</td>\n",
       "      <td>1768329941909</td>\n",
       "      <td>1768329941909</td>\n",
       "      <td>1970-01-21 11:12:09.941909</td>\n",
       "      <td>1970-01-21 11:12:09.941909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ac_fad5329d-1395-456e-bdd0-ec477b431e40</td>\n",
       "      <td>Nobody lets me in their game and it makes my t...</td>\n",
       "      <td>That sounds really hard. When people don\u2019t let...</td>\n",
       "      <td>test</td>\n",
       "      <td>t</td>\n",
       "      <td>admin_upload</td>\n",
       "      <td>1768329941909</td>\n",
       "      <td>1768329941909</td>\n",
       "      <td>1970-01-21 11:12:09.941909</td>\n",
       "      <td>1970-01-21 11:12:09.941909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               scenario_id  \\\n",
       "0  ac_aa954a2c-6955-40e7-aea2-941fb665167a   \n",
       "1  ac_c645a8c5-50c5-4a1b-b0b6-b1dc326a3083   \n",
       "2  ac_af07da7b-97d8-4613-9520-4bfea726db56   \n",
       "3  ac_303cdc8f-b3a9-4391-ac48-a472e3602e59   \n",
       "4  ac_fad5329d-1395-456e-bdd0-ec477b431e40   \n",
       "\n",
       "                                         prompt_text  \\\n",
       "0  what if you see a ghost but you think its your...   \n",
       "1  How do I stop procrastinating my college essay...   \n",
       "2  Why does my bike wobble more when I go slow, i...   \n",
       "3  How do I correct my teacher about my name with...   \n",
       "4  Nobody lets me in their game and it makes my t...   \n",
       "\n",
       "                                       response_text set_name is_active  \\\n",
       "0  Sometimes our brains can \u201ctrick\u201d us\u2014especially...     test         t   \n",
       "1  That feeling is super common. Your brain isn\u2019t...     test         t   \n",
       "2  When you ride a bike, it\u2019s normal for it to fe...     test         t   \n",
       "3  You can correct your teacher in a way that\u2019s q...     test         t   \n",
       "4  That sounds really hard. When people don\u2019t let...     test         t   \n",
       "\n",
       "         source     created_at     updated_at        created_at_datetime  \\\n",
       "0  admin_upload  1768329941909  1768329941909 1970-01-21 11:12:09.941909   \n",
       "1  admin_upload  1768329941909  1768329941909 1970-01-21 11:12:09.941909   \n",
       "2  admin_upload  1768329941909  1768329941909 1970-01-21 11:12:09.941909   \n",
       "3  admin_upload  1768329941909  1768329941909 1970-01-21 11:12:09.941909   \n",
       "4  admin_upload  1768329941909  1768329941909 1970-01-21 11:12:09.941909   \n",
       "\n",
       "         updated_at_datetime  \n",
       "0 1970-01-21 11:12:09.941909  \n",
       "1 1970-01-21 11:12:09.941909  \n",
       "2 1970-01-21 11:12:09.941909  \n",
       "3 1970-01-21 11:12:09.941909  \n",
       "4 1970-01-21 11:12:09.941909  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attention_checks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_number</th>\n",
       "      <th>active_ms_delta</th>\n",
       "      <th>cumulative_ms</th>\n",
       "      <th>created_at</th>\n",
       "      <th>cumulative_ms_datetime</th>\n",
       "      <th>created_at_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50221205-492d-4d84-8e2a-7237210fcce2</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1768329980845</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1970-01-21 11:12:09.980845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>855be001-5646-4024-818c-974251c60746</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1768329981950</td>\n",
       "      <td>1970-01-01 00:16:40</td>\n",
       "      <td>1970-01-21 11:12:09.981950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f33b92c8-aaf6-45a4-86e5-f5543a057b9a</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1768329981975</td>\n",
       "      <td>1970-01-01 00:16:40</td>\n",
       "      <td>1970-01-21 11:12:09.981975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14de855f-161f-46ee-afb0-7b6db3586cd1</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>1</td>\n",
       "      <td>16000</td>\n",
       "      <td>17000</td>\n",
       "      <td>1768329998047</td>\n",
       "      <td>1970-01-01 04:43:20</td>\n",
       "      <td>1970-01-21 11:12:09.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6975c74e-e3e7-48a5-9e17-71b3ac8885e7</td>\n",
       "      <td>1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17000</td>\n",
       "      <td>1768329998082</td>\n",
       "      <td>1970-01-01 04:43:20</td>\n",
       "      <td>1970-01-21 11:12:09.998082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                               user_id  \\\n",
       "0  50221205-492d-4d84-8e2a-7237210fcce2  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "1  855be001-5646-4024-818c-974251c60746  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "2  f33b92c8-aaf6-45a4-86e5-f5543a057b9a  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "3  14de855f-161f-46ee-afb0-7b6db3586cd1  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "4  6975c74e-e3e7-48a5-9e17-71b3ac8885e7  1fc4965c-0c08-43b3-9bfd-02cbf5a02cf6   \n",
       "\n",
       "   session_number  active_ms_delta  cumulative_ms     created_at  \\\n",
       "0               1                0              0  1768329980845   \n",
       "1               1             1000           1000  1768329981950   \n",
       "2               1                0           1000  1768329981975   \n",
       "3               1            16000          17000  1768329998047   \n",
       "4               1                0          17000  1768329998082   \n",
       "\n",
       "  cumulative_ms_datetime        created_at_datetime  \n",
       "0    1970-01-01 00:00:00 1970-01-21 11:12:09.980845  \n",
       "1    1970-01-01 00:16:40 1970-01-21 11:12:09.981950  \n",
       "2    1970-01-01 00:16:40 1970-01-21 11:12:09.981975  \n",
       "3    1970-01-01 04:43:20 1970-01-21 11:12:09.998047  \n",
       "4    1970-01-01 04:43:20 1970-01-21 11:12:09.998082  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_activity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tables = [\n",
    "    (df_users, 'df_users'),\n",
    "    (df_child_profiles, 'df_child_profiles'),\n",
    "    (df_selections, 'df_selections'),\n",
    "    (df_exit_quiz, 'df_exit_quiz'),\n",
    "    (df_scenario_assignments, 'df_scenario_assignments'),\n",
    "    (df_scenarios, 'df_scenarios'),\n",
    "    (df_attention_checks, 'df_attention_checks'),\n",
    "    (df_activity, 'df_activity')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.5: Explore Scenario Tables\n",
    "\n",
    "Display the head of all scenario-related tables to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display head of all scenario-related tables\n",
    "print(\"=\"*70)\n",
    "print(\"SCENARIO TABLES OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "scenario_tables = {\n",
    "    'scenarios': df_scenarios if 'df_scenarios' in globals() else None,\n",
    "    'scenario_assignments': df_scenario_assignments if 'df_scenario_assignments' in globals() else None,\n",
    "    'attention_check_scenarios': df_attention_checks if 'df_attention_checks' in globals() else None,\n",
    "    'assignment_session_activity': df_session_activity if 'df_session_activity' in globals() else None,\n",
    "}\n",
    "\n",
    "for table_name, df in scenario_tables.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TABLE: {table_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if df is not None and len(df) > 0:\n",
    "        print(f\"Shape: {df.shape[0]} rows \u00d7 {df.shape[1]} columns\")\n",
    "        print(f\"\\nColumns: {', '.join(df.columns.tolist())}\")\n",
    "        print(f\"\\nFirst 3 rows:\")\n",
    "        print(\"-\"*70)\n",
    "        display(df.head(3))\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  Table not found or empty\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SCENARIO TABLES SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "for table_name, df in scenario_tables.items():\n",
    "    if df is not None and len(df) > 0:\n",
    "        print(f\"\u2713 {table_name}: {len(df)} rows\")\n",
    "    else:\n",
    "        print(f\"\u26aa {table_name}: empty or not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cleaned DataFrames to data_exports/...\n",
      "  \u2713 df_users: 4 rows -> df_users.csv, df_users.pkl\n",
      "  \u2713 df_child_profiles: 4 rows -> df_child_profiles.csv, df_child_profiles.pkl\n",
      "  \u2713 df_selections: 47 rows -> df_selections.csv, df_selections.pkl\n",
      "  \u2713 df_exit_quiz: 5 rows -> df_exit_quiz.csv, df_exit_quiz.pkl\n",
      "  \u2713 df_scenario_assignments: 55 rows -> df_scenario_assignments.csv, df_scenario_assignments.pkl\n",
      "  \u2713 df_scenarios: 50 rows -> df_scenarios.csv, df_scenarios.pkl\n",
      "  \u2713 df_attention_checks: 50 rows -> df_attention_checks.csv, df_attention_checks.pkl\n",
      "  \u2713 df_activity: 521 rows -> df_activity.csv, df_activity.pkl\n",
      "\n",
      "\u2713 Saved 8 tables\n",
      "\u2713 Summary saved to data_exports/summary.json\n"
     ]
    }
   ],
   "source": [
    "# Create output directory with timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = Path(\"data-exports\") / timestamp\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Saving cleaned DataFrames to {output_dir}/...\")\n",
    "\n",
    "saved_files = {}\n",
    "for df, table_name in cleaned_tables:\n",
    "    if len(df) > 0:\n",
    "        # Save as pickle only (for git upload)\n",
    "        pkl_file = output_dir / f\"{table_name}.pkl\"\n",
    "        df.to_pickle(pkl_file)\n",
    "        \n",
    "        saved_files[table_name] = {\n",
    "            'pkl': str(pkl_file),\n",
    "            'rows': len(df),\n",
    "            'columns': len(df.columns)\n",
    "        }\n",
    "        print(f\"  \u2713 {table_name}: {len(df)} rows \u00d7 {len(df.columns)} cols -> {pkl_file.name}\")\n",
    "\n",
    "print(f\"\\n\u2713 Saved {len(saved_files)} tables to {output_dir}\")\n",
    "\n",
    "# Create summary JSON\n",
    "summary = {\n",
    "    'extraction_date': datetime.now().isoformat(),\n",
    "    'dump_file': str(dump_file) if 'dump_file' in locals() else None,\n",
    "    'tables': {}\n",
    "}\n",
    "\n",
    "for df, table_name in cleaned_tables:\n",
    "    if len(df) > 0:\n",
    "        summary['tables'][table_name] = {\n",
    "            'row_count': len(df),\n",
    "            'column_count': len(df.columns),\n",
    "            'columns': list(df.columns),\n",
    "            'memory_usage_mb': round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2)\n",
    "        }\n",
    "\n",
    "summary_file = output_dir / \"summary.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\u2713 Summary saved to {summary_file.name}\")\n",
    "print(f\"\\nOutput location: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "All relevant tables have been extracted, cleaned, and saved to the `data_exports/` directory.\n",
    "\n",
    "You can now load the cleaned data using:\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_pickle('data_exports/table_name.pkl')\n",
    "```\n",
    "\n",
    "The data has been:\n",
    "- Parsed from the PostgreSQL dump\n",
    "- Cleaned (null bytes removed, strings normalized)\n",
    "- Transformed (timestamps converted, JSON parsed, types corrected)\n",
    "- Saved in both CSV and pickle formats for easy access"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parentalcontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}