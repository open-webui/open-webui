{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Database Dump Transformation and Cleaning\n",
        "\n",
        "This notebook transforms the PostgreSQL dump file into cleaned pandas DataFrames for analysis.\n",
        "\n",
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# # Install required dependencies\n",
        "# import sys\n",
        "# import subprocess\n",
        "\n",
        "# def install_package(package):\n",
        "#     \"\"\"Install a package using pip.\"\"\"\n",
        "#     try:\n",
        "#         __import__(package)\n",
        "#         print(f\"✓ {package} is already installed\")\n",
        "#     except ImportError:\n",
        "#         print(f\"Installing {package}...\")\n",
        "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
        "#         print(f\"✓ {package} installed successfully\")\n",
        "\n",
        "# # Install required packages\n",
        "# required_packages = [\n",
        "#     \"pandas\",\n",
        "#     \"numpy\",\n",
        "#     \"jupyter\",\n",
        "#     \"ipython\"\n",
        "# ]\n",
        "\n",
        "# print(\"Checking and installing dependencies...\\n\")\n",
        "# for package in required_packages:\n",
        "#     install_package(package)\n",
        "\n",
        "# print(\"\\n✓ All dependencies are ready!\")\n",
        "# print(\"\\nNote: If you need to convert PostgreSQL custom format dumps, you'll also need:\")\n",
        "# print(\"  - PostgreSQL client tools (pg_restore)\")\n",
        "# print(\"  - macOS: brew install postgresql\")\n",
        "# print(\"  - Ubuntu: sudo apt-get install postgresql-client\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking and installing dependencies...\n",
            "\n",
            "Installing pandas...\n",
            "✓ pandas installed successfully\n",
            "✓ numpy is already installed\n",
            "✓ jupyter is already installed\n",
            "Installing ipython...\n",
            "✓ ipython installed successfully\n",
            "\n",
            "✓ All dependencies are ready!\n",
            "\n",
            "Note: If you need to convert PostgreSQL custom format dumps, you'll also need:\n",
            "  - PostgreSQL client tools (pg_restore)\n",
            "  - macOS: brew install postgresql\n",
            "  - Ubuntu: sudo apt-get install postgresql-client\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pandas version: 3.0.0\n",
            "NumPy version: 2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Locate and Verify Dump File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Find the dump file\n",
        "dump_files = [\n",
        "    Path.home() / \"Downloads\" / \"b078-20260113-215725.dump\",\n",
        "    Path(\"/workspace\") / \"heroku_psql_181025.dump\",\n",
        "    Path(\"./heroku_psql_181025.dump\"),\n",
        "    Path(\"./b078-20260113-215725.dump\")\n",
        "]\n",
        "\n",
        "dump_file = None\n",
        "for df in dump_files:\n",
        "    if df.exists():\n",
        "        dump_file = df\n",
        "        break\n",
        "\n",
        "if dump_file:\n",
        "    print(f\"Found dump file: {dump_file}\")\n",
        "    print(f\"File size: {dump_file.stat().st_size / 1024:.2f} KB\")\n",
        "    \n",
        "    # Check file format\n",
        "    with open(dump_file, 'rb') as f:\n",
        "        header = f.read(5)\n",
        "        if header == b'PGDMP':\n",
        "            print(\"Format: PostgreSQL custom format (requires pg_restore)\")\n",
        "            file_format = 'custom'\n",
        "        else:\n",
        "            print(\"Format: Plain SQL\")\n",
        "            file_format = 'sql'\n",
        "else:\n",
        "    print(\"ERROR: Could not find dump file.\")\n",
        "    print(\"Please ensure the dump file is in one of these locations:\")\n",
        "    for df in dump_files:\n",
        "        print(f\"  - {df}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found dump file: /Users/johndriscoll/Downloads/b078-20260113-215725.dump\n",
            "File size: 248.83 KB\n",
            "Format: PostgreSQL custom format (requires pg_restore)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manual Conversion Instructions\n",
        "\n",
        "If automatic conversion failed, convert the dump file manually using one of these methods:\n",
        "\n",
        "### Method 1: Using PostgreSQL 16 (Recommended)\n",
        "\n",
        "Since PostgreSQL 16 is \"keg-only\" (not in PATH), use the full path:\n",
        "\n",
        "**For Apple Silicon Macs:**\n",
        "```bash\n",
        "/opt/homebrew/opt/postgresql@17/bin/pg_restore --no-owner --no-privileges -f ~/Downloads/b078-20260113-215725.sql ~/Downloads/b078-20260113-215725.dump\n",
        "```\n",
        "\n",
        "**For Intel Macs:**\n",
        "```bash\n",
        "/usr/local/opt/postgresql@16/bin/pg_restore --no-owner --no-privileges -f ~/Downloads/b078-20260113-215725.sql ~/Downloads/b078-20260113-215725.dump\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Step 2: Convert Dump to SQL (if needed)\n",
        "\n",
        "This cell will automatically convert the PostgreSQL custom format dump to SQL format."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Check if we need to convert\n",
        "if 'dump_file' in globals() and 'file_format' in globals() and file_format == 'custom':\n",
        "    print(\"=\"*60)\n",
        "    print(\"CONVERTING CUSTOM FORMAT DUMP TO SQL\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Define output SQL file\n",
        "    sql_file = dump_file.with_suffix('.sql')\n",
        "    \n",
        "    # Check if SQL file already exists\n",
        "    if sql_file.exists():\n",
        "        print(f\"\\n✓ SQL file already exists: {sql_file}\")\n",
        "        print(f\"  File size: {sql_file.stat().st_size / 1024:.2f} KB\")\n",
        "        print(\"  Using existing file. Skipping conversion.\")\n",
        "        dump_file = sql_file\n",
        "        file_format = 'sql'\n",
        "    else:\n",
        "        print(f\"\\nInput dump file: {dump_file}\")\n",
        "        print(f\"Output SQL file: {sql_file}\")\n",
        "        \n",
        "        # Try to find pg_restore in various locations\n",
        "        pg_restore_paths = [\n",
        "            # PostgreSQL 17 (newest)\n",
        "            '/opt/homebrew/opt/postgresql@17/bin/pg_restore',\n",
        "            '/usr/local/opt/postgresql@17/bin/pg_restore',\n",
        "            # PostgreSQL 16\n",
        "            '/opt/homebrew/opt/postgresql@16/bin/pg_restore',\n",
        "            '/usr/local/opt/postgresql@16/bin/pg_restore',\n",
        "            # PostgreSQL 15\n",
        "            '/opt/homebrew/opt/postgresql@15/bin/pg_restore',\n",
        "            '/usr/local/opt/postgresql@15/bin/pg_restore',\n",
        "            # Standard locations\n",
        "            '/opt/homebrew/bin/pg_restore',\n",
        "            '/usr/local/bin/pg_restore',\n",
        "            'pg_restore',  # In PATH\n",
        "        ]\n",
        "        \n",
        "        pg_restore_path = None\n",
        "        version_info = None\n",
        "        \n",
        "        print(\"\\nSearching for pg_restore...\")\n",
        "        for path in pg_restore_paths:\n",
        "            try:\n",
        "                if path == 'pg_restore':\n",
        "                    # Check if it's in PATH\n",
        "                    result = subprocess.run(\n",
        "                        ['which', 'pg_restore'],\n",
        "                        capture_output=True,\n",
        "                        text=True\n",
        "                    )\n",
        "                    if result.returncode == 0:\n",
        "                        path = result.stdout.strip()\n",
        "                    else:\n",
        "                        continue\n",
        "                \n",
        "                if os.path.exists(path):\n",
        "                    # Check version\n",
        "                    result = subprocess.run(\n",
        "                        [path, '--version'],\n",
        "                        capture_output=True,\n",
        "                        text=True,\n",
        "                        check=True\n",
        "                    )\n",
        "                    pg_restore_path = path\n",
        "                    version_info = result.stdout.strip()\n",
        "                    print(f\"✓ Found: {version_info}\")\n",
        "                    print(f\"  Location: {path}\")\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        \n",
        "        if pg_restore_path:\n",
        "            print(f\"\\nConverting dump to SQL...\")\n",
        "            print(f\"  Command: {pg_restore_path} --no-owner --no-privileges -f {sql_file} {dump_file}\")\n",
        "            \n",
        "            try:\n",
        "                result = subprocess.run(\n",
        "                    [pg_restore_path, '--no-owner', '--no-privileges', '-f', str(sql_file), str(dump_file)],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=300  # 5 minute timeout\n",
        "                )\n",
        "                \n",
        "                if result.returncode == 0:\n",
        "                    if sql_file.exists():\n",
        "                        print(f\"\\n✓✓✓ CONVERSION SUCCESSFUL! ✓✓✓\")\n",
        "                        print(f\"  Created: {sql_file}\")\n",
        "                        print(f\"  File size: {sql_file.stat().st_size / 1024:.2f} KB\")\n",
        "                        dump_file = sql_file\n",
        "                        file_format = 'sql'\n",
        "                        print(\"\\n✓ Ready to proceed with parsing!\")\n",
        "                    else:\n",
        "                        print(f\"\\n✗ Conversion reported success but SQL file not found!\")\n",
        "                        print(f\"  Expected location: {sql_file}\")\n",
        "                else:\n",
        "                    error_msg = result.stderr.strip() if result.stderr else result.stdout.strip()\n",
        "                    print(f\"\\n✗ CONVERSION FAILED\")\n",
        "                    print(f\"  Error: {error_msg}\")\n",
        "                    \n",
        "                    if 'unsupported version' in error_msg.lower():\n",
        "                        print(\"\\n\" + \"=\"*60)\n",
        "                        print(\"VERSION INCOMPATIBILITY\")\n",
        "                        print(\"=\"*60)\n",
        "                        print(f\"\\nThe dump file format version is not supported by {version_info}.\")\n",
        "                        print(\"\\nSOLUTIONS:\")\n",
        "                        print(\"\\n1. Try a newer PostgreSQL version (17 or 18)\")\n",
        "                        print(\"   brew install postgresql@17\")\n",
        "                        print(f\"   /opt/homebrew/opt/postgresql@17/bin/pg_restore -f {sql_file} {dump_file}\")\n",
        "                        print(\"\\n2. Use Docker:\")\n",
        "                        print(f\"   docker run --rm -v ~/Downloads:/data postgres:17 \\\\\")\n",
        "                        print(f\"     pg_restore -f /data/{sql_file.name} /data/{dump_file.name}\")\n",
        "                        print(\"\\n3. Check if SQL file was already created manually\")\n",
        "                    else:\n",
        "                        print(\"\\nPlease check the error message above and try manual conversion.\")\n",
        "                        print(\"See the manual conversion instructions in the previous cell.\")\n",
        "            except subprocess.TimeoutExpired:\n",
        "                print(\"\\n✗ Conversion timed out after 5 minutes.\")\n",
        "                print(\"  The dump file might be very large or there's an issue.\")\n",
        "            except Exception as e:\n",
        "                print(f\"\\n✗ Error during conversion: {e}\")\n",
        "        else:\n",
        "            print(\"\\n✗ pg_restore not found!\")\n",
        "            print(\"\\nPlease install PostgreSQL client tools:\")\n",
        "            print(\"  brew install postgresql@17\")\n",
        "            print(\"\\nOr use Docker (see manual conversion instructions above).\")\n",
        "            print(\"\\nAfter manual conversion, set:\")\n",
        "            print(f\"  dump_file = Path('{sql_file}')\")\n",
        "            print(f\"  file_format = 'sql'\")\n",
        "            print(\"\\nThen proceed to the next cell.\")\n",
        "\n",
        "elif 'dump_file' in globals() and 'file_format' in globals() and file_format == 'sql':\n",
        "    print(\"✓ Dump file is already in SQL format.\")\n",
        "    print(f\"  File: {dump_file}\")\n",
        "    if isinstance(dump_file, Path) and dump_file.exists():\n",
        "        print(f\"  File size: {dump_file.stat().st_size / 1024:.2f} KB\")\n",
        "    print(\"\\nReady to proceed with parsing!\")\n",
        "else:\n",
        "    print(\"⚠ Cannot convert: dump_file or file_format not set.\")\n",
        "    print(\"Please run the previous cell to locate and detect the dump file format.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "! /opt/homebrew/opt/postgresql@17/bin/pg_restore --no-owner --no-privileges -v -f ~/Downloads/b078-20260113-215725.sql ~/Downloads/b078-20260113-215725.dump"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pg_restore: creating SCHEMA \"public\"\n",
            "pg_restore: creating EXTENSION \"pg_stat_statements\"\n",
            "pg_restore: creating COMMENT \"EXTENSION \"pg_stat_statements\"\"\n",
            "pg_restore: creating TABLE \"public.alembic_version\"\n",
            "pg_restore: creating TABLE \"public.assignment_session_activity\"\n",
            "pg_restore: creating TABLE \"public.attention_check_question\"\n",
            "pg_restore: creating TABLE \"public.attention_check_response\"\n",
            "pg_restore: creating TABLE \"public.attention_check_scenarios\"\n",
            "pg_restore: creating TABLE \"public.auth\"\n",
            "pg_restore: creating TABLE \"public.channel\"\n",
            "pg_restore: creating TABLE \"public.channel_member\"\n",
            "pg_restore: creating TABLE \"public.chat\"\n",
            "pg_restore: creating TABLE \"public.chatidtag\"\n",
            "pg_restore: creating TABLE \"public.child_profile\"\n",
            "pg_restore: creating TABLE \"public.config\"\n",
            "pg_restore: creating SEQUENCE \"public.config_id_seq\"\n",
            "pg_restore: creating SEQUENCE OWNED BY \"public.config_id_seq\"\n",
            "pg_restore: creating TABLE \"public.consent_audit\"\n",
            "pg_restore: creating TABLE \"public.document\"\n",
            "pg_restore: creating TABLE \"public.exit_quiz_response\"\n",
            "pg_restore: creating TABLE \"public.feedback\"\n",
            "pg_restore: creating TABLE \"public.file\"\n",
            "pg_restore: creating TABLE \"public.folder\"\n",
            "pg_restore: creating TABLE \"public.function\"\n",
            "pg_restore: creating TABLE \"public.group\"\n",
            "pg_restore: creating TABLE \"public.knowledge\"\n",
            "pg_restore: creating TABLE \"public.memory\"\n",
            "pg_restore: creating TABLE \"public.message\"\n",
            "pg_restore: creating TABLE \"public.message_reaction\"\n",
            "pg_restore: creating TABLE \"public.model\"\n",
            "pg_restore: creating TABLE \"public.moderation_session\"\n",
            "pg_restore: creating TABLE \"public.moderation_session_activity\"\n",
            "pg_restore: creating TABLE \"public.note\"\n",
            "pg_restore: creating TABLE \"public.oauth_session\"\n",
            "pg_restore: creating TABLE \"public.prompt\"\n",
            "pg_restore: creating TABLE \"public.scenario_assignments\"\n",
            "pg_restore: creating TABLE \"public.scenarios\"\n",
            "pg_restore: creating TABLE \"public.selection\"\n",
            "pg_restore: creating TABLE \"public.tag\"\n",
            "pg_restore: creating TABLE \"public.tool\"\n",
            "pg_restore: creating TABLE \"public.user\"\n",
            "pg_restore: creating DEFAULT \"public.config id\"\n",
            "pg_restore: processing data for table \"public.alembic_version\"\n",
            "pg_restore: processing data for table \"public.assignment_session_activity\"\n",
            "pg_restore: processing data for table \"public.attention_check_question\"\n",
            "pg_restore: processing data for table \"public.attention_check_response\"\n",
            "pg_restore: processing data for table \"public.attention_check_scenarios\"\n",
            "pg_restore: processing data for table \"public.auth\"\n",
            "pg_restore: processing data for table \"public.channel\"\n",
            "pg_restore: processing data for table \"public.channel_member\"\n",
            "pg_restore: processing data for table \"public.chat\"\n",
            "pg_restore: processing data for table \"public.chatidtag\"\n",
            "pg_restore: processing data for table \"public.child_profile\"\n",
            "pg_restore: processing data for table \"public.config\"\n",
            "pg_restore: processing data for table \"public.consent_audit\"\n",
            "pg_restore: processing data for table \"public.document\"\n",
            "pg_restore: processing data for table \"public.exit_quiz_response\"\n",
            "pg_restore: processing data for table \"public.feedback\"\n",
            "pg_restore: processing data for table \"public.file\"\n",
            "pg_restore: processing data for table \"public.folder\"\n",
            "pg_restore: processing data for table \"public.function\"\n",
            "pg_restore: processing data for table \"public.group\"\n",
            "pg_restore: processing data for table \"public.knowledge\"\n",
            "pg_restore: processing data for table \"public.memory\"\n",
            "pg_restore: processing data for table \"public.message\"\n",
            "pg_restore: processing data for table \"public.message_reaction\"\n",
            "pg_restore: processing data for table \"public.model\"\n",
            "pg_restore: processing data for table \"public.moderation_session\"\n",
            "pg_restore: processing data for table \"public.moderation_session_activity\"\n",
            "pg_restore: processing data for table \"public.note\"\n",
            "pg_restore: processing data for table \"public.oauth_session\"\n",
            "pg_restore: processing data for table \"public.prompt\"\n",
            "pg_restore: processing data for table \"public.scenario_assignments\"\n",
            "pg_restore: processing data for table \"public.scenarios\"\n",
            "pg_restore: processing data for table \"public.selection\"\n",
            "pg_restore: processing data for table \"public.tag\"\n",
            "pg_restore: processing data for table \"public.tool\"\n",
            "pg_restore: processing data for table \"public.user\"\n",
            "pg_restore: executing SEQUENCE SET config_id_seq\n",
            "pg_restore: creating CONSTRAINT \"public.alembic_version alembic_version_pkc\"\n",
            "pg_restore: creating CONSTRAINT \"public.assignment_session_activity assignment_session_activity_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.attention_check_question attention_check_question_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.attention_check_response attention_check_response_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.attention_check_scenarios attention_check_scenarios_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.auth auth_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.channel_member channel_member_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.channel channel_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.chat chat_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.chat chat_share_id_key\"\n",
            "pg_restore: creating CONSTRAINT \"public.chatidtag chatidtag_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.child_profile child_profile_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.config config_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.consent_audit consent_audit_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.document document_name_key\"\n",
            "pg_restore: creating CONSTRAINT \"public.document document_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.exit_quiz_response exit_quiz_response_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.feedback feedback_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.file file_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.folder folder_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.function function_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.group group_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.knowledge knowledge_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.memory memory_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.message message_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.message_reaction message_reaction_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.model model_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.moderation_session_activity moderation_session_activity_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.moderation_session moderation_session_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.note note_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.oauth_session oauth_session_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.tag pk_id_user_id\"\n",
            "pg_restore: creating CONSTRAINT \"public.prompt prompt_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.scenario_assignments scenario_assignments_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.scenarios scenarios_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.selection selection_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.tool tool_pkey\"\n",
            "pg_restore: creating CONSTRAINT \"public.user uq_user_prolific_pid\"\n",
            "pg_restore: creating CONSTRAINT \"public.user user_api_key_key\"\n",
            "pg_restore: creating CONSTRAINT \"public.user user_oauth_sub_key\"\n",
            "pg_restore: creating CONSTRAINT \"public.user user_pkey\"\n",
            "pg_restore: creating INDEX \"public.folder_id_idx\"\n",
            "pg_restore: creating INDEX \"public.folder_id_user_id_idx\"\n",
            "pg_restore: creating INDEX \"public.idx_ac_scenarios_is_active\"\n",
            "pg_restore: creating INDEX \"public.idx_ac_scenarios_set_name\"\n",
            "pg_restore: creating INDEX \"public.idx_ac_scenarios_trait_theme\"\n",
            "pg_restore: creating INDEX \"public.idx_acq_created_at\"\n",
            "pg_restore: creating INDEX \"public.idx_acr_created_at\"\n",
            "pg_restore: creating INDEX \"public.idx_acr_question\"\n",
            "pg_restore: creating INDEX \"public.idx_acr_user\"\n",
            "pg_restore: creating INDEX \"public.idx_assignment_activity_created_at\"\n",
            "pg_restore: creating INDEX \"public.idx_assignments_assigned_at\"\n",
            "pg_restore: creating INDEX \"public.idx_assignments_participant_id\"\n",
            "pg_restore: creating INDEX \"public.idx_assignments_participant_scenario\"\n",
            "pg_restore: creating INDEX \"public.idx_assignments_scenario_id\"\n",
            "pg_restore: creating INDEX \"public.idx_assignments_status\"\n",
            "pg_restore: creating INDEX \"public.idx_child_profile_attempt\"\n",
            "pg_restore: creating INDEX \"public.idx_child_profile_created_at\"\n",
            "pg_restore: creating INDEX \"public.idx_child_profile_user_current\"\n",
            "pg_restore: creating INDEX \"public.idx_child_profile_user_id\"\n",
            "pg_restore: creating INDEX \"public.idx_child_profile_user_session_current\"\n",
            "pg_restore: creating INDEX \"public.idx_consent_audit_prolific\"\n",
            "pg_restore: creating INDEX \"public.idx_consent_audit_timestamp\"\n",
            "pg_restore: creating INDEX \"public.idx_consent_audit_user\"\n",
            "pg_restore: creating INDEX \"public.idx_exit_quiz_attempt\"\n",
            "pg_restore: creating INDEX \"public.idx_exit_quiz_child_id\"\n",
            "pg_restore: creating INDEX \"public.idx_exit_quiz_created_at\"\n",
            "pg_restore: creating INDEX \"public.idx_exit_quiz_user_current\"\n",
            "pg_restore: creating INDEX \"public.idx_exit_quiz_user_id\"\n",
            "pg_restore: creating INDEX \"public.idx_mod_activity_created_at\"\n",
            "pg_restore: creating INDEX \"public.idx_mod_activity_user_child_session\"\n",
            "pg_restore: creating INDEX \"public.idx_mod_session_final\"\n",
            "pg_restore: creating INDEX \"public.idx_mod_session_user_session\"\n",
            "pg_restore: creating INDEX \"public.idx_moderation_session_child_id\"\n",
            "pg_restore: creating INDEX \"public.idx_moderation_session_created_at\"\n",
            "pg_restore: creating INDEX \"public.idx_moderation_session_user_id\"\n",
            "pg_restore: creating INDEX \"public.idx_oauth_session_expires_at\"\n",
            "pg_restore: creating INDEX \"public.idx_oauth_session_user_id\"\n",
            "pg_restore: creating INDEX \"public.idx_oauth_session_user_provider\"\n",
            "pg_restore: creating INDEX \"public.idx_scenarios_is_active\"\n",
            "pg_restore: creating INDEX \"public.idx_scenarios_n_assigned\"\n",
            "pg_restore: creating INDEX \"public.idx_scenarios_polarity\"\n",
            "pg_restore: creating INDEX \"public.idx_scenarios_set_name\"\n",
            "pg_restore: creating INDEX \"public.idx_scenarios_source\"\n",
            "pg_restore: creating INDEX \"public.idx_scenarios_trait\"\n",
            "pg_restore: creating INDEX \"public.idx_selection_assignment_id\"\n",
            "pg_restore: creating INDEX \"public.idx_selection_chat_id\"\n",
            "pg_restore: creating INDEX \"public.idx_selection_created_at\"\n",
            "pg_restore: creating INDEX \"public.idx_selection_message_id\"\n",
            "pg_restore: creating INDEX \"public.idx_selection_scenario_id\"\n",
            "pg_restore: creating INDEX \"public.idx_selection_source\"\n",
            "pg_restore: creating INDEX \"public.idx_selection_user_id\"\n",
            "pg_restore: creating INDEX \"public.is_global_idx\"\n",
            "pg_restore: creating INDEX \"public.updated_at_user_id_idx\"\n",
            "pg_restore: creating INDEX \"public.user_id_archived_idx\"\n",
            "pg_restore: creating INDEX \"public.user_id_idx\"\n",
            "pg_restore: creating INDEX \"public.user_id_pinned_idx\"\n",
            "pg_restore: creating FK CONSTRAINT \"public.oauth_session oauth_session_user_id_fkey\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dump_file = \"~/Downloads/b078-20260113-215725.sql\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Parse SQL Dump and Extract Tables\n",
        "\n",
        "We'll parse the SQL dump to extract table data into pandas DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def parse_sql_dump(sql_file_path):\n",
        "    \"\"\"Parse SQL dump file and extract table data.\"\"\"\n",
        "    print(f\"Reading SQL file: {sql_file_path}\")\n",
        "    \n",
        "    with open(sql_file_path, 'rb') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    # Try to decode\n",
        "    try:\n",
        "        text_content = content.decode('utf-8')\n",
        "    except UnicodeDecodeError:\n",
        "        print(\"Warning: UTF-8 decode failed, trying latin-1...\")\n",
        "        text_content = content.decode('latin-1', errors='ignore')\n",
        "    \n",
        "    tables_data = {}\n",
        "    \n",
        "    # Find all COPY statements\n",
        "    copy_pattern = r'COPY \"public\"\\.\"(\\w+)\"\\s*\\(([^)]+)\\)\\s+FROM stdin;'\n",
        "    copy_matches = list(re.finditer(copy_pattern, text_content, re.MULTILINE))\n",
        "    \n",
        "    print(f\"Found {len(copy_matches)} COPY statements\")\n",
        "    \n",
        "    for i, copy_match in enumerate(copy_matches):\n",
        "        table_name = copy_match.group(1)\n",
        "        columns_str = copy_match.group(2)\n",
        "        \n",
        "        # Parse column names\n",
        "        columns = [col.strip().strip('\"') for col in columns_str.split(',')]\n",
        "        \n",
        "        # Find data section\n",
        "        start_pos = copy_match.end()\n",
        "        \n",
        "        # Find end marker\n",
        "        if i + 1 < len(copy_matches):\n",
        "            end_pos = copy_matches[i + 1].start()\n",
        "        else:\n",
        "            end_marker = text_content.find('\\\\.', start_pos)\n",
        "            end_pos = end_marker if end_marker != -1 else len(text_content)\n",
        "        \n",
        "        data_section = text_content[start_pos:end_pos]\n",
        "        \n",
        "        # Parse tab-separated values\n",
        "        rows = []\n",
        "        lines = data_section.strip().split('\\n')\n",
        "        \n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith('\\\\'):\n",
        "                continue\n",
        "            \n",
        "            # Split by tab\n",
        "            values = line.split('\\t')\n",
        "            \n",
        "            if len(values) != len(columns):\n",
        "                continue\n",
        "            \n",
        "            row = {}\n",
        "            for col, val in zip(columns, values):\n",
        "                # Handle NULL\n",
        "                if val == '\\\\N':\n",
        "                    row[col] = None\n",
        "                else:\n",
        "                    # Try to parse JSON\n",
        "                    if val.startswith('{') or val.startswith('['):\n",
        "                        try:\n",
        "                            row[col] = json.loads(val)\n",
        "                        except:\n",
        "                            row[col] = val\n",
        "                    else:\n",
        "                        row[col] = val\n",
        "            \n",
        "            rows.append(row)\n",
        "        \n",
        "        if rows:\n",
        "            tables_data[table_name] = pd.DataFrame(rows)\n",
        "            print(f\"  ✓ {table_name}: {len(rows)} rows\")\n",
        "    \n",
        "    return tables_data\n",
        "\n",
        "# Parse the dump\n",
        "# Handle path expansion and format detection\n",
        "if 'dump_file' in globals() and dump_file:\n",
        "    # Convert to Path object and expand ~ if it's a string\n",
        "    if isinstance(dump_file, str):\n",
        "        dump_file = Path(dump_file).expanduser()\n",
        "    elif not isinstance(dump_file, Path):\n",
        "        dump_file = Path(dump_file)\n",
        "    \n",
        "    # Auto-detect format if not set\n",
        "    if 'file_format' not in globals() or not file_format:\n",
        "        if dump_file.suffix == '.sql':\n",
        "            file_format = 'sql'\n",
        "        elif dump_file.suffix == '.dump':\n",
        "            file_format = 'custom'\n",
        "        else:\n",
        "            # Check file header\n",
        "            with open(dump_file, 'rb') as f:\n",
        "                header = f.read(5)\n",
        "                if header == b'PGDMP':\n",
        "                    file_format = 'custom'\n",
        "                else:\n",
        "                    file_format = 'sql'\n",
        "    \n",
        "    # Verify file exists\n",
        "    if not dump_file.exists():\n",
        "        print(f\"ERROR: File not found: {dump_file}\")\n",
        "        print(f\"Please check the path and try again.\")\n",
        "        raw_dataframes = {}\n",
        "    elif file_format == 'sql':\n",
        "        print(f\"Parsing SQL dump: {dump_file}\")\n",
        "        raw_dataframes = parse_sql_dump(str(dump_file))\n",
        "        print(f\"\\nTotal tables extracted: {len(raw_dataframes)}\")\n",
        "        print(f\"Tables: {', '.join(sorted(raw_dataframes.keys()))}\")\n",
        "    else:\n",
        "        print(f\"Cannot proceed: dump file is in {file_format} format, not SQL.\")\n",
        "        print(\"Please convert to SQL format first (see conversion cells above).\")\n",
        "        raw_dataframes = {}\n",
        "else:\n",
        "    print(\"ERROR: dump_file variable not set.\")\n",
        "    print(\"Please set dump_file to the path of your SQL dump file.\")\n",
        "    print(\"Example: dump_file = Path('~/Downloads/b078-20260113-215725.sql')\")\n",
        "    raw_dataframes = {}"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cannot proceed: dump file is in custom format, not SQL.\n",
            "Please convert to SQL format first (see conversion cells above).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Define Relevant Tables\n",
        "\n",
        "We'll focus on these tables for analysis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RELEVANT_TABLES = [\n",
        "    'user',\n",
        "    'chat',\n",
        "    'message',\n",
        "    'child_profile',\n",
        "    'selection',\n",
        "    'moderation_scenario',\n",
        "    'moderation_session',\n",
        "    'moderation_applied',\n",
        "    'moderation_question_answer',\n",
        "    'exit_quiz_response',\n",
        "    'scenario_assignments',\n",
        "    'scenarios',\n",
        "    'attention_check_scenarios',\n",
        "    'assignment_session_activity',\n",
        "]\n",
        "\n",
        "print(\"Relevant tables for analysis:\")\n",
        "for table in RELEVANT_TABLES:\n",
        "    status = \"✓\" if table in raw_dataframes else \"✗\"\n",
        "    count = len(raw_dataframes[table]) if table in raw_dataframes else 0\n",
        "    print(f\"  {status} {table}: {count} rows\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Clean and Transform Data\n",
        "\n",
        "Now we'll clean and transform each relevant table systematically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1: Helper Functions for Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def convert_timestamps(df, timestamp_cols=None):\n",
        "    \"\"\"Convert timestamp columns to datetime.\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    if timestamp_cols is None:\n",
        "        # Auto-detect timestamp columns\n",
        "        timestamp_cols = [col for col in df.columns \n",
        "                         if 'at' in col.lower() or 'time' in col.lower()]\n",
        "    \n",
        "    for col in timestamp_cols:\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            # Try numeric conversion first\n",
        "            if df[col].dtype in ['int64', 'float64']:\n",
        "                sample_val = df[col].dropna()\n",
        "                if len(sample_val) > 0:\n",
        "                    val = sample_val.iloc[0]\n",
        "                    # Determine unit based on magnitude\n",
        "                    if val > 1e12:\n",
        "                        # Nanoseconds\n",
        "                        df[f'{col}_datetime'] = pd.to_datetime(df[col] / 1e9, unit='s', errors='coerce')\n",
        "                    elif val > 1e9:\n",
        "                        # Milliseconds\n",
        "                        df[f'{col}_datetime'] = pd.to_datetime(df[col], unit='ms', errors='coerce')\n",
        "                    else:\n",
        "                        # Seconds\n",
        "                        df[f'{col}_datetime'] = pd.to_datetime(df[col], unit='s', errors='coerce')\n",
        "            else:\n",
        "                # Try direct datetime conversion\n",
        "                df[f'{col}_datetime'] = pd.to_datetime(df[col], errors='coerce')\n",
        "        except Exception as e:\n",
        "            print(f\"    Warning: Could not convert {col}: {e}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def clean_strings(df):\n",
        "    \"\"\"Clean string columns.\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            # Convert to string, handle None\n",
        "            df[col] = df[col].astype(str).replace('None', None).replace('nan', None)\n",
        "            # Remove null bytes\n",
        "            df[col] = df[col].str.replace('\\x00', '', regex=False)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def parse_json_column(df, col_name, new_col_name=None):\n",
        "    \"\"\"Parse JSON column into Python dict/list.\"\"\"\n",
        "    if new_col_name is None:\n",
        "        new_col_name = f'{col_name}_parsed'\n",
        "    \n",
        "    if col_name not in df.columns:\n",
        "        return df\n",
        "    \n",
        "    def parse_json(val):\n",
        "        if isinstance(val, (dict, list)):\n",
        "            return val\n",
        "        if isinstance(val, str):\n",
        "            if val.startswith('{') or val.startswith('['):\n",
        "                try:\n",
        "                    return json.loads(val)\n",
        "                except:\n",
        "                    return None\n",
        "        return None\n",
        "    \n",
        "    df[new_col_name] = df[col_name].apply(parse_json)\n",
        "    return df\n",
        "\n",
        "print(\"Helper functions defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2: Clean User Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if 'user' in raw_dataframes:\n",
        "    print(\"Cleaning user table...\")\n",
        "    df_users = raw_dataframes['user'].copy()\n",
        "    \n",
        "    # Basic cleaning\n",
        "    df_users = clean_strings(df_users)\n",
        "    df_users = convert_timestamps(df_users)\n",
        "    \n",
        "    # Parse JSON fields\n",
        "    df_users = parse_json_column(df_users, 'info')\n",
        "    df_users = parse_json_column(df_users, 'settings')\n",
        "    \n",
        "    # Convert date_of_birth if present\n",
        "    if 'date_of_birth' in df_users.columns:\n",
        "        df_users['date_of_birth'] = pd.to_datetime(df_users['date_of_birth'], errors='coerce')\n",
        "    \n",
        "    print(f\"  ✓ Cleaned user table: {len(df_users)} rows, {len(df_users.columns)} columns\")\n",
        "    print(f\"  Columns: {', '.join(df_users.columns[:10])}...\" if len(df_users.columns) > 10 else f\"  Columns: {', '.join(df_users.columns)}\")\n",
        "else:\n",
        "    print(\"✗ User table not found\")\n",
        "    df_users = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3: Clean Chat Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if 'chat' in raw_dataframes:\n",
        "    print(\"Cleaning chat table...\")\n",
        "    df_chats = raw_dataframes['chat'].copy()\n",
        "    \n",
        "    # Basic cleaning\n",
        "    df_chats = clean_strings(df_chats)\n",
        "    df_chats = convert_timestamps(df_chats)\n",
        "    \n",
        "    # Parse JSON fields\n",
        "    df_chats = parse_json_column(df_chats, 'chat')\n",
        "    df_chats = parse_json_column(df_chats, 'meta')\n",
        "    \n",
        "    # Extract message count from chat JSON\n",
        "    if 'chat_parsed' in df_chats.columns:\n",
        "        def count_messages(chat_data):\n",
        "            if isinstance(chat_data, dict):\n",
        "                history = chat_data.get('history', {})\n",
        "                messages = history.get('messages', {})\n",
        "                if isinstance(messages, dict):\n",
        "                    return len(messages)\n",
        "            return 0\n",
        "        \n",
        "        df_chats['message_count'] = df_chats['chat_parsed'].apply(count_messages)\n",
        "    \n",
        "    # Convert boolean columns\n",
        "    for col in ['archived', 'pinned']:\n",
        "        if col in df_chats.columns:\n",
        "            df_chats[col] = df_chats[col].astype(str).str.lower() == 'true'\n",
        "    \n",
        "    print(f\"  ✓ Cleaned chat table: {len(df_chats)} rows, {len(df_chats.columns)} columns\")\n",
        "    print(f\"  Total messages across all chats: {df_chats['message_count'].sum() if 'message_count' in df_chats.columns else 'N/A'}\")\n",
        "else:\n",
        "    print(\"✗ Chat table not found\")\n",
        "    df_chats = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4: Clean Message Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if 'message' in raw_dataframes:\n",
        "    print(\"Cleaning message table...\")\n",
        "    df_messages = raw_dataframes['message'].copy()\n",
        "    \n",
        "    # Basic cleaning\n",
        "    df_messages = clean_strings(df_messages)\n",
        "    df_messages = convert_timestamps(df_messages)\n",
        "    \n",
        "    # Parse JSON fields\n",
        "    df_messages = parse_json_column(df_messages, 'data')\n",
        "    df_messages = parse_json_column(df_messages, 'meta')\n",
        "    \n",
        "    print(f\"  ✓ Cleaned message table: {len(df_messages)} rows, {len(df_messages.columns)} columns\")\n",
        "    if 'role' in df_messages.columns:\n",
        "        print(f\"  Messages by role:\")\n",
        "        print(df_messages['role'].value_counts().to_string())\n",
        "else:\n",
        "    print(\"✗ Message table not found\")\n",
        "    df_messages = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5: Clean Child Profile Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if 'child_profile' in raw_dataframes:\n",
        "    print(\"Cleaning child_profile table...\")\n",
        "    df_child_profiles = raw_dataframes['child_profile'].copy()\n",
        "    \n",
        "    # Basic cleaning\n",
        "    df_child_profiles = clean_strings(df_child_profiles)\n",
        "    df_child_profiles = convert_timestamps(df_child_profiles)\n",
        "    \n",
        "    # Parse JSON fields if any\n",
        "    for col in df_child_profiles.columns:\n",
        "        if df_child_profiles[col].dtype == 'object':\n",
        "            sample = df_child_profiles[col].dropna().astype(str)\n",
        "            if len(sample) > 0 and sample.str.startswith('{').any():\n",
        "                df_child_profiles = parse_json_column(df_child_profiles, col)\n",
        "    \n",
        "    # Convert boolean columns\n",
        "    for col in ['is_current', 'is_only_child']:\n",
        "        if col in df_child_profiles.columns:\n",
        "            df_child_profiles[col] = df_child_profiles[col].astype(str).str.lower() == 'true'\n",
        "    \n",
        "    # Convert numeric columns\n",
        "    for col in ['attempt_number', 'session_number']:\n",
        "        if col in df_child_profiles.columns:\n",
        "            df_child_profiles[col] = pd.to_numeric(df_child_profiles[col], errors='coerce')\n",
        "    \n",
        "    print(f\"  ✓ Cleaned child_profile table: {len(df_child_profiles)} rows, {len(df_child_profiles.columns)} columns\")\n",
        "    print(f\"  Unique users: {df_child_profiles['user_id'].nunique() if 'user_id' in df_child_profiles.columns else 'N/A'}\")\n",
        "else:\n",
        "    print(\"✗ Child profile table not found\")\n",
        "    df_child_profiles = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.6: Clean Selection Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if 'selection' in raw_dataframes:\n",
        "    print(\"Cleaning selection table...\")\n",
        "    df_selections = raw_dataframes['selection'].copy()\n",
        "    \n",
        "    # Basic cleaning\n",
        "    df_selections = clean_strings(df_selections)\n",
        "    df_selections = convert_timestamps(df_selections)\n",
        "    \n",
        "    # Parse JSON fields\n",
        "    df_selections = parse_json_column(df_selections, 'meta')\n",
        "    \n",
        "    # Convert numeric columns\n",
        "    for col in ['start_offset', 'end_offset']:\n",
        "        if col in df_selections.columns:\n",
        "            df_selections[col] = pd.to_numeric(df_selections[col], errors='coerce')\n",
        "    \n",
        "    print(f\"  ✓ Cleaned selection table: {len(df_selections)} rows, {len(df_selections.columns)} columns\")\n",
        "    if 'role' in df_selections.columns:\n",
        "        print(f\"  Selections by role:\")\n",
        "        print(df_selections['role'].value_counts().to_string())\n",
        "    if 'source' in df_selections.columns:\n",
        "        print(f\"  Selections by source:\")\n",
        "        print(df_selections['source'].value_counts().to_string())\n",
        "else:\n",
        "    print(\"✗ Selection table not found\")\n",
        "    df_selections = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.7: Clean Moderation Tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Moderation Scenario\n",
        "if 'moderation_scenario' in raw_dataframes:\n",
        "    print(\"Cleaning moderation_scenario table...\")\n",
        "    df_mod_scenarios = raw_dataframes['moderation_scenario'].copy()\n",
        "    df_mod_scenarios = clean_strings(df_mod_scenarios)\n",
        "    df_mod_scenarios = convert_timestamps(df_mod_scenarios)\n",
        "    \n",
        "    # Convert boolean columns\n",
        "    for col in ['is_applicable']:\n",
        "        if col in df_mod_scenarios.columns:\n",
        "            df_mod_scenarios[col] = df_mod_scenarios[col].astype(str).str.lower() == 'true'\n",
        "    \n",
        "    print(f\"  ✓ Cleaned moderation_scenario: {len(df_mod_scenarios)} rows\")\n",
        "else:\n",
        "    df_mod_scenarios = pd.DataFrame()\n",
        "\n",
        "# Moderation Session\n",
        "if 'moderation_session' in raw_dataframes:\n",
        "    print(\"Cleaning moderation_session table...\")\n",
        "    df_mod_sessions = raw_dataframes['moderation_session'].copy()\n",
        "    df_mod_sessions = clean_strings(df_mod_sessions)\n",
        "    df_mod_sessions = convert_timestamps(df_mod_sessions)\n",
        "    \n",
        "    # Parse JSON fields\n",
        "    json_cols = ['strategies', 'custom_instructions', 'highlighted_texts', \n",
        "                 'refactored_response', 'session_metadata']\n",
        "    for col in json_cols:\n",
        "        if col in df_mod_sessions.columns:\n",
        "            df_mod_sessions = parse_json_column(df_mod_sessions, col)\n",
        "    \n",
        "    # Convert boolean and numeric columns\n",
        "    if 'is_final_version' in df_mod_sessions.columns:\n",
        "        df_mod_sessions['is_final_version'] = df_mod_sessions['is_final_version'].astype(str).str.lower() == 'true'\n",
        "    for col in ['scenario_index', 'attempt_number', 'version_number']:\n",
        "        if col in df_mod_sessions.columns:\n",
        "            df_mod_sessions[col] = pd.to_numeric(df_mod_sessions[col], errors='coerce')\n",
        "    \n",
        "    print(f\"  ✓ Cleaned moderation_session: {len(df_mod_sessions)} rows\")\n",
        "else:\n",
        "    df_mod_sessions = pd.DataFrame()\n",
        "\n",
        "# Moderation Applied\n",
        "if 'moderation_applied' in raw_dataframes:\n",
        "    print(\"Cleaning moderation_applied table...\")\n",
        "    df_mod_applied = raw_dataframes['moderation_applied'].copy()\n",
        "    df_mod_applied = clean_strings(df_mod_applied)\n",
        "    df_mod_applied = convert_timestamps(df_mod_applied)\n",
        "    \n",
        "    # Parse JSON fields\n",
        "    json_cols = ['strategies', 'custom_instructions', 'highlighted_texts', 'refactored_response']\n",
        "    for col in json_cols:\n",
        "        if col in df_mod_applied.columns:\n",
        "            df_mod_applied = parse_json_column(df_mod_applied, col)\n",
        "    \n",
        "    if 'confirmed_preferred' in df_mod_applied.columns:\n",
        "        df_mod_applied['confirmed_preferred'] = df_mod_applied['confirmed_preferred'].astype(str).str.lower() == 'true'\n",
        "    if 'version_index' in df_mod_applied.columns:\n",
        "        df_mod_applied['version_index'] = pd.to_numeric(df_mod_applied['version_index'], errors='coerce')\n",
        "    \n",
        "    print(f\"  ✓ Cleaned moderation_applied: {len(df_mod_applied)} rows\")\n",
        "else:\n",
        "    df_mod_applied = pd.DataFrame()\n",
        "\n",
        "# Moderation Question Answer\n",
        "if 'moderation_question_answer' in raw_dataframes:\n",
        "    print(\"Cleaning moderation_question_answer table...\")\n",
        "    df_mod_qa = raw_dataframes['moderation_question_answer'].copy()\n",
        "    df_mod_qa = clean_strings(df_mod_qa)\n",
        "    df_mod_qa = convert_timestamps(df_mod_qa)\n",
        "    print(f\"  ✓ Cleaned moderation_question_answer: {len(df_mod_qa)} rows\")\n",
        "else:\n",
        "    df_mod_qa = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.8: Clean Exit Quiz Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if 'exit_quiz_response' in raw_dataframes:\n",
        "    print(\"Cleaning exit_quiz_response table...\")\n",
        "    df_exit_quiz = raw_dataframes['exit_quiz_response'].copy()\n",
        "    \n",
        "    # Basic cleaning\n",
        "    df_exit_quiz = clean_strings(df_exit_quiz)\n",
        "    df_exit_quiz = convert_timestamps(df_exit_quiz)\n",
        "    \n",
        "    # Parse JSON fields\n",
        "    df_exit_quiz = parse_json_column(df_exit_quiz, 'answers')\n",
        "    df_exit_quiz = parse_json_column(df_exit_quiz, 'score')\n",
        "    df_exit_quiz = parse_json_column(df_exit_quiz, 'meta')\n",
        "    \n",
        "    # Convert boolean and numeric columns\n",
        "    if 'is_current' in df_exit_quiz.columns:\n",
        "        df_exit_quiz['is_current'] = df_exit_quiz['is_current'].astype(str).str.lower() == 'true'\n",
        "    if 'attempt_number' in df_exit_quiz.columns:\n",
        "        df_exit_quiz['attempt_number'] = pd.to_numeric(df_exit_quiz['attempt_number'], errors='coerce')\n",
        "    \n",
        "    print(f\"  ✓ Cleaned exit_quiz_response table: {len(df_exit_quiz)} rows, {len(df_exit_quiz.columns)} columns\")\n",
        "else:\n",
        "    print(\"✗ Exit quiz table not found\")\n",
        "    df_exit_quiz = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.9: Clean Scenario Tables (if present)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Scenario Assignments\n",
        "if 'scenario_assignments' in raw_dataframes:\n",
        "    print(\"Cleaning scenario_assignments table...\")\n",
        "    df_scenario_assignments = raw_dataframes['scenario_assignments'].copy()\n",
        "    df_scenario_assignments = clean_strings(df_scenario_assignments)\n",
        "    df_scenario_assignments = convert_timestamps(df_scenario_assignments)\n",
        "    \n",
        "    # Convert numeric columns\n",
        "    numeric_cols = ['alpha', 'eligible_pool_size', 'n_assigned_before', 'weight', \n",
        "                   'sampling_prob', 'assignment_position', 'issue_any', 'duration_seconds']\n",
        "    for col in numeric_cols:\n",
        "        if col in df_scenario_assignments.columns:\n",
        "            df_scenario_assignments[col] = pd.to_numeric(df_scenario_assignments[col], errors='coerce')\n",
        "    \n",
        "    print(f\"  ✓ Cleaned scenario_assignments: {len(df_scenario_assignments)} rows\")\n",
        "else:\n",
        "    df_scenario_assignments = pd.DataFrame()\n",
        "\n",
        "# Scenarios\n",
        "if 'scenarios' in raw_dataframes:\n",
        "    print(\"Cleaning scenarios table...\")\n",
        "    df_scenarios = raw_dataframes['scenarios'].copy()\n",
        "    df_scenarios = clean_strings(df_scenarios)\n",
        "    df_scenarios = convert_timestamps(df_scenarios)\n",
        "    \n",
        "    # Convert boolean and numeric columns\n",
        "    if 'is_active' in df_scenarios.columns:\n",
        "        df_scenarios['is_active'] = df_scenarios['is_active'].astype(str).str.lower() == 'true'\n",
        "    numeric_cols = ['n_assigned', 'n_completed', 'n_skipped', 'n_abandoned']\n",
        "    for col in numeric_cols:\n",
        "        if col in df_scenarios.columns:\n",
        "            df_scenarios[col] = pd.to_numeric(df_scenarios[col], errors='coerce')\n",
        "    \n",
        "    print(f\"  ✓ Cleaned scenarios: {len(df_scenarios)} rows\")\n",
        "else:\n",
        "    df_scenarios = pd.DataFrame()\n",
        "\n",
        "# Attention Check Scenarios\n",
        "if 'attention_check_scenarios' in raw_dataframes:\n",
        "    print(\"Cleaning attention_check_scenarios table...\")\n",
        "    df_attention_checks = raw_dataframes['attention_check_scenarios'].copy()\n",
        "    df_attention_checks = clean_strings(df_attention_checks)\n",
        "    df_attention_checks = convert_timestamps(df_attention_checks)\n",
        "    \n",
        "    if 'is_active' in df_attention_checks.columns:\n",
        "        df_attention_checks['is_active'] = df_attention_checks['is_active'].astype(str).str.lower() == 'true'\n",
        "    \n",
        "    print(f\"  ✓ Cleaned attention_check_scenarios: {len(df_attention_checks)} rows\")\n",
        "else:\n",
        "    df_attention_checks = pd.DataFrame()\n",
        "\n",
        "# Assignment Session Activity\n",
        "if 'assignment_session_activity' in raw_dataframes:\n",
        "    print(\"Cleaning assignment_session_activity table...\")\n",
        "    df_activity = raw_dataframes['assignment_session_activity'].copy()\n",
        "    df_activity = clean_strings(df_activity)\n",
        "    df_activity = convert_timestamps(df_activity)\n",
        "    \n",
        "    # Convert numeric columns\n",
        "    numeric_cols = ['session_number', 'active_ms_delta', 'cumulative_ms']\n",
        "    for col in numeric_cols:\n",
        "        if col in df_activity.columns:\n",
        "            df_activity[col] = pd.to_numeric(df_activity[col], errors='coerce')\n",
        "    \n",
        "    print(f\"  ✓ Cleaned assignment_session_activity: {len(df_activity)} rows\")\n",
        "else:\n",
        "    df_activity = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Data Quality Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"DATA QUALITY SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cleaned_tables = {\n",
        "    'users': df_users if 'df_users' in globals() else pd.DataFrame(),\n",
        "    'chats': df_chats if 'df_chats' in globals() else pd.DataFrame(),\n",
        "    'messages': df_messages if 'df_messages' in globals() else pd.DataFrame(),\n",
        "    'child_profiles': df_child_profiles if 'df_child_profiles' in globals() else pd.DataFrame(),\n",
        "    'selections': df_selections if 'df_selections' in globals() else pd.DataFrame(),\n",
        "    'moderation_scenarios': df_mod_scenarios if 'df_mod_scenarios' in globals() else pd.DataFrame(),\n",
        "    'moderation_sessions': df_mod_sessions if 'df_mod_sessions' in globals() else pd.DataFrame(),\n",
        "    'moderation_applied': df_mod_applied if 'df_mod_applied' in globals() else pd.DataFrame(),\n",
        "    'moderation_qa': df_mod_qa if 'df_mod_qa' in globals() else pd.DataFrame(),\n",
        "    'exit_quiz': df_exit_quiz if 'df_exit_quiz' in globals() else pd.DataFrame(),\n",
        "    'scenario_assignments': df_scenario_assignments if 'df_scenario_assignments' in globals() else pd.DataFrame(),\n",
        "    'scenarios': df_scenarios if 'df_scenarios' in globals() else pd.DataFrame(),\n",
        "    'attention_checks': df_attention_checks if 'df_attention_checks' in globals() else pd.DataFrame(),\n",
        "    'activity': df_activity if 'df_activity' in globals() else pd.DataFrame(),\n",
        "}\n",
        "\n",
        "for table_name, df in cleaned_tables.items():\n",
        "    if len(df) > 0:\n",
        "        print(f\"\\n{table_name.upper()}:\")\n",
        "        print(f\"  Rows: {len(df)}\")\n",
        "        print(f\"  Columns: {len(df.columns)}\")\n",
        "        print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
        "        \n",
        "        # Check for nulls\n",
        "        null_counts = df.isnull().sum()\n",
        "        if null_counts.sum() > 0:\n",
        "            print(f\"  Columns with nulls: {len(null_counts[null_counts > 0])}\")\n",
        "            top_nulls = null_counts[null_counts > 0].head(5)\n",
        "            for col, count in top_nulls.items():\n",
        "                pct = (count / len(df)) * 100\n",
        "                print(f\"    - {col}: {count} ({pct:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"\\n{table_name.upper()}: No data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Save Cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create output directory\n",
        "output_dir = Path(\"data_exports\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Saving cleaned DataFrames to {output_dir}/...\")\n",
        "\n",
        "saved_files = {}\n",
        "for table_name, df in cleaned_tables.items():\n",
        "    if len(df) > 0:\n",
        "        # Save as CSV\n",
        "        csv_file = output_dir / f\"{table_name}.csv\"\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        \n",
        "        # Save as pickle (faster loading)\n",
        "        pkl_file = output_dir / f\"{table_name}.pkl\"\n",
        "        df.to_pickle(pkl_file)\n",
        "        \n",
        "        saved_files[table_name] = {\n",
        "            'csv': str(csv_file),\n",
        "            'pkl': str(pkl_file),\n",
        "            'rows': len(df)\n",
        "        }\n",
        "        print(f\"  ✓ {table_name}: {len(df)} rows -> {csv_file.name}, {pkl_file.name}\")\n",
        "\n",
        "print(f\"\\n✓ Saved {len(saved_files)} tables\")\n",
        "\n",
        "# Create summary JSON\n",
        "summary = {\n",
        "    'extraction_date': datetime.now().isoformat(),\n",
        "    'dump_file': str(dump_file) if 'dump_file' in locals() else None,\n",
        "    'tables': {}\n",
        "}\n",
        "\n",
        "for table_name, df in cleaned_tables.items():\n",
        "    if len(df) > 0:\n",
        "        summary['tables'][table_name] = {\n",
        "            'row_count': len(df),\n",
        "            'column_count': len(df.columns),\n",
        "            'columns': list(df.columns),\n",
        "            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024\n",
        "        }\n",
        "\n",
        "summary_file = output_dir / \"summary.json\"\n",
        "with open(summary_file, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"✓ Summary saved to {summary_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Quick Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display sample data from key tables\n",
        "print(\"Sample data from key tables:\\n\")\n",
        "\n",
        "if len(df_users) > 0:\n",
        "    print(\"USERS (first 3 rows):\")\n",
        "    print(df_users.head(3).to_string())\n",
        "    print(\"\\n\")\n",
        "\n",
        "if len(df_chats) > 0:\n",
        "    print(\"CHATS (first 3 rows):\")\n",
        "    display_cols = ['id', 'user_id', 'title', 'created_at_datetime', 'message_count']\n",
        "    available_cols = [col for col in display_cols if col in df_chats.columns]\n",
        "    print(df_chats[available_cols].head(3).to_string())\n",
        "    print(\"\\n\")\n",
        "\n",
        "if len(df_selections) > 0:\n",
        "    print(\"SELECTIONS (first 3 rows):\")\n",
        "    display_cols = ['id', 'user_id', 'role', 'source', 'created_at_datetime']\n",
        "    available_cols = [col for col in display_cols if col in df_selections.columns]\n",
        "    print(df_selections[available_cols].head(3).to_string())\n",
        "    print(\"\\n\")\n",
        "\n",
        "if len(df_child_profiles) > 0:\n",
        "    print(\"CHILD PROFILES (first 3 rows):\")\n",
        "    display_cols = ['id', 'user_id', 'name', 'child_age', 'child_gender', 'is_current']\n",
        "    available_cols = [col for col in display_cols if col in df_child_profiles.columns]\n",
        "    print(df_child_profiles[available_cols].head(3).to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "All relevant tables have been extracted, cleaned, and saved to the `data_exports/` directory.\n",
        "\n",
        "You can now load the cleaned data using:\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.read_pickle('data_exports/table_name.pkl')\n",
        "```\n",
        "\n",
        "The data has been:\n",
        "- Parsed from the PostgreSQL dump\n",
        "- Cleaned (null bytes removed, strings normalized)\n",
        "- Transformed (timestamps converted, JSON parsed, types corrected)\n",
        "- Saved in both CSV and pickle formats for easy access"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "parentalcontrol",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}